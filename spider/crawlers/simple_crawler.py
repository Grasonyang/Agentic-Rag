"""
ç°¡åŒ–ç‰ˆç¶²é çˆ¬èŸ² - å°ˆæ³¨æ–¼æ ¸å¿ƒçˆ¬èŸ²åŠŸèƒ½
"""

import asyncio
import logging
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from crawl4ai import AsyncWebCrawler

logger = logging.getLogger(__name__)

@dataclass
class SimpleCrawlResult:
    """ç°¡åŒ–çš„çˆ¬èŸ²çµæœ"""
    url: str
    success: bool
    title: str = ""
    content: str = ""
    markdown: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None
    response_time: Optional[float] = None

class SimpleWebCrawler:
    """ç°¡åŒ–ç‰ˆç¶²é çˆ¬èŸ² - å»é™¤è³‡æ–™åº«ä¾è³´"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬èŸ²"""
        self.crawler = None
        
    async def crawl_url(self, url: str, **kwargs) -> SimpleCrawlResult:
        """
        çˆ¬å–å–®å€‹ URL
        
        Args:
            url: è¦çˆ¬å–çš„ URL
            **kwargs: é¡å¤–çš„çˆ¬èŸ²é…ç½®
            
        Returns:
            SimpleCrawlResult: çˆ¬èŸ²çµæœ
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ•·ï¸ é–‹å§‹çˆ¬å–: {url}")
            
            # åˆå§‹åŒ–çˆ¬èŸ²ï¼ˆå¦‚æœé‚„æ²’æœ‰ï¼‰
            if not self.crawler:
                self.crawler = AsyncWebCrawler(
                    headless=True,
                    verbose=False
                )
                await self.crawler.start()
            
            # åŸ·è¡Œçˆ¬èŸ²
            result = await self.crawler.arun(
                url=url,
                word_count_threshold=10,
                **kwargs
            )
            
            response_time = time.time() - start_time
            
            if result.success:
                logger.info(f"âœ… çˆ¬å–æˆåŠŸ: {url} ({response_time:.2f}s)")
                return SimpleCrawlResult(
                    url=url,
                    success=True,
                    title=result.metadata.get('title', '') if result.metadata else '',
                    content=result.cleaned_html or '',
                    markdown=result.markdown or '',
                    metadata=result.metadata or {},
                    response_time=response_time
                )
            else:
                logger.warning(f"âŒ çˆ¬å–å¤±æ•—: {url} - {result.error_message}")
                return SimpleCrawlResult(
                    url=url,
                    success=False,
                    error=result.error_message,
                    response_time=response_time
                )
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = str(e)
            logger.error(f"âŒ çˆ¬å–ç•°å¸¸: {url} - {error_msg}")
            return SimpleCrawlResult(
                url=url,
                success=False,
                error=error_msg,
                response_time=response_time
            )
    
    async def crawl_batch(self, urls: List[str], max_concurrent: int = 5) -> List[SimpleCrawlResult]:
        """
        æ‰¹é‡çˆ¬å– URLs
        
        Args:
            urls: URL åˆ—è¡¨
            max_concurrent: æœ€å¤§ä½µç™¼æ•¸
            
        Returns:
            List[SimpleCrawlResult]: çˆ¬èŸ²çµæœåˆ—è¡¨
        """
        results = []
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def crawl_with_semaphore(url):
            async with semaphore:
                return await self.crawl_url(url)
        
        logger.info(f"ğŸš€ é–‹å§‹æ‰¹é‡çˆ¬å– {len(urls)} å€‹ URLs (ä½µç™¼æ•¸: {max_concurrent})")
        start_time = time.time()
        
        # åŸ·è¡Œæ‰¹é‡çˆ¬èŸ²
        tasks = [crawl_with_semaphore(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è™•ç†ç•°å¸¸çµæœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(SimpleCrawlResult(
                    url=urls[i],
                    success=False,
                    error=str(result)
                ))
            else:
                processed_results.append(result)
        
        total_time = time.time() - start_time
        success_count = sum(1 for r in processed_results if r.success)
        
        logger.info(f"ğŸ“Š æ‰¹é‡çˆ¬å–å®Œæˆ: {success_count}/{len(urls)} æˆåŠŸ ({total_time:.2f}s)")
        
        return processed_results
    
    async def close(self):
        """é—œé–‰çˆ¬èŸ²"""
        if self.crawler:
            await self.crawler.close()
            self.crawler = None
            logger.info("ğŸ”’ çˆ¬èŸ²å·²é—œé–‰")
    
    async def __aenter__(self):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€²å…¥"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        await self.close()
