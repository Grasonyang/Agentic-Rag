"""
Enhanced RAG Spider - å®Œå…¨å°æ‡‰ schema.sql è³‡æ–™åº«æ¶æ§‹
èˆ‡ database/models.py å®Œå…¨æ•´åˆçš„ç©©å®šçˆ¬èŸ²å¯¦ä½œ

å¢å¼·åŠŸèƒ½ï¼š
- å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
- é€£æ¥æ± å’Œå¥åº·æª¢æŸ¥  
- çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„
- é€Ÿç‡é™åˆ¶å’Œåçˆ¬èŸ²å°ç­–
- æ•¸æ“šåº«äº‹å‹™ç®¡ç†
- æ–·ç·šæ¢å¾©æ©Ÿåˆ¶
"""

import asyncio
import hashlib
import xml.etree.ElementTree as ET
import os
import json
import time
import signal
from typing import List, Dict, Any, Optional, Tuple, Set
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from dataclasses import dataclass

from database.models import (
    DiscoveredURLModel, ArticleModel, ChunkModel, SitemapModel,
    CrawlStatus, ChangeFreq, ModelFactory
)

# å°å…¥å¢å¼·çš„å·¥å…·é¡
from spider.utils.enhanced_logger import get_spider_logger
from spider.utils.connection_manager import EnhancedConnectionManager, ConnectionConfig
from spider.utils.database_manager import EnhancedDatabaseManager, DatabaseConfig
from spider.utils.retry_manager import RetryManager, RetryConfig, RetryReason


@dataclass
class SpiderConfig:
    """çˆ¬èŸ²é…ç½®"""
    # åŸºæœ¬è¨­ç½®
    max_concurrent: int = 5
    delay: float = 1.0
    
    # é‡è©¦è¨­ç½®
    max_retries: int = 3
    retry_delay: float = 1.0
    
    # è¶…æ™‚è¨­ç½®
    request_timeout: float = 30.0
    read_timeout: float = 60.0
    
    # å…§å®¹è™•ç†
    chunk_size: int = 500
    max_content_length: int = 1024 * 1024  # 1MB
    
    # é€Ÿç‡é™åˆ¶
    requests_per_second: float = 2.0
    burst_requests: int = 5
    
    # åçˆ¬èŸ²å°ç­–
    randomize_delay: bool = True
    use_proxy_rotation: bool = False
    rotate_user_agents: bool = True


class EnhancedRAGSpider:
    """
    å¢å¼·ç‰ˆ RAG ç³»çµ±çˆ¬èŸ² - å®Œå…¨å°æ‡‰ schema.sql æ¶æ§‹
    
    å¢å¼·åŠŸèƒ½ï¼š
    - å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
    - é€£æ¥æ± å’Œå¥åº·æª¢æŸ¥
    - çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„
    - é€Ÿç‡é™åˆ¶å’Œåçˆ¬èŸ²å°ç­–
    - æ•¸æ“šåº«äº‹å‹™ç®¡ç†
    - æ–·ç·šæ¢å¾©æ©Ÿåˆ¶
    """
    
    def __init__(self, config: Optional[SpiderConfig] = None):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆçˆ¬èŸ²
        
        Args:
            config: çˆ¬èŸ²é…ç½®
        """
        self.config = config or SpiderConfig()
        self.logger = get_spider_logger("enhanced_rag_spider")
        
        # é€£æ¥ç®¡ç†å™¨
        self.connection_manager: Optional[EnhancedConnectionManager] = None
        self.database_manager: Optional[EnhancedDatabaseManager] = None
        
        # ç‹€æ…‹ç®¡ç†
        self.is_running = False
        self.is_paused = False
        self._shutdown_event = asyncio.Event()
        
        # çµ±è¨ˆä¿¡æ¯
        self.stats = {
            "sitemaps_processed": 0,
            "urls_discovered": 0,
            "articles_crawled": 0,
            "chunks_created": 0,
            "errors": 0,
            "retries": 0,
            "start_time": None,
            "processed_urls": set(),  # é¿å…é‡è¤‡è™•ç†
            "failed_urls": set()      # è¨˜éŒ„å¤±æ•—çš„URL
        }
        
        # è¨­ç½®ä¿¡è™Ÿè™•ç†å™¨
        self._setup_signal_handlers()
        
    def _setup_signal_handlers(self):
        """è¨­ç½®ä¿¡è™Ÿè™•ç†å™¨ä»¥æ”¯æŒå„ªé›…é—œé–‰"""
        def signal_handler(signum, frame):
            self.logger.info(f"æ”¶åˆ°ä¿¡è™Ÿ {signum}ï¼Œæº–å‚™é—œé–‰çˆ¬èŸ²...")
            asyncio.create_task(self.shutdown())
        
        try:
            signal.signal(signal.SIGINT, signal_handler)
            signal.signal(signal.SIGTERM, signal_handler)
        except ValueError:
            # åœ¨æŸäº›ç’°å¢ƒä¸­å¯èƒ½ç„¡æ³•è¨­ç½®ä¿¡è™Ÿè™•ç†å™¨
            pass
        
    async def __aenter__(self):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.initialize()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.shutdown()
    
    async def initialize(self):
        """åˆå§‹åŒ–çˆ¬èŸ²çµ„ä»¶"""
        try:
            self.logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¢å¼·ç‰ˆ RAG çˆ¬èŸ²...")
            
            # å‰µå»ºé€£æ¥é…ç½®
            conn_config = ConnectionConfig(
                timeout=self.config.request_timeout,
                read_timeout=self.config.read_timeout,
                connector_limit=self.config.max_concurrent * 2,
                max_retries=self.config.max_retries,
                retry_delay=self.config.retry_delay,
                requests_per_second=self.config.requests_per_second,
                burst_requests=self.config.burst_requests
            )
            
            # å‰µå»ºæ•¸æ“šåº«é…ç½®
            db_config = DatabaseConfig(
                max_retries=self.config.max_retries,
                retry_delay=self.config.retry_delay
            )
            
            # åˆå§‹åŒ–ç®¡ç†å™¨
            self.connection_manager = EnhancedConnectionManager(conn_config)
            self.database_manager = EnhancedDatabaseManager(db_config)
            
            # å•Ÿå‹•ç®¡ç†å™¨
            await self.connection_manager.__aenter__()
            await self.database_manager.__aenter__()
            
            self.is_running = True
            self.stats["start_time"] = time.time()
            
            self.logger.info("âœ… å¢å¼·ç‰ˆ RAG çˆ¬èŸ²åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬èŸ²åˆå§‹åŒ–å¤±æ•—: {e}")
            await self.shutdown()
            raise
    
    async def shutdown(self):
        """é—œé–‰çˆ¬èŸ²"""
        if not self.is_running:
            return
        
        self.logger.info("ğŸ›‘ æ­£åœ¨é—œé–‰å¢å¼·ç‰ˆ RAG çˆ¬èŸ²...")
        self.is_running = False
        self._shutdown_event.set()
        
        # é—œé–‰ç®¡ç†å™¨
        if self.connection_manager:
            await self.connection_manager.__aexit__(None, None, None)
        
        if self.database_manager:
            await self.database_manager.__aexit__(None, None, None)
        
        # è¨˜éŒ„æœ€çµ‚çµ±è¨ˆ
        self.logger.log_statistics()
        self.logger.info("âœ… å¢å¼·ç‰ˆ RAG çˆ¬èŸ²å·²é—œé–‰")
    
    async def pause(self):
        """æš«åœçˆ¬èŸ²"""
        self.is_paused = True
        self.logger.info("â¸ï¸ çˆ¬èŸ²å·²æš«åœ")
    
    async def resume(self):
        """æ¢å¾©çˆ¬èŸ²"""
        self.is_paused = False
        self.logger.info("â–¶ï¸ çˆ¬èŸ²å·²æ¢å¾©")
    
    async def _check_pause_and_shutdown(self):
        """æª¢æŸ¥æš«åœå’Œé—œé–‰ç‹€æ…‹"""
        while self.is_paused and self.is_running:
            await asyncio.sleep(1)
        
        if not self.is_running:
            raise asyncio.CancelledError("çˆ¬èŸ²å·²é—œé–‰")
    
    async def parse_sitemap(self, sitemap_url: str, update_db: bool = True) -> Tuple[bool, List[DiscoveredURLModel]]:
        """
        è§£æ sitemap ä¸¦è¿”å›ç™¼ç¾çš„ URL
        
        Args:
            sitemap_url: Sitemap URL
            update_db: æ˜¯å¦æ›´æ–°è³‡æ–™åº«ä¸­çš„ sitemap ç‹€æ…‹
            
        Returns:
            Tuple[bool, List[DiscoveredURLModel]]: (æˆåŠŸæ¨™èªŒ, URLåˆ—è¡¨)
        """
        await self._check_pause_and_shutdown()
        
        # é¿å…é‡è¤‡è™•ç†
        if sitemap_url in self.stats["processed_urls"]:
            self.logger.warning(f"âš ï¸ Sitemap å·²è™•ç†éï¼Œè·³é: {sitemap_url}")
            return True, []
        
        try:
            self.logger.info(f"ğŸ—ºï¸ é–‹å§‹è§£æ Sitemap: {sitemap_url}")
            
            # è¨˜éŒ„è«‹æ±‚é–‹å§‹
            request_context = self.logger.log_request_start(sitemap_url, "GET")
            
            # ä¸‹è¼‰ Sitemap
            async with self.connection_manager.get(sitemap_url) as response:
                if response.status != 200:
                    error_msg = f"HTTP {response.status}"
                    self.logger.log_request_error(request_context, Exception(error_msg), response.status)
                    self.stats["errors"] += 1
                    return False, []
                
                content = await response.text()
                
                # è¨˜éŒ„æˆåŠŸ
                self.logger.log_request_success(request_context, response.status, len(content))
            
            # è§£æ XML
            discovered_urls = self._parse_sitemap_xml(content, sitemap_url)
            
            # æ‰¹é‡æ’å…¥ç™¼ç¾çš„ URL
            if discovered_urls and self.database_manager:
                count = await self.database_manager.bulk_create_discovered_urls(discovered_urls)
                self.logger.log_sitemap_parsing(sitemap_url, count, True)
            else:
                self.logger.log_sitemap_parsing(sitemap_url, len(discovered_urls), True)
            
            # æ¨™è¨˜ç‚ºå·²è™•ç†
            self.stats["processed_urls"].add(sitemap_url)
            self.stats["sitemaps_processed"] += 1
            self.stats["urls_discovered"] += len(discovered_urls)
            
            return True, discovered_urls
            
        except Exception as e:
            self.logger.log_sitemap_parsing(sitemap_url, 0, False, e)
            self.stats["errors"] += 1
            self.stats["failed_urls"].add(sitemap_url)
            return False, []
    
    def _parse_sitemap_xml(self, xml_content: str, sitemap_url: str) -> List[DiscoveredURLModel]:
        """
        è§£æ XML å…§å®¹ä¸¦æå– URL ä¿¡æ¯
        """
        discovered_urls = []
        
        try:
            root = ET.fromstring(xml_content)
            
            # å®šç¾© XML å‘½åç©ºé–“
            namespaces = {
                'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'
            }
            
            # æª¢æŸ¥æ˜¯å¦ç‚º sitemapindex æ ¼å¼
            if root.tag.endswith('sitemapindex'):
                self.logger.info(f"ğŸ“‹ æª¢æ¸¬åˆ° sitemap index æ ¼å¼: {sitemap_url}")
                # è™•ç† sitemapindex - æå–å­ sitemap URLs
                for sitemap_elem in root.findall('.//sitemap:sitemap', namespaces):
                    try:
                        loc_elem = sitemap_elem.find('sitemap:loc', namespaces)
                        if loc_elem is None or not loc_elem.text:
                            continue
                        
                        sub_sitemap_url = loc_elem.text.strip()
                        self.logger.info(f"ğŸ”— ç™¼ç¾å­ sitemap: {sub_sitemap_url}")
                        
                        # ä½¿ç”¨åŒæ­¥æ–¹å¼è™•ç†å­ sitemapï¼Œé¿å…äº‹ä»¶å¾ªç’°è¡çª
                        try:
                            import requests
                            response = requests.get(sub_sitemap_url, timeout=10)
                            if response.status_code == 200:
                                sub_content = response.text
                                sub_urls = self._parse_sitemap_xml(sub_content, sub_sitemap_url)
                                discovered_urls.extend(sub_urls)
                                self.logger.info(f"âœ… å¾å­ sitemap ç²å¾— {len(sub_urls)} å€‹ URLs: {sub_sitemap_url}")
                                
                                # é™åˆ¶è™•ç†çš„å­ sitemap æ•¸é‡ï¼Œé¿å…éå¤šè«‹æ±‚
                                if len(discovered_urls) >= 50:  # é™åˆ¶ç¸½æ•¸
                                    self.logger.info(f"âš ï¸ å·²é”åˆ° URL é™åˆ¶ï¼Œåœæ­¢è™•ç†æ›´å¤šå­ sitemap")
                                    break
                            else:
                                self.logger.warning(f"âš ï¸ å­ sitemap è¨ªå•å¤±æ•— (HTTP {response.status_code}): {sub_sitemap_url}")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ è™•ç†å­ sitemap ç•°å¸¸: {sub_sitemap_url} - {e}")
                            
                    except Exception as e:
                        self.logger.warning(f"è§£æå­ sitemap å…ƒç´ æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        continue
                        
            else:
                # è™•ç†æ¨™æº– urlset æ ¼å¼
                self.logger.info(f"ğŸ“„ æª¢æ¸¬åˆ° urlset æ ¼å¼: {sitemap_url}")
                
                # æŸ¥æ‰¾æ‰€æœ‰ URL å…ƒç´ 
                for url_elem in root.findall('.//sitemap:url', namespaces):
                    try:
                        # æå–å¿…è¦å­—æ®µ
                        loc_elem = url_elem.find('sitemap:loc', namespaces)
                        if loc_elem is None or not loc_elem.text:
                            continue
                        
                        url = loc_elem.text.strip()
                        
                        # æå–å¯é¸å­—æ®µ
                        priority = None
                        priority_elem = url_elem.find('sitemap:priority', namespaces)
                        if priority_elem is not None and priority_elem.text:
                            try:
                                priority = float(priority_elem.text)
                                # ç¢ºä¿åœ¨ 0.0-1.0 ç¯„åœå…§
                                priority = max(0.0, min(1.0, priority))
                            except ValueError:
                                priority = None
                        
                        changefreq = None
                        changefreq_elem = url_elem.find('sitemap:changefreq', namespaces)
                        if changefreq_elem is not None and changefreq_elem.text:
                            try:
                                changefreq = ChangeFreq(changefreq_elem.text.lower())
                            except ValueError:
                                changefreq = None
                        
                        lastmod = None
                        lastmod_elem = url_elem.find('sitemap:lastmod', namespaces)
                        if lastmod_elem is not None and lastmod_elem.text:
                            try:
                                # è§£æ ISO 8601 æ—¥æœŸæ ¼å¼
                                lastmod_str = lastmod_elem.text.strip()
                                if 'T' in lastmod_str:
                                    lastmod = datetime.fromisoformat(lastmod_str.replace('Z', '+00:00'))
                                else:
                                    lastmod = datetime.fromisoformat(lastmod_str)
                            except ValueError:
                                lastmod = None
                        
                        # å‰µå»º DiscoveredURL æ¨¡å‹
                        url_model = ModelFactory.create_discovered_url(
                            url=url,
                            source_sitemap=sitemap_url,
                            priority=priority,
                            changefreq=changefreq,
                            lastmod=lastmod,
                            crawl_status=CrawlStatus.PENDING,
                            metadata={
                                "discovered_from": "sitemap",
                                "sitemap_url": sitemap_url,
                                "discovered_at": datetime.now().isoformat()
                            }
                        )
                        
                        discovered_urls.append(url_model)
                        
                    except Exception as e:
                        self.logger.warning(f"è§£æå–®å€‹ URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        continue
            
            self.logger.info(f"æˆåŠŸè§£æ {len(discovered_urls)} å€‹ URL")
            return discovered_urls
            
        except ET.ParseError as e:
            self.logger.error(f"XML è§£æéŒ¯èª¤: {e}")
            return []
        except Exception as e:
            self.logger.error(f"è§£æ Sitemap XML æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    async def crawl_url(self, url: str) -> Tuple[bool, Optional[ArticleModel]]:
        """
        çˆ¬å–å–®å€‹ URL ä¸¦å‰µå»ºæ–‡ç« è¨˜éŒ„
        
        Args:
            url: è¦çˆ¬å–çš„ URL
            
        Returns:
            Tuple[bool, Optional[ArticleModel]]: (æˆåŠŸæ¨™èªŒ, æ–‡ç« æ¨¡å‹)
        """
        await self._check_pause_and_shutdown()
        
        # é¿å…é‡è¤‡è™•ç†
        if url in self.stats["processed_urls"]:
            self.logger.warning(f"âš ï¸ URL å·²è™•ç†éï¼Œè·³é: {url}")
            return True, None
        
        # æª¢æŸ¥æ˜¯å¦åœ¨å¤±æ•—åˆ—è¡¨ä¸­
        if url in self.stats["failed_urls"]:
            self.logger.warning(f"âš ï¸ URL ä¹‹å‰å¤±æ•—éï¼Œè·³é: {url}")
            return False, None
        
        url_record = None
        try:
            self.logger.info(f"ğŸ•·ï¸ é–‹å§‹çˆ¬å– URL: {url}")
            
            # è¨˜éŒ„è«‹æ±‚é–‹å§‹
            request_context = self.logger.log_request_start(url, "GET")
            
            # æŸ¥æ‰¾å°æ‡‰çš„ discovered_url è¨˜éŒ„
            if self.database_manager:
                pending_urls = await self.database_manager.get_pending_urls(limit=1000)
                url_record = next((u for u in pending_urls if u.url == url), None)
                
                if url_record:
                    await self.database_manager.safe_update_crawl_status(
                        url_record.id, CrawlStatus.CRAWLING
                    )
            
            # ä¸‹è¼‰ç¶²é å…§å®¹
            async with self.connection_manager.get(url) as response:
                if response.status != 200:
                    error_msg = f"HTTP {response.status}"
                    if url_record and self.database_manager:
                        await self.database_manager.safe_update_crawl_status(
                            url_record.id, CrawlStatus.ERROR, error_msg
                        )
                    
                    self.logger.log_request_error(
                        request_context, Exception(error_msg), response.status
                    )
                    self.stats["errors"] += 1
                    self.stats["failed_urls"].add(url)
                    return False, None
                
                html_content = await response.text()
                
                # æª¢æŸ¥å…§å®¹é•·åº¦é™åˆ¶
                if len(html_content) > self.config.max_content_length:
                    self.logger.warning(f"âš ï¸ å…§å®¹éå¤§ï¼Œæˆªå–å‰ {self.config.max_content_length} å­—ç¬¦")
                    html_content = html_content[:self.config.max_content_length]
                
                # è¨˜éŒ„æˆåŠŸ
                self.logger.log_request_success(request_context, response.status, len(html_content))
            
            # è§£æç¶²é å…§å®¹
            title, content = self._extract_content(html_content, url)
            
            # è¨˜éŒ„å…§å®¹æå–
            self.logger.log_content_extraction(url, title, len(content), True)
            
            # å‰µå»ºæ–‡ç« æ¨¡å‹
            article_model = ModelFactory.create_article(
                url=url,
                title=title,
                content=content,
                crawled_from_url_id=url_record.id if url_record else None,
                metadata={
                    "crawled_at": datetime.now().isoformat(),
                    "content_length": len(content),
                    "title_length": len(title),
                    "response_status": response.status,
                    "content_type": response.headers.get('content-type', 'unknown')
                }
            )
            
            # ä¿å­˜æ–‡ç« 
            if self.database_manager:
                success = await self.database_manager.create_article(article_model)
                if success:
                    # æ›´æ–°çˆ¬å–ç‹€æ…‹ç‚ºå®Œæˆ
                    if url_record:
                        await self.database_manager.safe_update_crawl_status(
                            url_record.id, CrawlStatus.COMPLETED
                        )
                    
                    # æ¨™è¨˜ç‚ºå·²è™•ç†
                    self.stats["processed_urls"].add(url)
                    self.stats["articles_crawled"] += 1
                    self.logger.info(f"âœ… æˆåŠŸçˆ¬å–ä¸¦ä¿å­˜æ–‡ç« : {title[:50]}...")
                    return True, article_model
                else:
                    if url_record:
                        await self.database_manager.safe_update_crawl_status(
                            url_record.id, CrawlStatus.ERROR, "ä¿å­˜æ–‡ç« å¤±æ•—"
                        )
                    self.stats["errors"] += 1
                    self.stats["failed_urls"].add(url)
                    return False, None
            
            # å¦‚æœæ²’æœ‰æ•¸æ“šåº«ç®¡ç†å™¨ï¼Œä»ç„¶æ¨™è¨˜ç‚ºæˆåŠŸ
            self.stats["processed_urls"].add(url)
            self.stats["articles_crawled"] += 1
            return True, article_model
            
        except Exception as e:
            self.logger.log_content_extraction(url, "", 0, False, e)
            
            if url_record and self.database_manager:
                await self.database_manager.safe_update_crawl_status(
                    url_record.id, CrawlStatus.ERROR, str(e)
                )
            
            self.stats["errors"] += 1
            self.stats["failed_urls"].add(url)
            return False, None
    
    def _extract_content(self, html_content: str, url: str) -> Tuple[str, str]:
        """
        å¾ HTML å…§å®¹ä¸­æå–æ¨™é¡Œå’Œæ­£æ–‡
        
        Args:
            html_content: HTML å…§å®¹
            url: URLï¼ˆç”¨æ–¼ç”Ÿæˆé»˜èªæ¨™é¡Œï¼‰
            
        Returns:
            Tuple[str, str]: (æ¨™é¡Œ, å…§å®¹)
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ¨™é¡Œ
            title = ""
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text().strip()
            
            # å¦‚æœæ²’æœ‰æ¨™é¡Œï¼Œä½¿ç”¨ URL ç”Ÿæˆ
            if not title:
                parsed_url = urlparse(url)
                title = f"Page from {parsed_url.netloc}"
            
            # æå–ä¸»è¦å…§å®¹
            content = ""
            
            # å˜—è©¦ä¸åŒçš„å…§å®¹é¸æ“‡å™¨ï¼ˆæŒ‰å„ªå…ˆç´šæ’åºï¼‰
            content_selectors = [
                'main article',
                'main',
                'article',
                '.content',
                '#content', 
                '.main-content',
                '.article-content',
                '.post-content',
                '.entry-content',
                '[role="main"]'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                    if len(content) > 100:  # ç¢ºä¿æœ‰è¶³å¤ çš„å…§å®¹
                        break
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç‰¹å®šå®¹å™¨æˆ–å…§å®¹å¤ªå°‘ï¼Œæå– body å…§å®¹
            if not content or len(content) < 100:
                body = soup.find('body')
                if body:
                    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                    for tag in body.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                        tag.decompose()
                    content = body.get_text(separator=' ', strip=True)
            
            # æ¸…ç†å…§å®¹
            content = self._clean_content(content)
            
            return title, content
            
        except Exception as e:
            self.logger.error(f"å…§å®¹æå–å¤±æ•—: {e}")
            return "æå–å¤±æ•—", ""
    
    def _clean_content(self, content: str) -> str:
        """
        æ¸…ç†æå–çš„å…§å®¹
        
        Args:
            content: åŸå§‹å…§å®¹
            
        Returns:
            str: æ¸…ç†å¾Œçš„å…§å®¹
        """
        if not content:
            return ""
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—ç¬¦
        import re
        content = re.sub(r'\s+', ' ', content)
        content = content.strip()
        
        # é™åˆ¶å…§å®¹é•·åº¦
        if len(content) > self.config.max_content_length:
            content = content[:self.config.max_content_length]
        
        return content
    
    async def create_chunks(self, article: ArticleModel, chunk_size: int = None) -> List[ChunkModel]:
        """
        å°‡æ–‡ç« å…§å®¹åˆ†å¡Š
        
        Args:
            article: æ–‡ç« æ¨¡å‹
            chunk_size: å¡Šå¤§å°ï¼ˆå­—ç¬¦æ•¸ï¼‰ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨é…ç½®ä¸­çš„é»˜èªå€¼
            
        Returns:
            List[ChunkModel]: æ–‡ç« å¡Šåˆ—è¡¨
        """
        chunks = []
        
        if not article.content:
            return chunks
        
        chunk_size = chunk_size or self.config.chunk_size
        content = article.content
        content_length = len(content)
        
        # ç°¡å–®åˆ†å¡Šç­–ç•¥
        for i in range(0, content_length, chunk_size):
            chunk_content = content[i:i + chunk_size]
            
            chunk_model = ModelFactory.create_chunk(
                article_id=article.id,
                content=chunk_content,
                chunk_index=len(chunks),
                metadata={
                    "chunk_method": "simple_split",
                    "chunk_size": chunk_size,
                    "created_at": datetime.now().isoformat()
                }
            )
            
            chunks.append(chunk_model)
        
        # ä¿å­˜å¡Š
        if chunks and self.database_manager:
            count = await self.database_manager.create_chunks(chunks)
            self.stats["chunks_created"] += count
            self.logger.log_chunking(article.url, count, "simple_split")
        
        return chunks
    
    async def crawl_batch(self, urls: List[str], create_chunks: bool = True) -> Dict[str, Any]:
        """
        æ‰¹é‡çˆ¬å– URL
        
        Args:
            urls: URL åˆ—è¡¨
            create_chunks: æ˜¯å¦å‰µå»ºæ–‡ç« å¡Š
            
        Returns:
            Dict[str, Any]: çˆ¬å–çµæœçµ±è¨ˆ
        """
        if not urls:
            return {"success": 0, "failed": 0, "total": 0}
        
        self.logger.info(f"ğŸš€ é–‹å§‹æ‰¹é‡çˆ¬å– {len(urls)} å€‹ URL")
        
        # ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä¸¦ç™¼
        semaphore = asyncio.Semaphore(self.config.max_concurrent)
        
        async def crawl_with_delay(url):
            async with semaphore:
                # æª¢æŸ¥æš«åœå’Œé—œé–‰ç‹€æ…‹
                await self._check_pause_and_shutdown()
                
                # æ·»åŠ å»¶é²
                if self.config.delay > 0:
                    await asyncio.sleep(self.config.delay)
                
                success, article = await self.crawl_url(url)
                
                # å‰µå»ºæ–‡ç« å¡Š
                if success and article and create_chunks:
                    await self.create_chunks(article)
                
                return success
        
        # åŸ·è¡Œæ‰¹é‡çˆ¬å–
        tasks = [crawl_with_delay(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµ±è¨ˆçµæœ
        success_count = sum(1 for r in results if r is True)
        failed_count = len(results) - success_count
        
        self.logger.info(f"âœ… æ‰¹é‡çˆ¬å–å®Œæˆ: æˆåŠŸ {success_count}, å¤±æ•— {failed_count}")
        
        return {
            "success": success_count,
            "failed": failed_count,
            "total": len(urls)
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çˆ¬èŸ²çµ±è¨ˆä¿¡æ¯"""
        stats = self.stats.copy()
        
        # æ·»åŠ é‹è¡Œæ™‚é–“
        if self.stats["start_time"]:
            stats["runtime"] = time.time() - self.stats["start_time"]
        
        # æ·»åŠ é€£æ¥ç®¡ç†å™¨çµ±è¨ˆ
        if self.connection_manager:
            stats["connection_stats"] = self.connection_manager.get_stats()
        
        # æ·»åŠ æ•¸æ“šåº«çµ±è¨ˆ
        if self.database_manager:
            db_stats = asyncio.run(self.database_manager.get_stats())
            stats["database_stats"] = db_stats
        
        return stats


# ä¾¿æ·å‡½æ•¸ï¼ˆç‚ºäº†å‘å¾Œå…¼å®¹ï¼‰
async def parse_sitemap_urls(sitemap_urls: List[str]) -> List[DiscoveredURLModel]:
    """
    ä¾¿æ·å‡½æ•¸ï¼šæ‰¹é‡è§£æå¤šå€‹ Sitemap
    
    Args:
        sitemap_urls: Sitemap URL åˆ—è¡¨
        
    Returns:
        List[DiscoveredURLModel]: æ‰€æœ‰ç™¼ç¾çš„ URL
    """
    all_urls = []
    
    async with EnhancedRAGSpider() as spider:
        for sitemap_url in sitemap_urls:
            success, urls = await spider.parse_sitemap(sitemap_url)
            if success:
                all_urls.extend(urls)
    
    return all_urls

async def crawl_pending_urls(limit: int = 100, create_chunks: bool = True) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•¸ï¼šçˆ¬å–å¾…è™•ç†çš„ URL
    
    Args:
        limit: æœ€å¤§çˆ¬å–æ•¸é‡
        create_chunks: æ˜¯å¦å‰µå»ºæ–‡ç« å¡Š
        
    Returns:
        Dict[str, Any]: çˆ¬å–çµæœ
    """
    async with EnhancedRAGSpider() as spider:
        if not spider.database_manager:
            return {"error": "ç„¡æ³•é€£æ¥è³‡æ–™åº«"}
        
        # ç²å–å¾…çˆ¬å–çš„ URL
        pending_urls = await spider.database_manager.get_pending_urls(limit=limit)
        if not pending_urls:
            return {"message": "æ²’æœ‰å¾…çˆ¬å–çš„ URL", "total": 0}
        
        urls = [url.url for url in pending_urls]
        result = await spider.crawl_batch(urls, create_chunks=create_chunks)
    
    return result


# ä¿æŒå‘å¾Œå…¼å®¹æ€§ï¼Œå‰µå»ºä¸€å€‹åˆ¥å
RAGSpider = EnhancedRAGSpider


# å·¥ä½œæµç¨‹æ–¹æ³•ï¼ˆä¿æŒä¸è®Šï¼Œä½†ä½¿ç”¨ self.logger ä»£æ›¿ loggerï¼‰
def discover_sitemaps(base_url: str, output_file: str = "sitemaps.txt") -> bool:
    """
    ç™¼ç¾ä¸¦ä¿å­˜ç¶²ç«™çš„sitemap (åŒæ™‚ä¿å­˜åˆ°æ–‡ä»¶å’Œè³‡æ–™åº«)
    
    Args:
        base_url: ç¶²ç«™åŸºç¤URL æˆ– robots.txt URL
        output_file: è¼¸å‡ºæ–‡ä»¶å
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    logger = get_spider_logger("discover_sitemaps")
    
    try:
        print(f"ğŸ” æ­£åœ¨ç™¼ç¾ {base_url} çš„ sitemaps...")
        
        sitemap_urls = []
        
        # å¦‚æœURLå·²ç¶“æŒ‡å‘robots.txtï¼Œç›´æ¥ä½¿ç”¨
        if base_url.endswith('/robots.txt'):
            robots_url = base_url
            # å¾robots.txt URLæ¨å°åŸºç¤URL
            site_base = base_url.replace('/robots.txt', '')
        else:
            # å¦å‰‡æ§‹å»ºrobots.txt URL
            robots_url = urljoin(base_url, "/robots.txt")
            site_base = base_url
        
        print(f"ğŸ“– è®€å– robots.txt: {robots_url}")
        
        # å˜—è©¦ä¸‹è¼‰å’Œè§£ærobots.txt
        import requests
        try:
            response = requests.get(robots_url, timeout=10)
            if response.status_code == 200:
                robots_content = response.text
                print(f"âœ… æˆåŠŸä¸‹è¼‰ robots.txt ({len(robots_content)} å­—ç¬¦)")
                
                # è§£ærobots.txtä¸­çš„Sitemapæ¢ç›®
                for line in robots_content.split('\n'):
                    line = line.strip()
                    if line.lower().startswith('sitemap:'):
                        sitemap_url = line.split(':', 1)[1].strip()
                        if sitemap_url:
                            sitemap_urls.append(sitemap_url)
                            print(f"ğŸ—ºï¸ åœ¨ robots.txt ä¸­ç™¼ç¾ sitemap: {sitemap_url}")
                
                if not sitemap_urls:
                    print("âš ï¸ robots.txt ä¸­æ²’æœ‰æ‰¾åˆ° sitemap æ¢ç›®ï¼Œä½¿ç”¨é è¨­ä½ç½®")
                    # å¦‚æœrobots.txtä¸­æ²’æœ‰sitemapï¼Œä½¿ç”¨å¸¸è¦‹ä½ç½®
                    sitemap_urls = [
                        urljoin(site_base, "/sitemap.xml"),
                        urljoin(site_base, "/sitemap_index.xml")
                    ]
            else:
                print(f"âš ï¸ ç„¡æ³•ä¸‹è¼‰ robots.txt (HTTP {response.status_code})ï¼Œä½¿ç”¨é è¨­sitemapä½ç½®")
                sitemap_urls = [
                    urljoin(site_base, "/sitemap.xml"),
                    urljoin(site_base, "/sitemap_index.xml")
                ]
        except Exception as e:
            print(f"âš ï¸ è®€å– robots.txt å¤±æ•—: {e}")
            print("ä½¿ç”¨é è¨­ sitemap ä½ç½®")
            sitemap_urls = [
                urljoin(site_base, "/sitemap.xml"),
                urljoin(site_base, "/sitemap_index.xml")
            ]
        
        # å°‡URLsä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            for url in sitemap_urls:
                f.write(f"{url}\n")
        
        print(f"âœ… Sitemap URLs å·²ä¿å­˜åˆ° {output_file}")
        print(f"ğŸ“ ç¸½å…±ç™¼ç¾ {len(sitemap_urls)} å€‹æ½›åœ¨çš„ sitemap URLs")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç™¼ç¾ sitemap å¤±æ•—: {e}")
        print(f"âŒ ç™¼ç¾ sitemap å¤±æ•—: {e}")
        return False
