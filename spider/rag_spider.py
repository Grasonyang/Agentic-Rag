"""
RAG Spider - å®Œå…¨å°æ‡‰ schema.sql è³‡æ–™åº«æ¶æ§‹
èˆ‡ database/models.py å®Œå…¨æ•´åˆçš„çˆ¬èŸ²å¯¦ä½œ
"""

import asyncio
import aiohttp
import logging
import hashlib
import xml.etree.ElementTree as ET
import os
import json
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import time

from database.models import (
    DiscoveredURLModel, ArticleModel, ChunkModel, SitemapModel,
    CrawlStatus, ChangeFreq, ModelFactory
)
from database.operations import get_database_operations

logger = logging.getLogger(__name__)

class RAGSpider:
    """
    RAG ç³»çµ±çˆ¬èŸ² - å®Œå…¨å°æ‡‰ schema.sql æ¶æ§‹
    æ”¯æ´ sitemap è§£æå’Œç¶²é çˆ¬å–
    """
    
    def __init__(self, max_concurrent: int = 5, delay: float = 1.0):
        """
        åˆå§‹åŒ–çˆ¬èŸ²
        
        Args:
            max_concurrent: æœ€å¤§ä¸¦ç™¼æ•¸
            delay: è«‹æ±‚é–“éš”ï¼ˆç§’ï¼‰
        """
        self.max_concurrent = max_concurrent
        self.delay = delay
        self.session = None
        self.db = get_database_operations()
        
        # çµ±è¨ˆä¿¡æ¯
        self.stats = {
            "sitemaps_processed": 0,
            "urls_discovered": 0,
            "articles_crawled": 0,
            "chunks_created": 0,
            "errors": 0
        }
        
    async def __aenter__(self):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'RAG-Spider/1.0 (compatible; Python-aiohttp)',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-TW,en-US;q=0.9,en;q=0.8'
            }
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()

    async def parse_sitemap(self, sitemap_url: str, update_db: bool = True) -> Tuple[bool, List[DiscoveredURLModel]]:
        """
        è§£æ sitemap ä¸¦è¿”å›ç™¼ç¾çš„ URL
        å®Œå…¨å°æ‡‰ discovered_urls è¡¨æ ¼æ¶æ§‹
        
        Args:
            sitemap_url: Sitemap URL
            update_db: æ˜¯å¦æ›´æ–°è³‡æ–™åº«ä¸­çš„ sitemap ç‹€æ…‹
            
        Returns:
            Tuple[bool, List[DiscoveredURLModel]]: (æˆåŠŸæ¨™èªŒ, URLåˆ—è¡¨)
        """
        try:
            logger.info(f"é–‹å§‹è§£æ Sitemap: {sitemap_url}")
            
            # 1. å¦‚æœéœ€è¦æ›´æ–°è³‡æ–™åº«ï¼Œå…ˆæŸ¥æ‰¾å·²å­˜åœ¨çš„ sitemap è¨˜éŒ„
            sitemap_record_id = None
            if update_db and self.db:
                try:
                    from database.client import SupabaseClient
                    db_client = SupabaseClient()
                    admin_client = db_client.get_admin_client()
                    
                    if admin_client:
                        existing = admin_client.table("sitemaps").select("id").eq("url", sitemap_url).execute()
                        if existing.data:
                            sitemap_record_id = existing.data[0]["id"]
                            # æ›´æ–°ç‹€æ…‹ç‚ºæ­£åœ¨çˆ¬å–
                            admin_client.table("sitemaps").update({"status": "crawling"}).eq("id", sitemap_record_id).execute()
                            print(f"ğŸ”„ æ›´æ–° sitemap ç‹€æ…‹ç‚ºçˆ¬å–ä¸­: {sitemap_url}")
                except Exception as e:
                    logger.warning(f"æ›´æ–° sitemap ç‹€æ…‹å¤±æ•—: {e}")
            
            # 2. ä¸‹è¼‰ Sitemap
            async with self.session.get(sitemap_url) as response:
                if response.status != 200:
                    error_msg = f"HTTP {response.status}"
                    if update_db and sitemap_record_id:
                        try:
                            admin_client.table("sitemaps").update({
                                "status": "error",
                                "error_message": error_msg
                            }).eq("id", sitemap_record_id).execute()
                        except Exception as e:
                            logger.warning(f"æ›´æ–° sitemap éŒ¯èª¤ç‹€æ…‹å¤±æ•—: {e}")
                    logger.error(f"ä¸‹è¼‰ Sitemap å¤±æ•—: {error_msg}")
                    return False, []
                    
                content = await response.text()
                
            # 3. è§£æ XML
            discovered_urls = self._parse_sitemap_xml(content, sitemap_url)
            
            # 4. æ‰¹é‡æ’å…¥ç™¼ç¾çš„ URL
            if discovered_urls and self.db:
                count = self.db.bulk_create_discovered_urls(discovered_urls)
                logger.info(f"å¾ Sitemap ç™¼ç¾ {len(discovered_urls)} å€‹ URLï¼ŒæˆåŠŸæ’å…¥ {count} å€‹")
                
                # æ›´æ–° Sitemap ç‹€æ…‹ç‚ºå®Œæˆ
                if update_db and sitemap_record_id:
                    try:
                        admin_client.table("sitemaps").update({
                            "status": "completed",
                            "urls_count": count,
                            "parsed_at": datetime.now().isoformat()
                        }).eq("id", sitemap_record_id).execute()
                        print(f"âœ… æ›´æ–° sitemap å®Œæˆç‹€æ…‹: {sitemap_url} ({count} å€‹ URLs)")
                    except Exception as e:
                        logger.warning(f"æ›´æ–° sitemap å®Œæˆç‹€æ…‹å¤±æ•—: {e}")
            
            self.stats["sitemaps_processed"] += 1
            self.stats["urls_discovered"] += len(discovered_urls)
            
            return True, discovered_urls
            
        except Exception as e:
            logger.error(f"è§£æ Sitemap æ™‚ç™¼ç”ŸéŒ¯èª¤ {sitemap_url}: {e}")
            if update_db and sitemap_record_id:
                try:
                    admin_client.table("sitemaps").update({
                        "status": "error",
                        "error_message": str(e)
                    }).eq("id", sitemap_record_id).execute()
                except Exception as e2:
                    logger.warning(f"æ›´æ–° sitemap éŒ¯èª¤ç‹€æ…‹å¤±æ•—: {e2}")
            self.stats["errors"] += 1
            return False, []
    
    def _parse_sitemap_xml(self, xml_content: str, sitemap_url: str) -> List[DiscoveredURLModel]:
        """
        è§£æ XML å…§å®¹ä¸¦æå– URL ä¿¡æ¯
        å®Œå…¨å°æ‡‰ discovered_urls è¡¨æ ¼çš„æ‰€æœ‰æ¬„ä½
        æ”¯æ´ <urlset> å’Œ <sitemapindex> å…©ç¨®æ ¼å¼
        """
        discovered_urls = []
        
        try:
            root = ET.fromstring(xml_content)
            
            # å®šç¾© XML å‘½åç©ºé–“
            namespaces = {
                'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'
            }
            
            # æª¢æŸ¥æ˜¯å¦ç‚º sitemapindex æ ¼å¼
            if root.tag.endswith('sitemapindex'):
                print(f"ğŸ“‹ æª¢æ¸¬åˆ° sitemap index æ ¼å¼: {sitemap_url}")
                # è™•ç† sitemapindex - æå–å­ sitemap URLs
                for sitemap_elem in root.findall('.//sitemap:sitemap', namespaces):
                    try:
                        loc_elem = sitemap_elem.find('sitemap:loc', namespaces)
                        if loc_elem is None or not loc_elem.text:
                            continue
                        
                        sub_sitemap_url = loc_elem.text.strip()
                        print(f"ğŸ”— ç™¼ç¾å­ sitemap: {sub_sitemap_url}")
                        
                        # ä½¿ç”¨åŒæ­¥æ–¹å¼è™•ç†å­ sitemapï¼Œé¿å…äº‹ä»¶å¾ªç’°è¡çª
                        try:
                            import requests
                            response = requests.get(sub_sitemap_url, timeout=10)
                            if response.status_code == 200:
                                sub_content = response.text
                                sub_urls = self._parse_sitemap_xml(sub_content, sub_sitemap_url)
                                discovered_urls.extend(sub_urls)
                                print(f"âœ… å¾å­ sitemap ç²å¾— {len(sub_urls)} å€‹ URLs: {sub_sitemap_url}")
                                
                                # é™åˆ¶è™•ç†çš„å­ sitemap æ•¸é‡ï¼Œé¿å…éå¤šè«‹æ±‚
                                if len(discovered_urls) >= 50:  # é™åˆ¶ç¸½æ•¸
                                    print(f"âš ï¸ å·²é”åˆ° URL é™åˆ¶ï¼Œåœæ­¢è™•ç†æ›´å¤šå­ sitemap")
                                    break
                            else:
                                print(f"âš ï¸ å­ sitemap è¨ªå•å¤±æ•— (HTTP {response.status_code}): {sub_sitemap_url}")
                        except Exception as e:
                            print(f"âš ï¸ è™•ç†å­ sitemap ç•°å¸¸: {sub_sitemap_url} - {e}")
                            
                    except Exception as e:
                        logger.warning(f"è§£æå­ sitemap å…ƒç´ æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        continue
                        
            else:
                # è™•ç†æ¨™æº– urlset æ ¼å¼
                print(f"ğŸ“„ æª¢æ¸¬åˆ° urlset æ ¼å¼: {sitemap_url}")
                # æŸ¥æ‰¾æ‰€æœ‰ URL å…ƒç´ 
                for url_elem in root.findall('.//sitemap:url', namespaces):
                    try:
                        # æå–å¿…è¦å­—æ®µ
                        loc_elem = url_elem.find('sitemap:loc', namespaces)
                        if loc_elem is None or not loc_elem.text:
                            continue
                        
                        url = loc_elem.text.strip()
                        
                        # æå–å¯é¸å­—æ®µ
                        priority = None
                        priority_elem = url_elem.find('sitemap:priority', namespaces)
                        if priority_elem is not None and priority_elem.text:
                            try:
                                priority = float(priority_elem.text)
                                # ç¢ºä¿åœ¨ 0.0-1.0 ç¯„åœå…§
                                priority = max(0.0, min(1.0, priority))
                            except ValueError:
                                priority = None
                        
                        changefreq = None
                        changefreq_elem = url_elem.find('sitemap:changefreq', namespaces)
                        if changefreq_elem is not None and changefreq_elem.text:
                            try:
                                changefreq = ChangeFreq(changefreq_elem.text.lower())
                            except ValueError:
                                changefreq = None
                        
                        lastmod = None
                        lastmod_elem = url_elem.find('sitemap:lastmod', namespaces)
                        if lastmod_elem is not None and lastmod_elem.text:
                            try:
                                # è§£æ ISO 8601 æ—¥æœŸæ ¼å¼
                                lastmod_str = lastmod_elem.text.strip()
                                if 'T' in lastmod_str:
                                    lastmod = datetime.fromisoformat(lastmod_str.replace('Z', '+00:00'))
                                else:
                                    lastmod = datetime.fromisoformat(lastmod_str)
                            except ValueError:
                                lastmod = None
                        
                        # å‰µå»º DiscoveredURL æ¨¡å‹
                        url_model = ModelFactory.create_discovered_url(
                            url=url,
                            source_sitemap=sitemap_url,
                            priority=priority,
                            changefreq=changefreq,
                            lastmod=lastmod,
                            crawl_status=CrawlStatus.PENDING,
                            metadata={
                                "discovered_from": "sitemap",
                                "sitemap_url": sitemap_url,
                                "discovered_at": datetime.now().isoformat()
                            }
                        )
                        
                        discovered_urls.append(url_model)
                        
                    except Exception as e:
                        logger.warning(f"è§£æå–®å€‹ URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        continue
            
            logger.info(f"æˆåŠŸè§£æ {len(discovered_urls)} å€‹ URL")
            return discovered_urls
            
        except ET.ParseError as e:
            logger.error(f"XML è§£æéŒ¯èª¤: {e}")
            return []
        except Exception as e:
            logger.error(f"è§£æ Sitemap XML æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    async def _parse_sub_sitemap(self, sub_sitemap_url: str) -> Tuple[bool, List[DiscoveredURLModel]]:
        """
        è§£æå­ sitemap æ–‡ä»¶
        """
        try:
            async with self.session.get(sub_sitemap_url) as response:
                if response.status != 200:
                    return False, []
                content = await response.text()
                
            # è§£æå­ sitemap å…§å®¹
            discovered_urls = self._parse_sitemap_xml(content, sub_sitemap_url)
            return True, discovered_urls
            
        except Exception as e:
            logger.warning(f"è§£æå­ sitemap å¤±æ•— {sub_sitemap_url}: {e}")
            return False, []
    
    async def crawl_url(self, url: str) -> Tuple[bool, Optional[ArticleModel]]:
        """
        çˆ¬å–å–®å€‹ URL ä¸¦å‰µå»ºæ–‡ç« è¨˜éŒ„
        
        Args:
            url: è¦çˆ¬å–çš„ URL
            
        Returns:
            Tuple[bool, Optional[ArticleModel]]: (æˆåŠŸæ¨™èªŒ, æ–‡ç« æ¨¡å‹)
        """
        try:
            logger.info(f"é–‹å§‹çˆ¬å– URL: {url}")
            
            # 1. æ›´æ–°çˆ¬å–ç‹€æ…‹
            if self.db:
                # æŸ¥æ‰¾å°æ‡‰çš„ discovered_url è¨˜éŒ„
                pending_urls = self.db.get_pending_urls(limit=1000)
                url_record = next((u for u in pending_urls if u.url == url), None)
                
                if url_record:
                    self.db.update_crawl_status(url_record.id, CrawlStatus.CRAWLING)
            
            # 2. ä¸‹è¼‰ç¶²é å…§å®¹
            async with self.session.get(url) as response:
                if response.status != 200:
                    error_msg = f"HTTP {response.status}"
                    if url_record and self.db:
                        self.db.update_crawl_status(url_record.id, CrawlStatus.ERROR, error_msg)
                    logger.error(f"ä¸‹è¼‰ç¶²é å¤±æ•—: {error_msg}")
                    return False, None
                
                html_content = await response.text()
            
            # 3. è§£æç¶²é å…§å®¹
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ¨™é¡Œ
            title = ""
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text().strip()
            
            # æå–ä¸»è¦å…§å®¹
            content = ""
            
            # å˜—è©¦ä¸åŒçš„å…§å®¹é¸æ“‡å™¨
            content_selectors = [
                'main', 'article', '.content', '#content', 
                '.main-content', '.article-content', '.post-content'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                    break
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œæå– body å…§å®¹
            if not content:
                body = soup.find('body')
                if body:
                    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                    for tag in body.find_all(['script', 'style', 'nav', 'header', 'footer']):
                        tag.decompose()
                    content = body.get_text(separator=' ', strip=True)
            
            # 4. å‰µå»ºæ–‡ç« æ¨¡å‹
            article_model = ModelFactory.create_article(
                url=url,
                title=title,
                content=content,
                crawled_from_url_id=url_record.id if url_record else None,
                metadata={
                    "crawled_at": datetime.now().isoformat(),
                    "content_length": len(content),
                    "title_length": len(title)
                }
            )
            
            # 5. ä¿å­˜æ–‡ç« 
            if self.db:
                success = self.db.create_article(article_model)
                if success:
                    # æ›´æ–°çˆ¬å–ç‹€æ…‹ç‚ºå®Œæˆ
                    if url_record:
                        self.db.update_crawl_status(url_record.id, CrawlStatus.COMPLETED)
                    
                    self.stats["articles_crawled"] += 1
                    logger.info(f"æˆåŠŸçˆ¬å–ä¸¦ä¿å­˜æ–‡ç« : {title}")
                    return True, article_model
                else:
                    if url_record:
                        self.db.update_crawl_status(url_record.id, CrawlStatus.ERROR, "ä¿å­˜æ–‡ç« å¤±æ•—")
                    return False, None
            
            return True, article_model
            
        except Exception as e:
            logger.error(f"çˆ¬å– URL æ™‚ç™¼ç”ŸéŒ¯èª¤ {url}: {e}")
            if url_record and self.db:
                self.db.update_crawl_status(url_record.id, CrawlStatus.ERROR, str(e))
            self.stats["errors"] += 1
            return False, None
    
    async def create_chunks(self, article: ArticleModel, chunk_size: int = 500) -> List[ChunkModel]:
        """
        å°‡æ–‡ç« å…§å®¹åˆ†å¡Š
        
        Args:
            article: æ–‡ç« æ¨¡å‹
            chunk_size: å¡Šå¤§å°ï¼ˆå­—ç¬¦æ•¸ï¼‰
            
        Returns:
            List[ChunkModel]: æ–‡ç« å¡Šåˆ—è¡¨
        """
        chunks = []
        
        if not article.content:
            return chunks
        
        content = article.content
        content_length = len(content)
        
        # ç°¡å–®åˆ†å¡Šç­–ç•¥
        for i in range(0, content_length, chunk_size):
            chunk_content = content[i:i + chunk_size]
            
            chunk_model = ModelFactory.create_chunk(
                article_id=article.id,
                content=chunk_content,
                chunk_index=len(chunks),
                metadata={
                    "chunk_method": "simple_split",
                    "chunk_size": chunk_size,
                    "created_at": datetime.now().isoformat()
                }
            )
            
            chunks.append(chunk_model)
        
        # ä¿å­˜å¡Š
        if chunks and self.db:
            count = self.db.create_chunks(chunks)
            self.stats["chunks_created"] += count
            logger.info(f"ç‚ºæ–‡ç«  {article.title} å‰µå»ºäº† {count} å€‹å¡Š")
        
        return chunks
    
    async def crawl_batch(self, urls: List[str], create_chunks: bool = True) -> Dict[str, Any]:
        """
        æ‰¹é‡çˆ¬å– URL
        
        Args:
            urls: URL åˆ—è¡¨
            create_chunks: æ˜¯å¦å‰µå»ºæ–‡ç« å¡Š
            
        Returns:
            Dict[str, Any]: çˆ¬å–çµæœçµ±è¨ˆ
        """
        if not urls:
            return {"success": 0, "failed": 0, "total": 0}
        
        logger.info(f"é–‹å§‹æ‰¹é‡çˆ¬å– {len(urls)} å€‹ URL")
        
        # ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä¸¦ç™¼
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def crawl_with_delay(url):
            async with semaphore:
                # æ·»åŠ å»¶é²
                if self.delay > 0:
                    await asyncio.sleep(self.delay)
                
                success, article = await self.crawl_url(url)
                
                # å‰µå»ºæ–‡ç« å¡Š
                if success and article and create_chunks:
                    await self.create_chunks(article)
                
                return success
        
        # åŸ·è¡Œæ‰¹é‡çˆ¬å–
        tasks = [crawl_with_delay(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµ±è¨ˆçµæœ
        success_count = sum(1 for r in results if r is True)
        failed_count = len(results) - success_count
        
        logger.info(f"æ‰¹é‡çˆ¬å–å®Œæˆ: æˆåŠŸ {success_count}, å¤±æ•— {failed_count}")
        
        return {
            "success": success_count,
            "failed": failed_count,
            "total": len(urls)
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çˆ¬èŸ²çµ±è¨ˆä¿¡æ¯"""
        stats = self.stats.copy()
        
        # æ·»åŠ è³‡æ–™åº«çµ±è¨ˆ
        if self.db:
            stats["db_counts"] = {
                "sitemaps": self.db.get_table_count("sitemaps"),
                "discovered_urls": self.db.get_table_count("discovered_urls"),
                "articles": self.db.get_table_count("articles"),
                "article_chunks": self.db.get_table_count("article_chunks")
            }
            
            # ç²å–çˆ¬å–é€²åº¦
            progress = self.db.get_crawl_progress()
            stats["progress"] = progress
        
        return stats

# ä¾¿æ·å‡½æ•¸
async def parse_sitemap_urls(sitemap_urls: List[str]) -> List[DiscoveredURLModel]:
    """
    ä¾¿æ·å‡½æ•¸ï¼šæ‰¹é‡è§£æå¤šå€‹ Sitemap
    
    Args:
        sitemap_urls: Sitemap URL åˆ—è¡¨
        
    Returns:
        List[DiscoveredURLModel]: æ‰€æœ‰ç™¼ç¾çš„ URL
    """
    all_urls = []
    
    async with RAGSpider() as spider:
        for sitemap_url in sitemap_urls:
            success, urls = await spider.parse_sitemap(sitemap_url)
            if success:
                all_urls.extend(urls)
    
    return all_urls

async def crawl_pending_urls(limit: int = 100, create_chunks: bool = True) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•¸ï¼šçˆ¬å–å¾…è™•ç†çš„ URL
    
    Args:
        limit: æœ€å¤§çˆ¬å–æ•¸é‡
        create_chunks: æ˜¯å¦å‰µå»ºæ–‡ç« å¡Š
        
    Returns:
        Dict[str, Any]: çˆ¬å–çµæœ
    """
    db = get_database_operations()
    if not db:
        return {"error": "ç„¡æ³•é€£æ¥è³‡æ–™åº«"}
    
    # ç²å–å¾…çˆ¬å–çš„ URL
    pending_urls = db.get_pending_urls(limit=limit)
    if not pending_urls:
        return {"message": "æ²’æœ‰å¾…çˆ¬å–çš„ URL", "total": 0}
    
    urls = [url.url for url in pending_urls]
    
    async with RAGSpider() as spider:
        result = await spider.crawl_batch(urls, create_chunks=create_chunks)
    
    return result


# å·¥ä½œæµç¨‹æ–¹æ³•
def discover_sitemaps(base_url: str, output_file: str = "sitemaps.txt") -> bool:
    """
    ç™¼ç¾ä¸¦ä¿å­˜ç¶²ç«™çš„sitemap (åŒæ™‚ä¿å­˜åˆ°æ–‡ä»¶å’Œè³‡æ–™åº«)
    
    Args:
        base_url: ç¶²ç«™åŸºç¤URL æˆ– robots.txt URL
        output_file: è¼¸å‡ºæ–‡ä»¶å
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        print(f"ğŸ” æ­£åœ¨ç™¼ç¾ {base_url} çš„ sitemaps...")
        
        sitemap_urls = []
        
        # å¦‚æœURLå·²ç¶“æŒ‡å‘robots.txtï¼Œç›´æ¥ä½¿ç”¨
        if base_url.endswith('/robots.txt'):
            robots_url = base_url
            # å¾robots.txt URLæ¨å°åŸºç¤URL
            site_base = base_url.replace('/robots.txt', '')
        else:
            # å¦å‰‡æ§‹å»ºrobots.txt URL
            robots_url = urljoin(base_url, "/robots.txt")
            site_base = base_url
        
        print(f"ğŸ“– è®€å– robots.txt: {robots_url}")
        
        # å˜—è©¦ä¸‹è¼‰å’Œè§£ærobots.txt
        import requests
        try:
            response = requests.get(robots_url, timeout=10)
            if response.status_code == 200:
                robots_content = response.text
                print(f"âœ… æˆåŠŸä¸‹è¼‰ robots.txt ({len(robots_content)} å­—ç¬¦)")
                
                # è§£ærobots.txtä¸­çš„Sitemapæ¢ç›®
                for line in robots_content.split('\n'):
                    line = line.strip()
                    if line.lower().startswith('sitemap:'):
                        sitemap_url = line.split(':', 1)[1].strip()
                        if sitemap_url:
                            sitemap_urls.append(sitemap_url)
                            print(f"ğŸ—ºï¸ åœ¨ robots.txt ä¸­ç™¼ç¾ sitemap: {sitemap_url}")
                
                if not sitemap_urls:
                    print("âš ï¸ robots.txt ä¸­æ²’æœ‰æ‰¾åˆ° sitemap æ¢ç›®ï¼Œä½¿ç”¨é è¨­ä½ç½®")
                    # å¦‚æœrobots.txtä¸­æ²’æœ‰sitemapï¼Œä½¿ç”¨å¸¸è¦‹ä½ç½®
                    sitemap_urls = [
                        urljoin(site_base, "/sitemap.xml"),
                        urljoin(site_base, "/sitemap_index.xml")
                    ]
            else:
                print(f"âš ï¸ ç„¡æ³•ä¸‹è¼‰ robots.txt (HTTP {response.status_code})ï¼Œä½¿ç”¨é è¨­sitemapä½ç½®")
                sitemap_urls = [
                    urljoin(site_base, "/sitemap.xml"),
                    urljoin(site_base, "/sitemap_index.xml")
                ]
        except Exception as e:
            print(f"âš ï¸ è®€å– robots.txt å¤±æ•—: {e}")
            print("ä½¿ç”¨é è¨­ sitemap ä½ç½®")
            sitemap_urls = [
                urljoin(site_base, "/sitemap.xml"),
                urljoin(site_base, "/sitemap_index.xml")
            ]
        
        # å°‡URLsä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            for url in sitemap_urls:
                f.write(f"{url}\n")
        
        print(f"âœ… Sitemap URLs å·²ä¿å­˜åˆ° {output_file}")
        
        # é©—è­‰ä¸¦ä¿å­˜çœŸå¯¦å­˜åœ¨çš„sitemapåˆ°è³‡æ–™åº«
        valid_sitemaps = []
        print("ğŸ” é©—è­‰ sitemap æ˜¯å¦å­˜åœ¨...")
        
        import requests
        for sitemap_url in sitemap_urls:
            try:
                response = requests.get(sitemap_url, timeout=10)
                if response.status_code == 200:
                    # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ XML sitemap
                    content = response.text.strip()
                    if content and ('<urlset' in content or '<sitemapindex' in content):
                        valid_sitemaps.append(sitemap_url)
                        print(f"âœ… æœ‰æ•ˆçš„ sitemap: {sitemap_url}")
                    else:
                        print(f"âš ï¸ ä¸æ˜¯æœ‰æ•ˆçš„ sitemap XML: {sitemap_url}")
                else:
                    print(f"âŒ Sitemap ä¸å­˜åœ¨ (HTTP {response.status_code}): {sitemap_url}")
            except Exception as e:
                print(f"âŒ ç„¡æ³•è¨ªå• sitemap: {sitemap_url} - {e}")
        
        # åªå°‡é©—è­‰éçš„æœ‰æ•ˆsitemapä¿å­˜åˆ°è³‡æ–™åº«
        if valid_sitemaps:
            try:
                from database.client import SupabaseClient
                
                db_client = SupabaseClient()
                admin_client = db_client.get_admin_client()
                
                if admin_client:
                    saved_count = 0
                    
                    for sitemap_url in valid_sitemaps:
                        try:
                            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                            existing = admin_client.table("sitemaps").select("id").eq("url", sitemap_url).execute()
                            
                            if not existing.data:
                                # å¾ URL ä¸­æå– domain
                                parsed_url = urlparse(sitemap_url)
                                domain = parsed_url.netloc
                                
                                # æ’å…¥æ–°è¨˜éŒ„
                                sitemap_data = {
                                    "url": sitemap_url,
                                    "domain": domain,
                                    "status": "pending",
                                    "metadata": {
                                        "discovered_from": "robots_txt" if base_url.endswith('/robots.txt') else "auto_discovery",
                                        "base_url": base_url,
                                        "verified_at": datetime.now().isoformat(),
                                        "discovered_at": datetime.now().isoformat()
                                    }
                                }
                                
                                result = admin_client.table("sitemaps").insert(sitemap_data).execute()
                                
                                if result.data:
                                    saved_count += 1
                                    print(f"ğŸ“ å·²è¨˜éŒ„åˆ°è³‡æ–™åº«: {sitemap_url}")
                                else:
                                    print(f"âš ï¸ è¨˜éŒ„å¤±æ•—: {sitemap_url}")
                            else:
                                print(f"â„¹ï¸ å·²å­˜åœ¨æ–¼è³‡æ–™åº«: {sitemap_url}")
                                saved_count += 1
                                
                        except Exception as e:
                            logger.error(f"âŒ ä¿å­˜sitemapåˆ°è³‡æ–™åº«å¤±æ•— {sitemap_url}: {e}")
                            print(f"âš ï¸ è·³éè³‡æ–™åº«è¨˜éŒ„: {sitemap_url} - {e}")
                    
                    print(f"ğŸ’¾ è³‡æ–™åº«è¨˜éŒ„: {saved_count}/{len(valid_sitemaps)} å€‹æœ‰æ•ˆçš„ sitemap URLs")
                else:
                    print("âš ï¸ ç„¡æ³•ç²å–ç®¡ç†å“¡å®¢æˆ¶ç«¯ï¼Œè·³éè³‡æ–™åº«è¨˜éŒ„")
                    
            except Exception as e:
                logger.error(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
                print("âš ï¸ ç„¡æ³•é€£æ¥åˆ°è³‡æ–™åº«ï¼Œåƒ…ä¿å­˜åˆ°æ–‡ä»¶")
        else:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ sitemapï¼Œä¸è¨˜éŒ„åˆ°è³‡æ–™åº«")
        
        print(f"ğŸ“ ç¸½å…±ç™¼ç¾ {len(sitemap_urls)} å€‹æ½›åœ¨çš„ sitemap URLs")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç™¼ç¾ sitemap å¤±æ•—: {e}")
        print(f"âŒ ç™¼ç¾ sitemap å¤±æ•—: {e}")
        return False


def extract_urls_from_sitemaps(sitemap_list_file: str, output_file: str = "urls.txt", max_urls: int = 1000) -> bool:
    """
    å¾sitemapæ–‡ä»¶åˆ—è¡¨ä¸­æå–URLs (åŒæ™‚ä¿å­˜åˆ°æ–‡ä»¶å’Œè³‡æ–™åº«)
    
    Args:
        sitemap_list_file: sitemapåˆ—è¡¨æ–‡ä»¶
        output_file: è¼¸å‡ºURLæ–‡ä»¶
        max_urls: æœ€å¤§URLæ•¸é‡
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        print(f"ğŸ“– æ­£åœ¨è®€å– sitemap åˆ—è¡¨: {sitemap_list_file}")
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(sitemap_list_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {sitemap_list_file}")
            return False
        
        # è®€å–sitemapåˆ—è¡¨
        with open(sitemap_list_file, 'r', encoding='utf-8') as f:
            sitemap_urls = [line.strip() for line in f if line.strip()]
        
        print(f"ğŸ—ºï¸ æ‰¾åˆ° {len(sitemap_urls)} å€‹ sitemap URLs")
        
        if not sitemap_urls:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„sitemap URLs")
            return False
        
        # é‹è¡Œç•°æ­¥å‡½æ•¸æå–URLs
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            print("ğŸ”„ é–‹å§‹è§£æ sitemaps...")
            discovered_urls = loop.run_until_complete(
                parse_sitemap_urls(sitemap_urls[:3])  # é™åˆ¶è™•ç†æ•¸é‡
            )
            
            print(f"ğŸ”— å¾ sitemaps ä¸­ç™¼ç¾ {len(discovered_urls)} å€‹ URLs")
            
            # é™åˆ¶URLæ•¸é‡
            if len(discovered_urls) > max_urls:
                discovered_urls = discovered_urls[:max_urls]
                print(f"âš ï¸ é™åˆ¶URLæ•¸é‡ç‚º {max_urls}")
            
            # ä¿å­˜URLsåˆ°æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                for url_model in discovered_urls:
                    f.write(f"{url_model.url}\n")
            
            print(f"âœ… {len(discovered_urls)} å€‹ URLs å·²ä¿å­˜åˆ°æ–‡ä»¶ {output_file}")
            
            # æª¢æŸ¥è³‡æ–™åº«ä¸­çš„è¨˜éŒ„ç‹€æ…‹
            try:
                from database.client import SupabaseClient
                
                db_client = SupabaseClient()
                
                # ç²å–è³‡æ–™åº«çµ±è¨ˆ
                discovered_urls_result = db_client.table("discovered_urls").select("id", count="exact").execute()
                db_url_count = discovered_urls_result.count if discovered_urls_result.count is not None else 0
                print(f"ğŸ’¾ è³‡æ–™åº«ä¸­å·²æœ‰ {db_url_count} å€‹ discovered URLs è¨˜éŒ„")
                
                # é¡¯ç¤ºçˆ¬å–ç‹€æ…‹çµ±è¨ˆ
                try:
                    pending_result = db_client.table("discovered_urls").select("id", count="exact").eq("crawl_status", "pending").execute()
                    pending_count = pending_result.count if pending_result.count is not None else 0
                    completed_count = db_url_count - pending_count
                    print(f"ğŸ“Š çˆ¬å–ç‹€æ…‹: å¾…è™•ç† {pending_count} å€‹, å·²å®Œæˆ {completed_count} å€‹")
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•ç²å–çˆ¬å–ç‹€æ…‹: {e}")
                    
            except Exception as e:
                logger.error(f"âŒ è³‡æ–™åº«ç‹€æ…‹æª¢æŸ¥å¤±æ•—: {e}")
                print("âš ï¸ ç„¡æ³•é€£æ¥åˆ°è³‡æ–™åº«é€²è¡Œç‹€æ…‹æª¢æŸ¥")
            
            return True
            
        finally:
            loop.close()
        
    except Exception as e:
        logger.error(f"âŒ æå– URLs å¤±æ•—: {e}")
        print(f"âŒ æå– URLs å¤±æ•—: {e}")
        return False


def crawl_and_chunk_urls(url_list_file: str, chunk_size: int = 200) -> bool:
    """
    çˆ¬å–URLåˆ—è¡¨ä¸­çš„ç¶²é ä¸¦é€²è¡Œåˆ†å¡Š
    
    Args:
        url_list_file: URLåˆ—è¡¨æ–‡ä»¶
        chunk_size: åˆ†å¡Šå¤§å°
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        print(f"ğŸ“„ æ­£åœ¨è®€å– URL åˆ—è¡¨: {url_list_file}")
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(url_list_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {url_list_file}")
            return False
        
        # è®€å–URLåˆ—è¡¨
        with open(url_list_file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        
        print(f"ğŸ”— æ‰¾åˆ° {len(urls)} å€‹ URLs å¾…çˆ¬å–")
        
        if not urls:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URLs")
            return False
        
        # é™åˆ¶çˆ¬å–æ•¸é‡ä»¥é¿å…éè¼‰
        max_crawl = min(len(urls), 5)  # é™åˆ¶ç‚º5å€‹URL
        urls_to_crawl = urls[:max_crawl]
        
        print(f"ğŸš€ é–‹å§‹çˆ¬å–å‰ {max_crawl} å€‹ URLs...")
        
        # é‹è¡Œç•°æ­¥çˆ¬å–
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            async def run_crawl():
                async with RAGSpider() as spider:
                    return await spider.crawl_batch(urls_to_crawl, create_chunks=True)
            
            results = loop.run_until_complete(run_crawl())
            
            print(f"âœ… çˆ¬å–å®Œæˆ!")
            print(f"ğŸ“Š æˆåŠŸ: {results.get('success', 0)}, å¤±æ•—: {results.get('failed', 0)}")
            print(f"ğŸ“„ ç¸½è¨ˆ: {results.get('total', 0)}")
            
            # é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆ
            try:
                from database.client import SupabaseClient
                
                db_client = SupabaseClient()
                
                # ç²å–æ–‡ç« å’Œæ–‡ç« å¡Šçµ±è¨ˆ
                articles_result = db_client.table("articles").select("id", count="exact").execute()
                articles_count = articles_result.count if articles_result.count is not None else 0
                
                chunks_result = db_client.table("article_chunks").select("id", count="exact").execute()
                chunks_count = chunks_result.count if chunks_result.count is not None else 0
                
                print(f"ğŸ’¾ è³‡æ–™åº«è¨˜éŒ„: {articles_count} ç¯‡æ–‡ç« , {chunks_count} å€‹æ–‡ç« å¡Š")
                
            except Exception as e:
                logger.error(f"âŒ è³‡æ–™åº«çµ±è¨ˆç²å–å¤±æ•—: {e}")
                print(f"âš ï¸ ç„¡æ³•ç²å–è³‡æ–™åº«çµ±è¨ˆ: {e}")
            
            return True
            
        finally:
            loop.close()
        
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–å’Œåˆ†å¡Šå¤±æ•—: {e}")
        print(f"âŒ çˆ¬å–å¤±æ•—: {e}")
        return False
