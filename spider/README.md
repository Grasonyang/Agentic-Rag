# ğŸ•·ï¸ Spider æ¨¡çµ„ä½¿ç”¨æŒ‡å—

Spider æ¨¡çµ„æ˜¯ Agentic RAG æ¡†æ¶çš„æ ¸å¿ƒçˆ¬èŸ²å¼•æ“ï¼Œæä¾›äº†é«˜æ•ˆã€å¯æ“´å±•çš„ç¶²é çˆ¬å–å’Œæ–‡æœ¬è™•ç†åŠŸèƒ½ã€‚

## ğŸ—ï¸ æ¨¡çµ„æ¶æ§‹

```
spider/
â”œâ”€â”€ __init__.py                 # æ¨¡çµ„åˆå§‹åŒ–
â”œâ”€â”€ crawlers/                   # çˆ¬èŸ²å¯¦ç¾
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ web_crawler.py          # åŸºæ–¼ crawl4ai çš„é«˜æ€§èƒ½çˆ¬èŸ²
â”‚   â”œâ”€â”€ sitemap_parser.py       # Sitemap è§£æå’Œ URL æå–
â”‚   â”œâ”€â”€ url_scheduler.py        # URL æ’ç¨‹å™¨
â”‚   â””â”€â”€ progressive_crawler.py  # æ¼¸é€²å¼çˆ¬èŸ²
â”œâ”€â”€ chunking/                   # æ–‡æœ¬åˆ†å¡Šç­–ç•¥
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_chunker.py         # åŸºç¤åˆ†å¡Šå™¨æŠ½è±¡é¡
â”‚   â”œâ”€â”€ chunker_factory.py      # åˆ†å¡Šå™¨å·¥å» æ¨¡å¼
â”‚   â”œâ”€â”€ semantic_chunking.py    # èªç¾©ç›¸ä¼¼åº¦åˆ†å¡Š (å¯¦é©—æ€§)
â”‚   â”œâ”€â”€ sentence_chunking.py    # åŸºæ–¼å¥å­é‚Šç•Œçš„æ™ºèƒ½åˆ†å¡Š
â”‚   â””â”€â”€ sliding_window.py       # æ»‘å‹•çª—å£åˆ†å¡Šï¼Œä¿æŒä¸Šä¸‹æ–‡
â””â”€â”€ utils/                      # å·¥å…·æ¨¡çµ„
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ rate_limiter.py         # ä»¤ç‰Œæ¡¶ç®—æ³•é™é€Ÿå™¨
    â””â”€â”€ retry_manager.py        # æŒ‡æ•¸é€€é¿é‡è©¦æ©Ÿåˆ¶
```

## ğŸ”„ çˆ¬èŸ²æµç¨‹

è®€å– `robots.txt` â†’ `stream_discover` â†’ `URLScheduler` â†’ `ProgressiveCrawler`

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### 1. çˆ¬èŸ²å¼•æ“ (`crawlers/`)

#### WebCrawler - é«˜æ€§èƒ½çˆ¬èŸ²
åŸºæ–¼ `crawl4ai` çš„ç¾ä»£åŒ–çˆ¬èŸ²ï¼Œæ”¯æ´ JavaScript æ¸²æŸ“å’Œè¤‡é›œé é¢è™•ç†ã€‚

**ç‰¹é»:**
- ğŸŒ æ”¯æ´ JavaScript åŸ·è¡Œå’Œ SPA æ‡‰ç”¨
- ğŸ”„ è‡ªå‹•é‡è©¦æ©Ÿåˆ¶å’ŒéŒ¯èª¤è™•ç†
- âš¡ ç•°æ­¥ä½µç™¼è™•ç†
- ğŸ›¡ï¸ å…§å»ºååçˆ¬èŸ²æ©Ÿåˆ¶
- ğŸ“Š è©³ç´°çš„æ€§èƒ½ç›£æ§

#### SitemapParser - Sitemap è§£æå™¨
å°ˆé–€è™•ç† Sitemap æ–‡ä»¶çš„è§£æå’ŒURLç™¼ç¾ã€‚

**ç‰¹é»:**
- ğŸ—ºï¸ æ”¯æ´ XML Sitemap å’Œ Sitemap Index
- ğŸ” éæ­¸è§£æå¤šå±¤ Sitemap
- ğŸ“… æ”¯æ´ lastmodã€changefreqã€priority å±¬æ€§
- ğŸ¤– éµå¾ª robots.txt è¦ç¯„

#### URLScheduler - URL æ’ç¨‹å™¨
è² è²¬ç®¡ç†å¾…è™•ç† URL çš„ä½‡åˆ—èˆ‡ç‹€æ…‹ã€‚

**ç‰¹é»:**
- ğŸ§® å„ªå…ˆç´šæ’ç¨‹èˆ‡å»é‡
- ğŸ“‹ æ”¯æ´è³‡æ–™åº«æˆ–å¤–éƒ¨å„²å­˜
- ğŸ”„ èˆ‡çˆ¬èŸ²æµç¨‹ç·Šå¯†æ•´åˆ

#### ProgressiveCrawler - æ¼¸é€²å¼çˆ¬èŸ²
å¾ URLScheduler å–å¾—æ‰¹æ¬¡ URLï¼Œæ¼¸é€²å¼æ“´å±•çˆ¬å–ç¯„åœã€‚

**ç‰¹é»:**
- â›“ï¸ é€é `stream_discover` éˆå¼ç™¼ç¾
- ğŸ“¶ å¯èª¿æ•´æ‰¹æ¬¡èˆ‡ä½µç™¼
- ğŸ“ˆ æŒçºŒæ›´æ–°çˆ¬å–ç‹€æ…‹

### 2. æ–‡æœ¬åˆ†å¡Š (`chunking/`)

#### åˆ†å¡Šç­–ç•¥æ¦‚è¦½

| åˆ†å¡Šå™¨ | é©ç”¨å ´æ™¯ | ç‰¹é» | æ¨è–¦ç”¨é€” |
|--------|----------|------|----------|
| `SlidingWindowChunker` | ä¿æŒä¸Šä¸‹æ–‡é€£çºŒæ€§ | é‡ç–Šåˆ†å¡Šï¼Œé¿å…èªç¾©å‰²è£‚ | é•·æ–‡ç« ã€æŠ€è¡“æ–‡æª” |
| `SentenceChunker` | è‡ªç„¶èªè¨€é‚Šç•Œ | åŸºæ–¼å¥å­é‚Šç•Œï¼Œèªç¾©å®Œæ•´ | æ–°èæ–‡ç« ã€éƒ¨è½æ ¼ |
| `SemanticChunker` | èªç¾©ç›¸é—œæ€§ | åŸºæ–¼èªç¾©ç›¸ä¼¼åº¦åˆ†çµ„ | å­¸è¡“è«–æ–‡ã€ç ”ç©¶å ±å‘Š |

#### å·¥å» æ¨¡å¼ä½¿ç”¨
```python
from spider.chunking.chunker_factory import ChunkerFactory

# å‰µå»ºåˆ†å¡Šå™¨
chunker = ChunkerFactory.create_chunker('sliding_window', {
    'window_size': 100,
    'step_size': 50
})

# åŸ·è¡Œåˆ†å¡Š
chunks = chunker.chunk_text("å¾ˆé•·çš„æ–‡ç« å…§å®¹...")
```

### 3. å·¥å…·æ¨¡çµ„ (`utils/`)

#### RateLimiter - é€Ÿç‡é™åˆ¶å™¨
åŸºæ–¼ä»¤ç‰Œæ¡¶ç®—æ³•çš„é«˜æ•ˆé™é€Ÿå™¨ã€‚

**ç‰¹é»:**
- ğŸª£ ä»¤ç‰Œæ¡¶ç®—æ³•å¯¦ç¾
- âš¡ æ”¯æ´çªç™¼æµé‡
- ğŸ”§ å‹•æ…‹é€Ÿç‡èª¿æ•´
- ğŸ“Š å¯¦æ™‚çµ±è¨ˆç›£æ§

#### RetryManager - é‡è©¦ç®¡ç†å™¨
æ™ºèƒ½é‡è©¦æ©Ÿåˆ¶ï¼Œæ”¯æ´æŒ‡æ•¸é€€é¿å’Œå¤šç¨®å¤±æ•—æ¢ä»¶è™•ç†ã€‚

**ç‰¹é»:**
- ğŸ“ˆ æŒ‡æ•¸é€€é¿ç®—æ³•
- ğŸ¯ æ¢ä»¶å¼é‡è©¦
- ğŸ“ è©³ç´°éŒ¯èª¤è¨˜éŒ„
- â° è‡ªå®šç¾©è¶…æ™‚è™•ç†

## ğŸ’¡ åŸºæœ¬ä½¿ç”¨

### 1. ç°¡å–®çˆ¬å–ç¤ºä¾‹

```python
import asyncio
from spider.crawlers.web_crawler import WebCrawler

async def basic_crawl():
    crawler = WebCrawler()

    # çˆ¬å–å–®ä¸€ URL
    result = await crawler.crawl("https://example.com")

    if result['success']:
        print(f"æ¨™é¡Œ: {result['title']}")
        print(f"å…§å®¹é•·åº¦: {len(result['content'])}")
    else:
        print(f"çˆ¬å–å¤±æ•—: {result['error']}")

# åŸ·è¡Œ
asyncio.run(basic_crawl())
```

### 2. æ‰¹é‡çˆ¬å–ç¤ºä¾‹

```python
async def batch_crawl():
    crawler = WebCrawler()

    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com"
    ]

    # é€ä¸€çˆ¬å–
    results = []
    for u in urls:
        results.append(await crawler.crawl(u))

    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]

    print(f"æˆåŠŸ: {len(successful)}, å¤±æ•—: {len(failed)}")

    # é¡¯ç¤ºçµæœçµ±è¨ˆ
    for result in successful:
        print(f"âœ… {result['title'][:50]}...")

    for result in failed:
        print(f"âŒ {result['error']}")

asyncio.run(batch_crawl())
```

### 3. Sitemap è§£æç¤ºä¾‹

```python
from spider.crawlers.sitemap_parser import SitemapParser

async def sitemap_crawl():
    parser = SitemapParser()
    
    # å¾ robots.txt ç™¼ç¾ Sitemap
    robots_url = "https://example.com/robots.txt"
    sitemaps = await parser.discover_sitemaps_from_robots(robots_url)
    
    print(f"ç™¼ç¾ {len(sitemaps)} å€‹ Sitemap:")
    for sitemap in sitemaps:
        print(f"- {sitemap}")
    
    # è§£æ Sitemap ç²å– URL
    all_urls = []
    for sitemap_url in sitemaps:
        urls = await parser.parse_sitemap(sitemap_url)
        all_urls.extend(urls)
        print(f"å¾ {sitemap_url} ç²å– {len(urls)} å€‹ URL")
    
    print(f"ç¸½å…±ç™¼ç¾ {len(all_urls)} å€‹ URL")
    
    # é¡¯ç¤ºå‰ 10 å€‹ URL
    for i, url_info in enumerate(all_urls[:10], 1):
        print(f"{i}. {url_info['url']} (å„ªå…ˆç´š: {url_info.get('priority', 'N/A')})")

asyncio.run(sitemap_crawl())
```

## ğŸ”§ é€²éšé…ç½®

### 1. çˆ¬èŸ²é…ç½®

```python
from spider.crawlers.web_crawler import WebCrawler
from spider.utils.rate_limiter import RateLimiter

# å‰µå»ºè‡ªå®šç¾©é™é€Ÿå™¨
rate_limiter = RateLimiter(
    requests_per_second=2.0,  # æ¯ç§’ 2 å€‹è«‹æ±‚
    burst_size=5              # å…è¨±çªç™¼ 5 å€‹è«‹æ±‚
)

# é…ç½®çˆ¬èŸ²
crawler = WebCrawler(
    headless=True,                    # ç„¡é ­æ¨¡å¼
    user_agent="CustomBot/1.0",       # è‡ªå®šç¾© User-Agent
    timeout=30000,                    # 30 ç§’è¶…æ™‚
    delay=2.5,                        # è«‹æ±‚é–“éš” 2.5 ç§’
    rate_limiter=rate_limiter,        # ä½¿ç”¨è‡ªå®šç¾©é™é€Ÿå™¨
    max_retries=3                     # æœ€å¤§é‡è©¦æ¬¡æ•¸
)

# è¨­ç½®é¡å¤–é¸é …
crawler.set_viewport(1920, 1080)     # è¨­ç½®è¦–çª—å¤§å°
crawler.set_cookies(cookies_dict)    # è¨­ç½® cookies
crawler.add_headers({                # æ·»åŠ è‡ªå®šç¾© headers
    "Accept-Language": "zh-TW,zh;q=0.9",
    "Accept-Encoding": "gzip, deflate"
})
```

### 2. åˆ†å¡Šå™¨é…ç½®

```python
from spider.chunking.sliding_window import SlidingWindowChunker
from spider.chunking.sentence_chunking import SentenceChunker

# æ»‘å‹•çª—å£åˆ†å¡Šå™¨
sliding_chunker = SlidingWindowChunker(
    window_size=150,    # æ¯å¡Š 150 å€‹å­—ç¬¦
    step_size=75,       # æ­¥é•· 75 å€‹å­—ç¬¦ (50% é‡ç–Š)
    min_chunk_size=50   # æœ€å°å¡Šå¤§å°
)

# å¥å­åˆ†å¡Šå™¨
sentence_chunker = SentenceChunker(
    max_chunk_size=200,     # æœ€å¤§å¡Šå¤§å°
    min_chunk_size=50,      # æœ€å°å¡Šå¤§å°
    overlap_sentences=1     # é‡ç–Š 1 å€‹å¥å­
)

# ä½¿ç”¨åˆ†å¡Šå™¨
text = "å¾ˆé•·çš„æ–‡ç« å…§å®¹..."

sliding_chunks = sliding_chunker.chunk_text(text)
sentence_chunks = sentence_chunker.chunk_text(text)

print(f"æ»‘å‹•çª—å£åˆ†å¡Š: {len(sliding_chunks)} å¡Š")
print(f"å¥å­åˆ†å¡Š: {len(sentence_chunks)} å¡Š")
```

### 3. éŒ¯èª¤è™•ç†å’Œé‡è©¦

```python
from spider.utils.retry_manager import RetryManager
from spider.crawlers.web_crawler import WebCrawler, CrawlError

crawler = WebCrawler()

# å‰µå»ºé‡è©¦ç®¡ç†å™¨
retry_manager = RetryManager(
    max_retries=5,           # æœ€å¤§é‡è©¦ 5 æ¬¡
    base_delay=1.0,         # åŸºç¤å»¶é² 1 ç§’
    max_delay=60.0,         # æœ€å¤§å»¶é² 60 ç§’
    exponential_base=2.0    # æŒ‡æ•¸é€€é¿ä¿‚æ•¸
)

async def robust_crawl(url):
    """å¸¶é‡è©¦çš„çˆ¬å–å‡½æ•¸"""
    
    @retry_manager.retry_on_exception(
        exceptions=(CrawlError, TimeoutError),
        exclude_exceptions=(ValueError,)  # ä¸é‡è©¦çš„ä¾‹å¤–
    )
    async def _crawl():
        return await crawler.crawl(url)
    
    try:
        return await _crawl()
    except Exception as e:
        print(f"çˆ¬å– {url} æœ€çµ‚å¤±æ•—: {e}")
        return {'success': False, 'error': str(e)}

# ä½¿ç”¨
result = await robust_crawl("https://difficult-site.com")
```

## ğŸ› ï¸ è‡ªå®šç¾©æ“´å±•

### 1. è‡ªå®šç¾©çˆ¬èŸ²

```python
from spider.crawlers.web_crawler import WebCrawler

class CustomWebCrawler(WebCrawler):
    """è‡ªå®šç¾©çˆ¬èŸ²ï¼Œæ”¯æ´ç‰¹æ®Šç¶²ç«™"""
    
    async def pre_process_url(self, url: str) -> str:
        """URL é è™•ç†"""
        # æ·»åŠ å¿…è¦çš„åƒæ•¸
        if "special-site.com" in url:
            return f"{url}?format=full"
        return url
    
    async def post_process_content(self, content: str, url: str) -> str:
        """å…§å®¹å¾Œè™•ç†"""
        # ç§»é™¤ç‰¹å®šçš„ç„¡ç”¨å…§å®¹
        if "advertisement" in content.lower():
            # ç§»é™¤å»£å‘Šå…§å®¹çš„é‚è¼¯
            content = self.remove_ads(content)
        
        return content
    
    def remove_ads(self, content: str) -> str:
        """ç§»é™¤å»£å‘Šå…§å®¹"""
        # å¯¦ç¾å»£å‘Šç§»é™¤é‚è¼¯
        import re
        ad_patterns = [
            r'<div class="ad-.*?</div>',
            r'<!-- AD START -->.*?<!-- AD END -->',
        ]
        
        for pattern in ad_patterns:
            content = re.sub(pattern, '', content, flags=re.DOTALL)
        
        return content

# ä½¿ç”¨è‡ªå®šç¾©çˆ¬èŸ²
custom_crawler = CustomWebCrawler()
result = await custom_crawler.crawl("https://special-site.com/article")
```

### 2. è‡ªå®šç¾©åˆ†å¡Šå™¨

```python
from spider.chunking.base_chunker import BaseChunker
from typing import List
import re

class ParagraphChunker(BaseChunker):
    """åŸºæ–¼æ®µè½çš„åˆ†å¡Šå™¨"""
    
    def __init__(self, max_paragraphs_per_chunk: int = 3, min_chunk_size: int = 100):
        self.max_paragraphs_per_chunk = max_paragraphs_per_chunk
        self.min_chunk_size = min_chunk_size
    
    def chunk_text(self, text: str) -> List[str]:
        """åŸºæ–¼æ®µè½é€²è¡Œåˆ†å¡Š"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text.strip())
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        chunks = []
        current_chunk = []
        current_size = 0
        
        for paragraph in paragraphs:
            # å¦‚æœæ·»åŠ é€™å€‹æ®µè½æœƒè¶…éé™åˆ¶ï¼Œå…ˆä¿å­˜ç•¶å‰å¡Š
            if (len(current_chunk) >= self.max_paragraphs_per_chunk or
                current_size + len(paragraph) > self.min_chunk_size * 3):
                
                if current_chunk:
                    chunk_text = '\n\n'.join(current_chunk)
                    if len(chunk_text) >= self.min_chunk_size:
                        chunks.append(chunk_text)
                    current_chunk = []
                    current_size = 0
            
            current_chunk.append(paragraph)
            current_size += len(paragraph)
        
        # è™•ç†æœ€å¾Œä¸€å¡Š
        if current_chunk:
            chunk_text = '\n\n'.join(current_chunk)
            if len(chunk_text) >= self.min_chunk_size:
                chunks.append(chunk_text)
        
        return chunks

# è¨»å†Šè‡ªå®šç¾©åˆ†å¡Šå™¨
from spider.chunking.chunker_factory import ChunkerFactory
ChunkerFactory.register('paragraph', ParagraphChunker)

# ä½¿ç”¨
chunker = ChunkerFactory.create_chunker('paragraph', {
    'max_paragraphs_per_chunk': 2,
    'min_chunk_size': 150
})
```

### 3. è‡ªå®šç¾©é™é€Ÿå™¨

```python
from spider.utils.rate_limiter import RateLimiter
import time
import random

class AdaptiveRateLimiter(RateLimiter):
    """è‡ªé©æ‡‰é™é€Ÿå™¨ï¼Œæ ¹æ“šéŸ¿æ‡‰æ™‚é–“èª¿æ•´é€Ÿç‡"""
    
    def __init__(self, initial_rps: float = 1.0, min_rps: float = 0.1, max_rps: float = 10.0):
        super().__init__(initial_rps, burst_size=5)
        self.min_rps = min_rps
        self.max_rps = max_rps
        self.response_times = []
        self.adjustment_threshold = 10  # æ¯ N æ¬¡è«‹æ±‚èª¿æ•´ä¸€æ¬¡
        
    async def acquire(self) -> bool:
        """ç²å–ä»¤ç‰Œä¸¦è¨˜éŒ„"""
        acquired = await super().acquire()
        
        # æ¯éš”ä¸€æ®µæ™‚é–“èª¿æ•´é€Ÿç‡
        if len(self.response_times) >= self.adjustment_threshold:
            self._adjust_rate()
            self.response_times = []
        
        return acquired
    
    def record_response_time(self, response_time: float):
        """è¨˜éŒ„éŸ¿æ‡‰æ™‚é–“"""
        self.response_times.append(response_time)
    
    def _adjust_rate(self):
        """æ ¹æ“šéŸ¿æ‡‰æ™‚é–“èª¿æ•´é€Ÿç‡"""
        if not self.response_times:
            return
        
        avg_response_time = sum(self.response_times) / len(self.response_times)
        
        # å¦‚æœéŸ¿æ‡‰æ™‚é–“å¤ªé•·ï¼Œé™ä½é€Ÿç‡
        if avg_response_time > 5.0:  # 5 ç§’
            new_rps = max(self.requests_per_second * 0.7, self.min_rps)
        # å¦‚æœéŸ¿æ‡‰æ™‚é–“å¾ˆçŸ­ï¼Œå¯ä»¥æé«˜é€Ÿç‡
        elif avg_response_time < 1.0:  # 1 ç§’
            new_rps = min(self.requests_per_second * 1.2, self.max_rps)
        else:
            return  # ä¸éœ€è¦èª¿æ•´
        
        print(f"èª¿æ•´é€Ÿç‡: {self.requests_per_second:.2f} -> {new_rps:.2f} RPS")
        self.requests_per_second = new_rps
        self._update_token_bucket()
    
    def _update_token_bucket(self):
        """æ›´æ–°ä»¤ç‰Œæ¡¶åƒæ•¸"""
        self.token_refill_rate = self.requests_per_second
        # é‡æ–°è¨ˆç®—ä»¤ç‰Œè£œå……æ™‚é–“
        self.last_refill = time.time()

# ä½¿ç”¨è‡ªé©æ‡‰é™é€Ÿå™¨
adaptive_limiter = AdaptiveRateLimiter(initial_rps=2.0)
crawler = WebCrawler()

async def crawl_with_adaptive_rate(url):
    await adaptive_limiter.acquire()

    start_time = time.time()
    result = await crawler.crawl(url)
    response_time = time.time() - start_time

    adaptive_limiter.record_response_time(response_time)
    return result
```

## âš ï¸ å¸¸è¦‹å•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆ

### 1. çˆ¬èŸ²è¢«å°é–

**å•é¡Œ**: ç›®æ¨™ç¶²ç«™è¿”å› 403 æˆ– 429 éŒ¯èª¤

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# 1. èª¿æ•´è«‹æ±‚é »ç‡
crawler = WebCrawler(delay=5.0)  # å¢åŠ å»¶é²

# 2. ä½¿ç”¨æ›´çœŸå¯¦çš„ User-Agent
crawler = WebCrawler(
    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
)

# 3. æ·»åŠ æ›´å¤š headers
crawler.add_headers({
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
})

# 4. ä½¿ç”¨æœƒè©±å’Œ cookies
cookies = {"session_id": "your_session_id"}
crawler.set_cookies(cookies)
```

### 2. JavaScript å…§å®¹ç„¡æ³•ç²å–

**å•é¡Œ**: å‹•æ…‹ç”Ÿæˆçš„å…§å®¹ç„¡æ³•æŠ“å–

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# ä½¿ç”¨ WebCrawler
from spider.crawlers.web_crawler import WebCrawler

crawler = WebCrawler(
    headless=True,      # å•Ÿç”¨ç€è¦½å™¨æ¨¡å¼
    wait_time=3000,     # ç­‰å¾… 3 ç§’è®“ JS åŸ·è¡Œ
    timeout=30000       # å¢åŠ è¶…æ™‚æ™‚é–“
)

# ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç¾
await crawler.wait_for_element("div.content", timeout=10000)
```

### 3. è¨˜æ†¶é«”ä½¿ç”¨éå¤š

**å•é¡Œ**: çˆ¬å–å¤§é‡é é¢å¾Œè¨˜æ†¶é«”ä¸æ–·å¢é•·

**è§£æ±ºæ–¹æ¡ˆ**:
```python
from spider.crawlers.web_crawler import WebCrawler

# 1. é™åˆ¶ä½µç™¼æ•¸
crawler = WebCrawler()

# 2. å®šæœŸæ¸…ç†
async def crawl_with_cleanup(urls, batch_size=100):
    for i in range(0, len(urls), batch_size):
        batch = urls[i:i+batch_size]
        results = []
        for u in batch:
            results.append(await crawler.crawl(u))

        # è™•ç†çµæœ
        process_results(results)

        # å¼·åˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()

        # çŸ­æš«ä¼‘æ¯
        await asyncio.sleep(1)
```

### 4. åˆ†å¡Šçµæœä¸ç†æƒ³

**å•é¡Œ**: åˆ†å¡ŠæŠŠç›¸é—œå…§å®¹åˆ†é–‹äº†

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨æ»‘å‹•çª—å£å¢åŠ é‡ç–Š
chunker = SlidingWindowChunker(
    window_size=200,
    step_size=100,      # 50% é‡ç–Š
    overlap_sentences=2  # å¥å­ç´šé‡ç–Š
)

# 2. å˜—è©¦èªç¾©åˆ†å¡Š
chunker = ChunkerFactory.create_chunker('semantic', {
    'similarity_threshold': 0.7,  # é™ä½é–¾å€¼
    'min_chunk_size': 100
})

# 3. è‡ªå®šç¾©åˆ†å¡Šé‚è¼¯
def custom_chunk_by_headers(text):
    """åŸºæ–¼æ¨™é¡Œçµæ§‹åˆ†å¡Š"""
    import re
    
    # å°‹æ‰¾æ¨™é¡Œæ¨¡å¼
    header_pattern = r'^#{1,6}\s+.+$'
    lines = text.split('\n')
    
    chunks = []
    current_chunk = []
    
    for line in lines:
        if re.match(header_pattern, line.strip()) and current_chunk:
            # é‡åˆ°æ–°æ¨™é¡Œï¼Œä¿å­˜ç•¶å‰å¡Š
            chunks.append('\n'.join(current_chunk))
            current_chunk = [line]
        else:
            current_chunk.append(line)
    
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    
    return chunks
```

## ğŸ“ˆ æ€§èƒ½å„ªåŒ–å»ºè­°

### 1. çˆ¬èŸ²å„ªåŒ–

- **ä½µç™¼æ§åˆ¶**: æ ¹æ“šç›®æ¨™ç¶²ç«™å’Œç¶²è·¯ç‹€æ³èª¿æ•´ä½µç™¼æ•¸
- **é€£æ¥é‡ç”¨**: ä½¿ç”¨æŒä¹…é€£æ¥æ¸›å°‘å»ºç«‹é€£æ¥çš„é–‹éŠ·
- **å¿«å–æ©Ÿåˆ¶**: å¯¦ç¾å…§å®¹å¿«å–é¿å…é‡è¤‡çˆ¬å–
- **æ™ºèƒ½é‡è©¦**: ä½¿ç”¨æŒ‡æ•¸é€€é¿ï¼Œé¿å…ç„¡æ•ˆé‡è©¦

### 2. åˆ†å¡Šå„ªåŒ–

- **é è™•ç†**: ç§»é™¤ä¸å¿…è¦çš„ç©ºç™½å’Œæ ¼å¼å­—ç¬¦
- **ä¸¦è¡Œè™•ç†**: å°å¤§æ–‡æœ¬ä½¿ç”¨å¤šé€²ç¨‹åˆ†å¡Š
- **å¿«å–åˆ†å¡Š**: å°ç›¸åŒå…§å®¹ä½¿ç”¨å¿«å–åˆ†å¡Šçµæœ
- **å‹•æ…‹èª¿æ•´**: æ ¹æ“šå…§å®¹é¡å‹é¸æ“‡æœ€é©åˆçš„åˆ†å¡Šç­–ç•¥

### 3. è¨˜æ†¶é«”å„ªåŒ–

- **æµå¼è™•ç†**: å°å¤§æ–‡ä»¶ä½¿ç”¨æµå¼è®€å–å’Œè™•ç†
- **åŠæ™‚æ¸…ç†**: è™•ç†å®Œç•¢å¾Œç«‹å³é‡‹æ”¾å¤§å°è±¡
- **æ‰¹é‡è™•ç†**: åˆ†æ‰¹è™•ç†å¤§é‡URLï¼Œé¿å…è¨˜æ†¶é«”å †ç©
- **ç”Ÿæˆå™¨ä½¿ç”¨**: ä½¿ç”¨ç”Ÿæˆå™¨ä»£æ›¿åˆ—è¡¨å­˜å„²å¤§é‡æ•¸æ“š

---

**ğŸ’¡ æç¤º**: é€™å€‹æ¨¡çµ„è¨­è¨ˆæ™‚å……åˆ†è€ƒæ…®äº†å¯æ“´å±•æ€§å’Œéˆæ´»æ€§ã€‚å¦‚æœé è¨­åŠŸèƒ½ç„¡æ³•æ»¿è¶³æ‚¨çš„éœ€æ±‚ï¼Œè«‹åƒè€ƒè‡ªå®šç¾©æ“´å±•éƒ¨åˆ†ä¾†å¯¦ç¾æ‚¨çš„ç‰¹æ®Šéœ€æ±‚ã€‚

**ğŸ”— ç›¸é—œæ–‡æª”**: 
- [å®Œæ•´å°ˆæ¡ˆæ–‡æª”](../README.md)
- [è³‡æ–™åº«æ¨¡çµ„æ–‡æª”](../database/README.md)
- [API åƒè€ƒæ–‡æª”](https://github.com/Grasonyang/Agentic-Rag/wiki/API)
