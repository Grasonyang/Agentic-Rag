# Makefile for Agentic RAG System

.PHONY: help install test clean db-setup db-clear spider-test embedding-test
.PHONY: get-sitemap get-urls get-chunking get-embedding run-workflow
.PHONY: test-crawler check-db logs-clean db-check db-status db-fresh db-form db-tables
	@$(PYTHON) -c "import sys, os; sys.path.append('.'); from database.postgres_client import PostgreSQLClient; from datetime import datetime; client = PostgreSQLClient(); client.connect(); timestamp = datetime.now().strftime('%Y%m%d_%H%M%S'); backup_file=f'backups/db_backup_{timestamp}.sql'; print(f'ğŸ“ å‚™ä»½æ–‡ä»¶: {backup_file}'); client.disconnect()"

db-migrate: ## Run database migrations (if any)
	@echo "ğŸ”„ åŸ·è¡Œè³‡æ–™åº«é·ç§»..."
	@echo "âš ï¸  ç›®å‰æ²’æœ‰é·ç§»è…³æœ¬ï¼Œä½¿ç”¨ db-fresh é‡æ–°åˆå§‹åŒ–"

db-repair: ## Attempt to repair common database issues
	@echo "ğŸ”§ å˜—è©¦ä¿®å¾©è³‡æ–™åº«å•é¡Œ..."
	@$(PYTHON) -c "import sys; sys.path.append('.'); from database.postgres_client import PostgreSQLClient; client = PostgreSQLClient(); client.connect(); client.execute_query('ANALYZE', fetch=False); print('âœ… çµ±è¨ˆä¿¡æ¯å·²æ›´æ–°'); client.disconnect()"ng run-workflow
.PHONY: test-crawler check-db logs-clean

# Default goal
.DEFAULT_GOAL := help

# Variables
PYTHON := python3
VENV := venv
REQUIREMENTS := requirements.txt

# URLs and files for workflow
URL ?= https://example.com
SITEMAP_URL ?= https://example.com/sitemap.xml
SITEMAP_LIST ?= sitemaps.txt
URL_LIST ?= urls.txt
CHUNK_LIST ?= chunks.txt

# Help target
help: ## Show this help message
	@echo "Agentic RAG System - Available Commands:"
	@echo ""
	@echo "Setup and Environment:"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | grep -E "(install|clean|test)" | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-20s\033[0m %s\n", $$1, $$2}'
	@echo ""  
	@echo "Database Operations:"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | grep -E "(db-|check-)" | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-20s\033[0m %s\n", $$1, $$2}'
	@echo ""
	@echo "Crawler Testing:"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | grep -E "test-" | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-20s\033[0m %s\n", $$1, $$2}'
	@echo ""
	@echo "Workflow Scripts:"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | grep -E "(get-|run-)" | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-20s\033[0m %s\n", $$1, $$2}'
	@echo ""
	@echo "Logs and Maintenance:"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | grep -E "logs-" | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-20s\033[0m %s\n", $$1, $$2}'

# Installation and setup
install: ## Install dependencies
	$(PYTHON) -m pip install --upgrade pip
	$(PYTHON) -m pip install -r $(REQUIREMENTS)

clean: ## Clean up temporary files and caches
	find . -type f -name "*.pyc" -delete
	find . -type d -name "__pycache__" -exec rm -rf {} +
	find . -type d -name "*.egg-info" -exec rm -rf {} +
	rm -f *.log
	rm -f *.txt

# Database operations
db-setup: ## Setup fresh database with all schemas
	$(PYTHON) scripts/database/make-fresh.py

db-clear: ## Clear all database data
	$(PYTHON) scripts/database/make-clear.py

db-check: ## Run comprehensive database health check
	$(PYTHON) scripts/database/make-db-check.py

db-status: ## Quick database status check
	@echo "ğŸ” å¿«é€Ÿè³‡æ–™åº«ç‹€æ…‹æª¢æŸ¥..."
	@$(PYTHON) -c "import sys; sys.path.append('.'); from database.postgres_client import PostgreSQLClient; client = PostgreSQLClient(); success = client.connect(); print('âœ… è³‡æ–™åº«é€£æ¥æ­£å¸¸' if success else 'âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—'); client.disconnect() if success else None"

# Workflow scripts
get-sitemap: ## Discover and analyze sitemaps from URL
	@echo "ğŸ—ºï¸  Discovering sitemaps from: $(URL)"
	$(PYTHON) scripts/getSiteMap.py --url $(URL) --output $(SITEMAP_LIST)
	@echo "âœ… Sitemap discovery complete. Output: $(SITEMAP_LIST)"

get-urls: ## Extract URLs from sitemap list  
	@echo "ğŸ”— Extracting URLs from: $(SITEMAP_LIST)"
	$(PYTHON) scripts/getUrls.py --sitemap-list $(SITEMAP_LIST) --output $(URL_LIST)
	@echo "âœ… URL extraction complete. Output: $(URL_LIST)"

get-chunking: ## Crawl URLs and create content chunks
	@echo "ğŸ“„ Crawling URLs and chunking content from: $(URL_LIST)"
	$(PYTHON) scripts/getChunking.py --url-list $(URL_LIST) --output $(CHUNK_LIST)
	@echo "âœ… Content chunking complete. Output: $(CHUNK_LIST)"

get-embedding: ## Generate embeddings for chunks
	@echo "ğŸ§  Generating embeddings for: $(CHUNK_LIST)"
	$(PYTHON) scripts/getEmbedding.py --chunk-list $(CHUNK_LIST)
	@echo "âœ… Embedding generation complete. RAG system ready!"

# Complete workflow
run-workflow: ## Run complete RAG workflow (sitemap â†’ urls â†’ chunking â†’ embedding)
	@echo "ğŸš€ Starting complete RAG workflow..."
	@echo ""
	@$(MAKE) get-sitemap URL=$(URL)
	@echo ""
	@$(MAKE) get-urls SITEMAP_LIST=$(SITEMAP_LIST)
	@echo ""
	@$(MAKE) get-chunking URL_LIST=$(URL_LIST)
	@echo ""
	@$(MAKE) get-embedding CHUNK_LIST=$(CHUNK_LIST)
	@echo ""
	@echo "ğŸ‰ Complete RAG workflow finished!"
	@echo "ğŸ” Your knowledge base is ready for semantic search."

# Testing targets
test: ## Run all tests
	$(PYTHON) -m pytest tests/ -v

spider-test: ## Test spider functionality
	$(PYTHON) scripts/test.py

embedding-test: ## Test embedding functionality
	$(PYTHON) -c "from embedding.embedding import EmbeddingManager; em = EmbeddingManager(); print('âœ… Embedding system working')"

# Advanced workflow options
get-sitemap-custom: ## Custom sitemap discovery with options
	@echo "ğŸ—ºï¸  Custom sitemap discovery..."
	$(PYTHON) scripts/getSiteMap.py --url $(URL) --output $(SITEMAP_LIST) --max-depth 3 --batch-size 10

get-chunking-custom: ## Custom chunking with semantic strategy
	@echo "ğŸ“„ Custom semantic chunking..."
	$(PYTHON) scripts/getChunking.py --url-list $(URL_LIST) --output $(CHUNK_LIST) --chunker semantic --chunk-size 500 --overlap 100

get-embedding-gpu: ## Generate embeddings using GPU
	@echo "ğŸš€ GPU-accelerated embedding generation..."
	$(PYTHON) scripts/getEmbedding.py --chunk-list $(CHUNK_LIST) --device cuda --batch-size 32

# Monitoring and logs
show-stats: ## Show processing statistics
	@echo "ğŸ“Š Processing Statistics:"
	@echo "Sitemaps discovered: $$(grep -c 'Sitemap:' $(SITEMAP_LIST) 2>/dev/null || echo 0)"
	@echo "URLs extracted: $$(grep -c '^- http' $(URL_LIST) 2>/dev/null || echo 0)"  
	@echo "Chunks created: $$(grep -c '^## Chunk' $(CHUNK_LIST) 2>/dev/null || echo 0)"

show-logs: ## Show recent processing logs
	@echo "ğŸ“‹ Recent Processing Logs:"
	@tail -20 *.log 2>/dev/null || echo "No log files found"

# Cleanup workflow files
clean-workflow: ## Clean workflow output files
	rm -f $(SITEMAP_LIST) $(URL_LIST) $(CHUNK_LIST)

# New commands for improved logging and testing
db-health: ## Quick database health summary
	@echo "ğŸ¥ è³‡æ–™åº«å¥åº·æ‘˜è¦..."
	@$(PYTHON) -c "import sys; sys.path.append('.'); from database.postgres_client import PostgreSQLClient; client = PostgreSQLClient(); success = client.connect(); print('ğŸ“Š é€£æ¥ç‹€æ…‹:', 'âœ… æ­£å¸¸' if success else 'âŒ å¤±æ•—'); client.disconnect() if success else None"

# Database management commands
db-fresh: ## Reinitialize database with fresh schema
	@echo "ï¿½ é‡æ–°åˆå§‹åŒ–è³‡æ–™åº«..."
	$(PYTHON) scripts/database/make-fresh.py

db-fresh-force: ## Reinitialize database without confirmation
	@echo "ğŸ”¥ å¼·åˆ¶é‡æ–°åˆå§‹åŒ–è³‡æ–™åº«..."
	$(PYTHON) scripts/database/make-fresh.py --force

db-clear-force: ## Clear all database data (without confirmation)
	@echo "ğŸ”¥ å¼·åˆ¶æ¸…ç©ºè³‡æ–™åº«æ•¸æ“š..."
	$(PYTHON) scripts/database/make-clear.py --force

db-tables: ## Show database table information
	@echo "ğŸ“‹ è³‡æ–™åº«è¡¨æ ¼ä¿¡æ¯..."
	@$(PYTHON) -c "import sys; sys.path.append('.'); from database.postgres_client import PostgreSQLClient; client = PostgreSQLClient(); client.connect(); tables = ['discovered_urls', 'articles', 'article_chunks', 'sitemaps']; [print(f'ğŸ“Š {table}: {client.get_table_count(table) if client.table_exists(table) else \"ä¸å­˜åœ¨\"} ç­†è¨˜éŒ„') for table in tables]; client.disconnect()"

db-form: ## Get database form data (JSON format)
	@echo "ï¿½ ç²å–è³‡æ–™åº«è¡¨å–®æ•¸æ“š..."
	@$(PYTHON) scripts/database/make-db-check.py | grep -A 1000 "è³‡æ–™åº«è¡¨å–® JSON æ•¸æ“š:" | tail -n +3

test-crawler: ## Test crawler functionality (requires URL or SITEMAP_URL)
	@echo "ğŸ•·ï¸ é–‹å§‹çˆ¬èŸ²æ¸¬è©¦..."
	$(PYTHON) scripts/test_crawler.py $(if $(SITEMAP_URL),--sitemap $(SITEMAP_URL)) $(if $(URL),--url $(URL))

logs-clean: ## Clean old log files (older than 7 days)
	@echo "ğŸ§¹ æ¸…ç†èˆŠæ—¥èªŒæ–‡ä»¶..."
	find scripts/logs -name "*.log" -mtime +7 -delete 2>/dev/null || true
	@echo "âœ… æ—¥èªŒæ¸…ç†å®Œæˆ"

logs-show: ## Show recent logs from all scripts
	@echo "ğŸ“‹ æœ€è¿‘çš„è…³æœ¬æ—¥èªŒ:"
	@ls -la scripts/logs/ | tail -10
	@echo ""
	@echo "ğŸ“„ æœ€æ–°æ—¥èªŒå…§å®¹ (æœ€å¾Œ 20 è¡Œ):"
	@tail -20 scripts/logs/*.log 2>/dev/null | head -100 || echo "æ²’æœ‰æ‰¾åˆ°æ—¥èªŒæ–‡ä»¶"

output-show: ## Show recent output files
	@echo "ğŸ“ æœ€è¿‘ç”Ÿæˆçš„è¼¸å‡ºæ–‡ä»¶:"
	@ls -la scripts/output/ 2>/dev/null | tail -10 || echo "æ²’æœ‰è¼¸å‡ºæ–‡ä»¶"
	@echo "ğŸ—‘ï¸  Workflow files cleaned"
