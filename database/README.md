# ğŸ“Š Database æ¨¡çµ„ä½¿ç”¨æŒ‡å—

é€™å€‹æ¨¡çµ„æä¾›äº†å®Œæ•´çš„è³‡æ–™åº«æ“ä½œåŠŸèƒ½ï¼ŒåŒ…æ‹¬é€£æ¥ç®¡ç†ã€æ¨¡å‹å®šç¾©ã€ä»¥åŠå„ç¨®è³‡æ–™åº«æ“ä½œã€‚

## ğŸ—ï¸ è³‡æ–™åº«æ¶æ§‹

### æ ¸å¿ƒè¡¨çµæ§‹

#### 1. `articles` è¡¨ - æ–‡ç« ä¸»é«”
```sql
-- ä½¿ç”¨ UUID ä½œç‚ºä¸»éµä»¥æ”¯æ´åˆ†æ•£å¼ç³»çµ±
CREATE TABLE articles (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT UNIQUE NOT NULL,
    title TEXT,
    content TEXT,
    metadata JSONB DEFAULT '{}',
    status TEXT DEFAULT 'pending',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç´¢å¼•å„ªåŒ–
CREATE INDEX idx_articles_url ON articles(url);
CREATE INDEX idx_articles_status ON articles(status);
CREATE INDEX idx_articles_created_at ON articles(created_at);
```

#### 2. `article_chunks` è¡¨ - æ–‡ç« åˆ†å¡Šèˆ‡å‘é‡å„²å­˜
```sql
-- é«˜æ•ˆå‘é‡ç›¸ä¼¼æœç´¢çš„æ ¸å¿ƒè¡¨
CREATE TABLE article_chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    article_id UUID REFERENCES articles(id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    embedding VECTOR(384),  -- BGE-large-zh-v1.5 ç¶­åº¦
    chunk_index INTEGER DEFAULT 0,
    chunk_type TEXT DEFAULT 'sentence',
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- HNSW ç´¢å¼•ç”¨æ–¼é«˜æ•ˆå‘é‡æœç´¢ (Supabase æ¨è–¦)
CREATE INDEX idx_article_chunks_embedding ON article_chunks 
USING hnsw (embedding vector_cosine_ops) 
WITH (m = 16, ef_construction = 64);

-- å‚³çµ±ç´¢å¼•
CREATE INDEX idx_article_chunks_article_id ON article_chunks(article_id);
CREATE INDEX idx_article_chunks_chunk_index ON article_chunks(chunk_index);
```

#### 3. `crawl_logs` è¡¨ - çˆ¬å–è¨˜éŒ„èˆ‡ç‹€æ…‹è¿½è¹¤
```sql
CREATE TABLE crawl_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT NOT NULL,
    status TEXT DEFAULT 'pending',
    success BOOLEAN DEFAULT FALSE,
    error_message TEXT,
    response_time_ms INTEGER,
    http_status_code INTEGER,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç›£æ§èˆ‡åˆ†æç´¢å¼•
CREATE INDEX idx_crawl_logs_url ON crawl_logs(url);
CREATE INDEX idx_crawl_logs_status ON crawl_logs(status);
CREATE INDEX idx_crawl_logs_success ON crawl_logs(success);
```

#### 4. `search_logs` è¡¨ - æœç´¢è¡Œç‚ºåˆ†æ
```sql
CREATE TABLE search_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    query TEXT NOT NULL,
    query_embedding VECTOR(384),
    results_count INTEGER DEFAULT 0,
    search_duration_ms INTEGER,
    threshold FLOAT DEFAULT 0.7,
    user_id TEXT,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- åˆ†æèˆ‡å„ªåŒ–ç´¢å¼•
CREATE INDEX idx_search_logs_query ON search_logs(query);
CREATE INDEX idx_search_logs_created_at ON search_logs(created_at);
CREATE INDEX idx_search_logs_user_id ON search_logs(user_id);
```

### å…¶ä»–æ”¯æ´è¡¨

#### 5. `sitemaps` è¡¨ - Sitemap è¨˜éŒ„
```sql
CREATE TABLE sitemaps (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT UNIQUE NOT NULL,
    type TEXT DEFAULT 'sitemap',
    status TEXT DEFAULT 'pending',
    title TEXT,
    description TEXT,
    lastmod TIMESTAMPTZ,
    changefreq TEXT,
    priority FLOAT,
    urls_count INTEGER DEFAULT 0,
    parsed_at TIMESTAMPTZ,
    error_message TEXT,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

#### 6. `discovered_urls` è¡¨ - ç™¼ç¾çš„ URL
```sql
CREATE TABLE discovered_urls (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT NOT NULL,
    source_sitemap_id UUID REFERENCES sitemaps(id),
    url_type TEXT DEFAULT 'content',
    priority FLOAT,
    changefreq TEXT,
    lastmod TIMESTAMPTZ,
    crawl_status TEXT DEFAULT 'pending',
    crawl_attempts INTEGER DEFAULT 0,
    last_crawl_at TIMESTAMPTZ,
    error_message TEXT,
    article_id UUID REFERENCES articles(id),
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

## ğŸ“‹ æ¨¡å‹èªªæ˜

### æ ¸å¿ƒæ¨¡å‹é¡åˆ¥

| æ¨¡å‹é¡åˆ¥ | ç”¨é€” | ä¸»è¦æ¬„ä½ |
|---------|------|---------|
| `ArticleModel` | æ–‡ç« å…§å®¹å­˜å„² | id, url, title, content, metadata |
| `ChunkModel` | æ–‡ç« åˆ†å¡Šèˆ‡å‘é‡ | article_id, content, embedding, chunk_index |
| `SitemapModel` | Sitemap ç®¡ç† | url, type, status, urls_count |
| `DiscoveredURLModel` | URL ç™¼ç¾è¨˜éŒ„ | url, source_sitemap_id, crawl_status |
| `SearchLogModel` | æœç´¢è¨˜éŒ„ | query, results_count, response_time_ms |

### ç‹€æ…‹æšèˆ‰

```python
class CrawlStatus(Enum):
    PENDING = "pending"      # å¾…è™•ç†
    CRAWLING = "crawling"    # è™•ç†ä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    ERROR = "error"          # éŒ¯èª¤
    SKIPPED = "skipped"      # å·²è·³é

class SitemapType(Enum):
    SITEMAP = "sitemap"              # æ™®é€š sitemap
    SITEMAPINDEX = "sitemapindex"    # sitemap ç´¢å¼•
    URLSET = "urlset"                # URL é›†åˆ
```

## ğŸ”§ åŸºæœ¬ä½¿ç”¨

### 1. è³‡æ–™åº«é€£æ¥

```python
from database.client import SupabaseClient
from database.operations import DatabaseOperations

# åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
client = SupabaseClient()
db = DatabaseOperations()

# æ¸¬è©¦é€£æ¥
if client.test_connection():
    print("è³‡æ–™åº«é€£æ¥æˆåŠŸï¼")
```

### 2. æ–‡ç« æ“ä½œ

```python
from database.models import ArticleModel

# å‰µå»ºæ–‡ç« 
article = ArticleModel(
    url="https://example.com/article",
    title="ç¯„ä¾‹æ–‡ç« ",
    content="é€™æ˜¯æ–‡ç« å…§å®¹...",
    metadata={"author": "ä½œè€…", "tags": ["AI", "Python"]}
)

# å„²å­˜åˆ°è³‡æ–™åº«
article_id = db.create_article(article.to_dict())
print(f"æ–‡ç« å·²å„²å­˜ï¼ŒID: {article_id}")

# è®€å–æ–‡ç« 
retrieved_article = db.get_article_by_url("https://example.com/article")
if retrieved_article:
    article_obj = ArticleModel.from_dict(retrieved_article)
    print(f"æ–‡ç« æ¨™é¡Œ: {article_obj.title}")
```

### 3. åˆ†å¡Šæ“ä½œ

```python
from database.models import ChunkModel

# å‰µå»ºæ–‡ç« åˆ†å¡Š
chunk = ChunkModel(
    article_id=article_id,
    content="é€™æ˜¯æ–‡ç« çš„ç¬¬ä¸€å€‹åˆ†å¡Š...",
    chunk_index=0,
    embedding=[0.1, 0.2, 0.3, ...],  # 384 ç¶­å‘é‡
    metadata={"chunk_type": "paragraph"}
)

# å„²å­˜åˆ†å¡Š
chunk_id = db.create_chunk(chunk.to_dict())
print(f"åˆ†å¡Šå·²å„²å­˜ï¼ŒID: {chunk_id}")
```

### 4. èªç¾©æœç´¢

```python
from embedding.embedding import embed_text

# åŸ·è¡Œèªç¾©æœç´¢
query = "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ"
query_embedding = embed_text(query)

results = db.semantic_search(
    query_text=query,
    query_embedding=query_embedding,
    match_threshold=0.75,
    match_count=5
)

for result in results:
    print(f"ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
    print(f"å…§å®¹: {result['content'][:100]}...")
    print(f"ä¾†æº: {result['article_url']}")
    print("-" * 50)
```

### 5. Sitemap ç®¡ç†

```python
from database.models import SitemapModel, SitemapType, CrawlStatus

# å‰µå»º Sitemap è¨˜éŒ„
sitemap = SitemapModel(
    url="https://example.com/sitemap.xml",
    sitemap_type=SitemapType.SITEMAP,
    status=CrawlStatus.PENDING,
    urls_count=100
)

# å„²å­˜ Sitemap
sitemap_id = db.create_sitemap(sitemap.to_dict())

# æ›´æ–°ç‹€æ…‹
db.update_sitemap_status(sitemap_id, CrawlStatus.COMPLETED)
```

## ğŸ“Š RPC å‡½æ•¸ä½¿ç”¨

### 1. èªç¾©æœç´¢å‡½æ•¸

```python
# ç›´æ¥èª¿ç”¨ RPC å‡½æ•¸
search_params = {
    'query_text': 'äººå·¥æ™ºæ…§',
    'query_embedding': query_embedding,
    'match_threshold': 0.7,
    'match_count': 10
}

results = client.supabase.rpc('semantic_search', search_params).execute()
print("æœç´¢çµæœ:", results.data)
```

### 2. ç³»çµ±çµ±è¨ˆå‡½æ•¸

```python
# ç²å–ç³»çµ±çµ±è¨ˆ
stats = client.supabase.rpc('get_system_stats').execute()
print(f"ç¸½æ–‡ç« æ•¸: {stats.data['total_articles']}")
print(f"ç¸½åˆ†å¡Šæ•¸: {stats.data['total_chunks']}")
print(f"æˆåŠŸçˆ¬å–: {stats.data['successful_crawls']}")
print(f"å¤±æ•—çˆ¬å–: {stats.data['failed_crawls']}")
print(f"è³‡æ–™åº«å¤§å°: {stats.data['database_size_mb']} MB")
```

### 3. è¡¨æ ¼è³‡è¨ŠæŸ¥è©¢

```python
# ç²å–æ‰€æœ‰è¡¨æ ¼è³‡è¨Š
tables = client.supabase.rpc('get_all_tables').execute()
for table in tables.data:
    print(f"è¡¨æ ¼: {table['table_name']}, è¨˜éŒ„æ•¸: {table['row_count']}")
```

## ğŸ” é€²éšæŸ¥è©¢

### 1. è¤‡é›œæ¢ä»¶æŸ¥è©¢

```python
# æŸ¥è©¢ç‰¹å®šç‹€æ…‹çš„æ–‡ç« 
articles = client.supabase.from_('articles')\
    .select('*')\
    .eq('status', 'completed')\
    .order('created_at', desc=True)\
    .limit(10)\
    .execute()

print(f"æ‰¾åˆ° {len(articles.data)} ç¯‡å·²å®Œæˆçš„æ–‡ç« ")
```

### 2. èšåˆæŸ¥è©¢

```python
# çµ±è¨ˆæ¯å€‹åŸŸåçš„æ–‡ç« æ•¸é‡
from urllib.parse import urlparse

domain_stats = {}
articles = client.supabase.from_('articles').select('url').execute()

for article in articles.data:
    domain = urlparse(article['url']).netloc
    domain_stats[domain] = domain_stats.get(domain, 0) + 1

print("å„åŸŸåæ–‡ç« çµ±è¨ˆ:")
for domain, count in sorted(domain_stats.items(), key=lambda x: x[1], reverse=True):
    print(f"{domain}: {count} ç¯‡")
```

### 3. æ™‚é–“ç¯„åœæŸ¥è©¢

```python
from datetime import datetime, timedelta

# æŸ¥è©¢æœ€è¿‘ 7 å¤©çš„æœç´¢è¨˜éŒ„
week_ago = (datetime.now() - timedelta(days=7)).isoformat()

recent_searches = client.supabase.from_('search_logs')\
    .select('query, results_count, created_at')\
    .gte('created_at', week_ago)\
    .order('created_at', desc=True)\
    .execute()

print(f"æœ€è¿‘ 7 å¤©å…±æœ‰ {len(recent_searches.data)} æ¬¡æœç´¢")
```

## ğŸ› ï¸ ç®¡ç†å·¥å…·

### 1. è³‡æ–™åº«æ¸…ç†

```python
# æ¸…ç†å¤±æ•—çš„çˆ¬å–è¨˜éŒ„ (è¶…é 30 å¤©)
cleanup_date = (datetime.now() - timedelta(days=30)).isoformat()

cleanup_result = client.supabase.from_('crawl_logs')\
    .delete()\
    .eq('success', False)\
    .lt('created_at', cleanup_date)\
    .execute()

print(f"æ¸…ç†äº† {len(cleanup_result.data)} æ¢å¤±æ•—è¨˜éŒ„")
```

### 2. é‡æ–°è¨ˆç®—çµ±è¨ˆ

```python
# æ›´æ–°æ–‡ç« çš„å­—æ•¸çµ±è¨ˆ
articles = client.supabase.from_('articles').select('id, content').execute()

for article in articles.data:
    word_count = len(article['content'].split())
    
    client.supabase.from_('articles')\
        .update({'word_count': word_count})\
        .eq('id', article['id'])\
        .execute()

print(f"æ›´æ–°äº† {len(articles.data)} ç¯‡æ–‡ç« çš„å­—æ•¸çµ±è¨ˆ")
```

### 3. ç´¢å¼•å„ªåŒ–

```python
# æª¢æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…æ³
index_stats = client.supabase.rpc('get_index_usage').execute()
for stat in index_stats.data:
    print(f"ç´¢å¼• {stat['indexname']}: ä½¿ç”¨æ¬¡æ•¸ {stat['idx_scan']}")
```

## ğŸ“ˆ æ€§èƒ½ç›£æ§

### 1. æŸ¥è©¢æ€§èƒ½åˆ†æ

```python
import time

# æ¸¬è©¦æœç´¢æ€§èƒ½
start_time = time.time()

results = db.semantic_search(
    query_text="æ¸¬è©¦æŸ¥è©¢",
    query_embedding=test_embedding,
    match_count=100
)

end_time = time.time()
duration_ms = (end_time - start_time) * 1000

print(f"æœç´¢è€—æ™‚: {duration_ms:.2f} ms")
print(f"çµæœæ•¸é‡: {len(results)}")
print(f"å¹³å‡æ¯çµæœè€—æ™‚: {duration_ms/len(results):.2f} ms")
```

### 2. è³‡æ–™åº«å¤§å°ç›£æ§

```python
# ç›£æ§å„è¡¨çš„å¤§å°
table_sizes = client.supabase.rpc('get_table_sizes').execute()

for table in table_sizes.data:
    print(f"{table['table_name']}: {table['size_mb']} MB")
```

## ğŸ”’ æ¬Šé™èˆ‡å®‰å…¨

### Supabase æ¬Šé™è¨­å®š

```sql
-- ç‚º anon è§’è‰²æä¾›è®€å–æ¬Šé™ (å…¬é–‹ API)
GRANT SELECT ON articles TO anon;
GRANT SELECT ON article_chunks TO anon;
GRANT SELECT ON search_logs TO anon;
GRANT SELECT ON crawl_logs TO anon;

-- ç‚º authenticated è§’è‰²æä¾›å®Œæ•´æ¬Šé™
GRANT ALL ON articles TO authenticated;
GRANT ALL ON article_chunks TO authenticated;
GRANT ALL ON search_logs TO authenticated;
GRANT ALL ON crawl_logs TO authenticated;

-- RPC å‡½æ•¸æ¬Šé™
GRANT EXECUTE ON FUNCTION semantic_search TO anon;
GRANT EXECUTE ON FUNCTION get_system_stats TO anon;
GRANT EXECUTE ON FUNCTION get_all_tables TO anon;
```

### å®‰å…¨æœ€ä½³å¯¦è¸

1. **ä½¿ç”¨ç’°å¢ƒè®Šæ•¸**: æ•æ„Ÿè³‡è¨Šå¦‚è³‡æ–™åº«å¯†é‘°æ‡‰å­˜æ”¾åœ¨ `.env` æª”æ¡ˆä¸­
2. **é™åˆ¶ API æ¬Šé™**: å…¬é–‹ API åƒ…æä¾›å¿…è¦çš„è®€å–æ¬Šé™
3. **è¼¸å…¥é©—è­‰**: æ‰€æœ‰æ¨¡å‹éƒ½åŒ…å« `validate()` æ–¹æ³•é€²è¡Œè³‡æ–™é©—è­‰
4. **SQL æ³¨å…¥é˜²è­·**: ä½¿ç”¨åƒæ•¸åŒ–æŸ¥è©¢é¿å… SQL æ³¨å…¥

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **é€£æ¥å¤±æ•—**
   ```python
   # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
   import os
   print("SUPABASE_URL:", os.getenv('SUPABASE_URL'))
   print("SUPABASE_KEY:", os.getenv('SUPABASE_KEY'))
   ```

2. **å‘é‡ç¶­åº¦éŒ¯èª¤**
   ```python
   # ç¢ºèªå‘é‡ç¶­åº¦ç‚º 384 (BGE-large-zh-v1.5)
   if len(embedding) != 384:
       print(f"éŒ¯èª¤: å‘é‡ç¶­åº¦ç‚º {len(embedding)}ï¼Œæ‡‰ç‚º 384")
   ```

3. **æ¬Šé™éŒ¯èª¤**
   ```python
   # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨æ­£ç¢ºçš„ API é‡‘é‘°
   try:
       result = client.supabase.from_('articles').select('count').execute()
       print("æ¬Šé™æ­£å¸¸")
   except Exception as e:
       print(f"æ¬Šé™éŒ¯èª¤: {e}")
   ```

## ğŸ“š ç›¸é—œè³‡æº

- [Supabase å®˜æ–¹æ–‡ä»¶](https://supabase.com/docs)
- [pgvector ä½¿ç”¨æŒ‡å—](https://github.com/pgvector/pgvector)
- [PostgreSQL UUID æœ€ä½³å¯¦è¸](https://www.postgresql.org/docs/current/datatype-uuid.html)
- [HNSW ç´¢å¼•å„ªåŒ–](https://github.com/pgvector/pgvector#hnsw)

---

**ğŸ’¡ æç¤º**: å¦‚æœé‡åˆ°å•é¡Œï¼Œè«‹å…ˆæª¢æŸ¥è³‡æ–™åº«é€£æ¥å’Œæ¬Šé™è¨­å®šï¼Œå¤§éƒ¨åˆ†å•é¡Œéƒ½èˆ‡é…ç½®ç›¸é—œã€‚
