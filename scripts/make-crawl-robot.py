#!/usr/bin/env python3
"""
make-crawl-robot.py
çˆ¬èŸ²ç¬¬ä¸€éšæ®µï¼šè§£æ robots.txt å’Œç™¼ç¾é€£çµ
"""

import sys
import asyncio
import re
from pathlib import Path
from typing import List, Dict, Set
from urllib.parse import urljoin, urlparse
from datetime import datetime

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from database.client import SupabaseClient
from database.operations import DatabaseOperations
from database.models import SitemapModel, DiscoveredURLModel, RobotsTxtModel, SitemapType, URLType, CrawlStatus
from config import Config
import logging
import aiohttp

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

class RobotsParser:
    """robots.txt è§£æå™¨"""
    
    def __init__(self):
        self.sitemap_urls = []
        self.allowed_paths = []
        self.disallowed_paths = []
        self.crawl_delay = 1.0
    
    def parse_robots(self, robots_content: str, base_url: str) -> Dict:
        """è§£æ robots.txt å…§å®¹"""
        lines = robots_content.strip().split('\n')
        current_user_agent = None
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
                
            if line.lower().startswith('user-agent:'):
                current_user_agent = line.split(':', 1)[1].strip()
            elif line.lower().startswith('sitemap:'):
                sitemap_url = line.split(':', 1)[1].strip()
                self.sitemap_urls.append(sitemap_url)
            elif line.lower().startswith('allow:'):
                path = line.split(':', 1)[1].strip()
                self.allowed_paths.append(path)
            elif line.lower().startswith('disallow:'):
                path = line.split(':', 1)[1].strip()
                if path != '/':  # ä¸æ·»åŠ å®Œå…¨ç¦æ­¢çš„è·¯å¾‘
                    self.disallowed_paths.append(path)
            elif line.lower().startswith('crawl-delay:'):
                try:
                    self.crawl_delay = float(line.split(':', 1)[1].strip())
                except:
                    pass
        
        return {
            'sitemap_urls': self.sitemap_urls,
            'allowed_paths': self.allowed_paths,
            'disallowed_paths': self.disallowed_paths,
            'crawl_delay': self.crawl_delay,
            'base_url': base_url
        }

class SitemapParser:
    """Sitemap è§£æå™¨"""
    
    async def parse_sitemap(self, sitemap_url: str) -> List[str]:
        """è§£æ sitemap.xml ç²å– URL åˆ—è¡¨"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(sitemap_url, timeout=30) as response:
                    if response.status == 200:
                        content = await response.text()
                        return self._extract_urls_from_sitemap(content)
        except Exception as e:
            logger.error(f"è§£æ sitemap å¤±æ•— {sitemap_url}: {e}")
        return []
    
    def _extract_urls_from_sitemap(self, sitemap_content: str) -> List[str]:
        """å¾ sitemap å…§å®¹ä¸­æå– URL"""
        urls = []
        
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå– <loc> æ¨™ç±¤ä¸­çš„ URL
        loc_pattern = r'<loc>(.*?)</loc>'
        matches = re.findall(loc_pattern, sitemap_content, re.IGNORECASE)
        
        for url in matches:
            url = url.strip()
            if url and not url.endswith('.xml'):  # æ’é™¤å…¶ä»– sitemap æ–‡ä»¶
                urls.append(url)
        
        return urls

class LinkDiscovery:
    """é€£çµç™¼ç¾å™¨"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
    
    async def discover_links_from_homepage(self) -> List[str]:
        """å¾é¦–é ç™¼ç¾é€£çµ"""
        try:
            from crawl4ai import AsyncWebCrawler
            
            async with AsyncWebCrawler(headless=True, verbose=False) as crawler:
                result = await crawler.arun(url=self.base_url, bypass_cache=True)
                
                if result.success and result.links:
                    internal_links = result.links.get('internal', [])
                    # éæ¿¾å‡ºæœ‰æ•ˆçš„å…§éƒ¨é€£çµ
                    valid_links = []
                    for link in internal_links[:50]:  # é™åˆ¶æ•¸é‡
                        if self._is_valid_link(link):
                            valid_links.append(link)
                    return valid_links
        except Exception as e:
            logger.error(f"å¾é¦–é ç™¼ç¾é€£çµå¤±æ•—: {e}")
        
        return []
    
    def _is_valid_link(self, url: str) -> bool:
        """æª¢æŸ¥é€£çµæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            
            # å¿…é ˆæ˜¯ç›¸åŒåŸŸå
            if parsed.netloc and parsed.netloc != self.domain:
                return False
            
            # æ’é™¤ä¸éœ€è¦çš„æ–‡ä»¶é¡å‹
            excluded_extensions = ['.pdf', '.jpg', '.png', '.gif', '.css', '.js', '.xml']
            if any(url.lower().endswith(ext) for ext in excluded_extensions):
                return False
            
            # æ’é™¤å¸¸è¦‹çš„ç³»çµ±è·¯å¾‘
            excluded_paths = ['/admin', '/api', '/login', '/register', '/search']
            if any(path in url.lower() for path in excluded_paths):
                return False
            
            return True
        except:
            return False

async def crawl_robot_phase(robot_urls: List[str] = None) -> Dict:
    """
    çˆ¬èŸ²ç¬¬ä¸€éšæ®µï¼šè§£æ robots.txt å’Œç™¼ç¾é€£çµ
    ä½¿ç”¨æ–°çš„è³‡æ–™åº«çµæ§‹ä¾†å„²å­˜ sitemap å±¤ç´šé—œä¿‚
    """
    print("ğŸ¤– RAG çˆ¬èŸ²éšæ®µ 1: Robot è§£æèˆ‡é€£çµç™¼ç¾")
    print("=" * 60)
    
    if robot_urls is None:
        robot_urls = [url.strip() for url in Config.TARGET_URLS if url.strip()]
    
    if not robot_urls:
        print("âŒ æ²’æœ‰æ‰¾åˆ° robots.txt URL")
        return {'discovered_urls': [], 'robots_info': {}}
    
    # åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
    try:
        client = SupabaseClient()
        db_ops = DatabaseOperations(client.get_client())
        logger.info("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
        return {'discovered_urls': [], 'robots_info': {}}
    
    all_discovered_urls = set()
    robots_info = {}
    
    try:
        # åˆå§‹åŒ–è§£æå™¨
        robots_parser = RobotsParser()
        sitemap_parser = SitemapParser()
        
        for robot_url in robot_urls:
            print(f"\nğŸ” è™•ç†: {robot_url}")
            base_url = robot_url.replace('/robots.txt', '')
            domain = urlparse(base_url).netloc
            
            try:
                # 1. è§£æ robots.txt
                async with aiohttp.ClientSession() as session:
                    async with session.get(robot_url, timeout=30) as response:
                        if response.status == 200:
                            robots_content = await response.text()
                            robots_data = robots_parser.parse_robots(robots_content, base_url)
                            robots_info[robot_url] = robots_data
                            
                            # å„²å­˜ robots.txt åˆ°è³‡æ–™åº«
                            robots_model = RobotsTxtModel(
                                domain=domain,
                                robots_url=robot_url,
                                content=robots_content,
                                sitemaps_count=len(robots_data['sitemap_urls']),
                                rules_count=len(robots_data['allowed_paths']) + len(robots_data['disallowed_paths']),
                                metadata={
                                    'crawl_delay': robots_data['crawl_delay'],
                                    'allowed_paths': robots_data['allowed_paths'],
                                    'disallowed_paths': robots_data['disallowed_paths']
                                }
                            )
                            
                            if db_ops.create_robots_txt(robots_model):
                                logger.info(f"âœ… robots.txt å·²å„²å­˜åˆ°è³‡æ–™åº«: {domain}")
                            
                            print(f"ğŸ“„ ç™¼ç¾ {len(robots_data['sitemap_urls'])} å€‹ sitemap")
                            print(f"â±ï¸ å»ºè­°çˆ¬å–å»¶é²: {robots_data['crawl_delay']} ç§’")
                            
                            # 2. è™•ç† sitemap å±¤ç´šçµæ§‹
                            for sitemap_url in robots_data['sitemap_urls']:
                                print(f"ğŸ—ºï¸ è™•ç† sitemap: {sitemap_url}")
                                
                                # å‰µå»ºæˆ–ç²å– sitemap è¨˜éŒ„
                                sitemap_model = SitemapModel(
                                    url=sitemap_url,
                                    sitemap_type=SitemapType.SITEMAP,
                                    status=CrawlStatus.PENDING,
                                    metadata={
                                        'source_robots': robot_url,
                                        'domain': domain
                                    }
                                )
                                
                                if db_ops.create_sitemap(sitemap_model):
                                    logger.info(f"âœ… Sitemap å·²å„²å­˜: {sitemap_url}")
                                
                                # è§£æ sitemap å…§å®¹
                                sitemap_urls = await sitemap_parser.parse_sitemap(sitemap_url)
                                
                                if sitemap_urls:
                                    # æ›´æ–° sitemap ç‹€æ…‹å’Œ URL æ•¸é‡
                                    db_sitemap = db_ops.get_sitemap_by_url(sitemap_url)
                                    if db_sitemap:
                                        db_ops.update_sitemap_status(db_sitemap.id, CrawlStatus.COMPLETED.value)
                                        
                                        # å‰µå»ºç™¼ç¾çš„ URL è¨˜éŒ„
                                        discovered_models = []
                                        for url in sitemap_urls:
                                            # åˆ¤æ–· URL é¡å‹
                                            url_type = URLType.SITEMAP if url.endswith('.xml') else URLType.CONTENT
                                            
                                            discovered_url = DiscoveredURLModel(
                                                url=url,
                                                source_sitemap_id=db_sitemap.id,
                                                url_type=url_type,
                                                priority=0.5,  # é»˜èªå„ªå…ˆç´š
                                                crawl_status=CrawlStatus.PENDING,
                                                metadata={
                                                    'discovered_from': sitemap_url,
                                                    'domain': domain
                                                }
                                            )
                                            discovered_models.append(discovered_url)
                                        
                                        # æ‰¹é‡å„²å­˜ç™¼ç¾çš„ URL
                                        created_count = db_ops.bulk_create_discovered_urls(discovered_models)
                                        logger.info(f"âœ… æ‰¹é‡å‰µå»º {created_count} å€‹ç™¼ç¾çš„ URL")
                                        
                                        all_discovered_urls.update(sitemap_urls)
                                        print(f"   âœ… ç™¼ç¾ä¸¦å„²å­˜ {len(sitemap_urls)} å€‹ URL")
                                    
                                else:
                                    # æ¨™è¨˜ç‚ºéŒ¯èª¤ç‹€æ…‹
                                    db_sitemap = db_ops.get_sitemap_by_url(sitemap_url)
                                    if db_sitemap:
                                        db_ops.update_sitemap_status(db_sitemap.id, CrawlStatus.ERROR.value, "ç„¡æ³•è§£æ sitemap å…§å®¹")
                            
                            # 3. å¾é¦–é ç™¼ç¾é€£çµï¼ˆå¯é¸ï¼‰
                            if len(all_discovered_urls) < 50:  # å¦‚æœç™¼ç¾çš„ URL ä¸å¤šï¼Œå˜—è©¦é¦–é ç™¼ç¾
                                link_discovery = LinkDiscovery(base_url)
                                homepage_links = await link_discovery.discover_links_from_homepage()
                                
                                if homepage_links:
                                    # å‰µå»ºä¸€å€‹ç‰¹æ®Šçš„ "é¦–é ç™¼ç¾" sitemap è¨˜éŒ„
                                    homepage_sitemap = SitemapModel(
                                        url=f"{base_url}/homepage-discovery",
                                        sitemap_type=SitemapType.URLSET,
                                        status=CrawlStatus.COMPLETED,
                                        title="Homepage Link Discovery",
                                        urls_count=len(homepage_links),
                                        metadata={
                                            'source_type': 'homepage_discovery',
                                            'base_url': base_url,
                                            'domain': domain
                                        }
                                    )
                                    
                                    if db_ops.create_sitemap(homepage_sitemap):
                                        db_sitemap = db_ops.get_sitemap_by_url(homepage_sitemap.url)
                                        if db_sitemap:
                                            # å„²å­˜é¦–é ç™¼ç¾çš„é€£çµ
                                            homepage_discovered = []
                                            for url in homepage_links:
                                                discovered_url = DiscoveredURLModel(
                                                    url=url,
                                                    source_sitemap_id=db_sitemap.id,
                                                    url_type=URLType.CONTENT,
                                                    priority=0.3,  # è¼ƒä½å„ªå…ˆç´š
                                                    crawl_status=CrawlStatus.PENDING,
                                                    metadata={
                                                        'discovered_from': 'homepage',
                                                        'domain': domain
                                                    }
                                                )
                                                homepage_discovered.append(discovered_url)
                                            
                                            created_count = db_ops.bulk_create_discovered_urls(homepage_discovered)
                                            all_discovered_urls.update(homepage_links)
                                            print(f"ğŸ  å¾é¦–é ç™¼ç¾ä¸¦å„²å­˜ {created_count} å€‹é€£çµ")
                        
                        else:
                            print(f"âŒ ç„¡æ³•è¨ªå• robots.txt: HTTP {response.status}")
                            
            except Exception as e:
                logger.error(f"è™•ç† {robot_url} å¤±æ•—: {e}")
                continue
        
        discovered_urls = list(all_discovered_urls)
        
        print(f"\nğŸ“Š Robot éšæ®µå®Œæˆ!")
        print(f"ğŸ”— ç¸½å…±ç™¼ç¾ {len(discovered_urls)} å€‹æœ‰æ•ˆ URL")
        print(f"ğŸ’¾ æ‰€æœ‰æ•¸æ“šå·²å„²å­˜åˆ°è³‡æ–™åº«")
        print(f"ğŸ¯ ä¸‹ä¸€æ­¥: åŸ·è¡Œ 'make crawl-data' é€²è¡Œå…§å®¹çˆ¬å–")
        
        # ä»ç„¶ä¿å­˜åˆ°æª”æ¡ˆä»¥ä¾¿å…¼å®¹æ€§
        urls_file = Path("discovered_urls.txt")
        with open(urls_file, 'w', encoding='utf-8') as f:
            for url in discovered_urls:
                f.write(f"{url}\n")
        
        print(f"ğŸ“„ URL åˆ—è¡¨ä¹Ÿå·²ä¿å­˜åˆ°: {urls_file}")
        
        # é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆ
        try:
            sitemap_stats = db_ops.get_sitemap_stats()
            if sitemap_stats:
                print(f"\nğŸ“Š è³‡æ–™åº«çµ±è¨ˆ:")
                for table_name, stats in sitemap_stats.items():
                    print(f"  {table_name}: {stats.get('count', 0)} æ¢è¨˜éŒ„")
        except Exception as e:
            logger.warning(f"ç„¡æ³•ç²å–çµ±è¨ˆä¿¡æ¯: {e}")
        
        return {
            'discovered_urls': discovered_urls,
            'robots_info': robots_info,
            'total_count': len(discovered_urls)
        }
        
    except Exception as e:
        logger.error(f"Robot éšæ®µå¤±æ•—: {e}")
        return {'discovered_urls': [], 'robots_info': {}}

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="çˆ¬èŸ² Robot éšæ®µ")
    parser.add_argument("--robots", nargs="+", help="æŒ‡å®š robots.txt URL")
    parser.add_argument("--save-to-db", action="store_true", help="ä¿å­˜çµæœåˆ°è³‡æ–™åº«")
    
    args = parser.parse_args()
    
    # åŸ·è¡Œ Robot éšæ®µ
    try:
        result = asyncio.run(crawl_robot_phase(args.robots))
        
        if result['discovered_urls']:
            print(f"\nğŸ‰ Robot éšæ®µå®Œæˆ! ç™¼ç¾ {len(result['discovered_urls'])} å€‹ URL")
            sys.exit(0)
        else:
            print(f"\nâŒ Robot éšæ®µå¤±æ•—!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹å¼ç•°å¸¸: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
