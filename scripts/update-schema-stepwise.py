#!/usr/bin/env python3
"""
æ›´æ–°è³‡æ–™åº« Schema - é€æ­¥æ·»åŠ  Sitemap ç›¸é—œè¡¨æ ¼
ç”±æ–¼ Supabase çš„é™åˆ¶ï¼Œæˆ‘å€‘éœ€è¦é€æ­¥åŸ·è¡Œ SQL èªå¥
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import logging
from database.client import SupabaseClient

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# å®šç¾©è¦åŸ·è¡Œçš„ SQL èªå¥ï¼ˆåˆ†æ­¥åŸ·è¡Œï¼‰
SQL_STATEMENTS = [
    # 1. å‰µå»º sitemaps è¡¨æ ¼
    """
    CREATE TABLE IF NOT EXISTS sitemaps (
        id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
        url TEXT UNIQUE NOT NULL,
        type TEXT NOT NULL DEFAULT 'sitemap',
        status TEXT DEFAULT 'pending',
        title TEXT,
        description TEXT,
        lastmod TIMESTAMP WITH TIME ZONE,
        changefreq TEXT,
        priority DECIMAL(2,1),
        urls_count INTEGER DEFAULT 0,
        parsed_at TIMESTAMP WITH TIME ZONE,
        error_message TEXT,
        metadata JSONB NOT NULL DEFAULT '{}',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );
    """,
    
    # 2. å‰µå»º robots_txt è¡¨æ ¼
    """
    CREATE TABLE IF NOT EXISTS robots_txt (
        id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
        domain TEXT UNIQUE NOT NULL,
        robots_url TEXT NOT NULL,
        content TEXT NOT NULL,
        parsed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        sitemaps_count INTEGER DEFAULT 0,
        rules_count INTEGER DEFAULT 0,
        metadata JSONB NOT NULL DEFAULT '{}',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );
    """,
    
    # 3. å‰µå»º discovered_urls è¡¨æ ¼ (éœ€è¦åœ¨ sitemaps ä¹‹å¾Œ)
    """
    CREATE TABLE IF NOT EXISTS discovered_urls (
        id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
        url TEXT UNIQUE NOT NULL,
        source_sitemap_id UUID REFERENCES sitemaps(id) ON DELETE CASCADE,
        url_type TEXT DEFAULT 'content',
        priority DECIMAL(2,1),
        changefreq TEXT,
        lastmod TIMESTAMP WITH TIME ZONE,
        crawl_status TEXT DEFAULT 'pending',
        crawl_attempts INTEGER DEFAULT 0,
        last_crawl_at TIMESTAMP WITH TIME ZONE,
        error_message TEXT,
        article_id UUID REFERENCES articles(id) ON DELETE SET NULL,
        metadata JSONB NOT NULL DEFAULT '{}',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );
    """,
    
    # 4. å‰µå»º sitemap_hierarchy è¡¨æ ¼
    """
    CREATE TABLE IF NOT EXISTS sitemap_hierarchy (
        id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
        parent_sitemap_id UUID REFERENCES sitemaps(id) ON DELETE CASCADE,
        child_sitemap_id UUID REFERENCES sitemaps(id) ON DELETE CASCADE,
        level INTEGER DEFAULT 0,
        discovery_order INTEGER DEFAULT 0,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        UNIQUE(parent_sitemap_id, child_sitemap_id)
    );
    """,
]

# ç´¢å¼•èªå¥
INDEX_STATEMENTS = [
    "CREATE INDEX IF NOT EXISTS idx_sitemaps_url ON sitemaps(url);",
    "CREATE INDEX IF NOT EXISTS idx_sitemaps_type ON sitemaps(type);",
    "CREATE INDEX IF NOT EXISTS idx_sitemaps_status ON sitemaps(status);",
    "CREATE INDEX IF NOT EXISTS idx_discovered_urls_url ON discovered_urls(url);",
    "CREATE INDEX IF NOT EXISTS idx_discovered_urls_status ON discovered_urls(crawl_status);",
    "CREATE INDEX IF NOT EXISTS idx_robots_txt_domain ON robots_txt(domain);",
]

def execute_sql_via_function(client, sql: str) -> bool:
    """é€šé RPC å‡½æ•¸åŸ·è¡Œ SQL"""
    try:
        # å‰µå»ºä¸€å€‹è‡¨æ™‚å‡½æ•¸ä¾†åŸ·è¡Œ SQL
        create_temp_func = f"""
        CREATE OR REPLACE FUNCTION temp_execute_sql()
        RETURNS TEXT
        LANGUAGE plpgsql
        AS $$
        BEGIN
            {sql}
            RETURN 'SUCCESS';
        EXCEPTION
            WHEN OTHERS THEN
                RETURN 'ERROR: ' || SQLERRM;
        END;
        $$;
        """
        
        # å…ˆå‰µå»ºå‡½æ•¸
        result = client.rpc('query', {'query': create_temp_func}).execute()
        
        # ç„¶å¾ŒåŸ·è¡Œå‡½æ•¸
        result = client.rpc('temp_execute_sql').execute()
        
        if result.data and 'SUCCESS' in str(result.data):
            return True
        else:
            logger.warning(f"SQL åŸ·è¡Œçµæœ: {result.data}")
            return False
            
    except Exception as e:
        logger.warning(f"SQL åŸ·è¡Œå¤±æ•—: {e}")
        return False

def update_schema_step_by_step():
    """é€æ­¥æ›´æ–°è³‡æ–™åº« schema"""
    
    try:
        logger.info("ğŸ“¡ é€£æ¥åˆ° Supabase...")
        client = SupabaseClient()
        supabase = client.get_client()
        
        logger.info("ğŸ”§ é–‹å§‹é€æ­¥åŸ·è¡Œ schema æ›´æ–°...")
        
        # åŸ·è¡Œè¡¨æ ¼å‰µå»ºèªå¥
        for i, sql in enumerate(SQL_STATEMENTS, 1):
            logger.info(f"ğŸ“ åŸ·è¡Œè¡¨æ ¼å‰µå»ºèªå¥ {i}/{len(SQL_STATEMENTS)}")
            try:
                # å˜—è©¦ç›´æ¥å‰µå»ºè¡¨æ ¼ (ä½¿ç”¨ Python ä»£ç¢¼)
                if "sitemaps" in sql and "CREATE TABLE" in sql:
                    # ä½¿ç”¨æ¨¡å‹å‰µå»ºçš„æ–¹å¼æ¸¬è©¦è¡¨æ ¼æ˜¯å¦å­˜åœ¨
                    try:
                        supabase.table("sitemaps").select("*").limit(1).execute()
                        logger.info("âœ… sitemaps è¡¨æ ¼å·²å­˜åœ¨")
                        continue
                    except:
                        logger.info("âŒ sitemaps è¡¨æ ¼ä¸å­˜åœ¨ï¼Œéœ€è¦å‰µå»º")
                
                elif "robots_txt" in sql and "CREATE TABLE" in sql:
                    try:
                        supabase.table("robots_txt").select("*").limit(1).execute()
                        logger.info("âœ… robots_txt è¡¨æ ¼å·²å­˜åœ¨")
                        continue
                    except:
                        logger.info("âŒ robots_txt è¡¨æ ¼ä¸å­˜åœ¨ï¼Œéœ€è¦å‰µå»º")
                
                elif "discovered_urls" in sql and "CREATE TABLE" in sql:
                    try:
                        supabase.table("discovered_urls").select("*").limit(1).execute()
                        logger.info("âœ… discovered_urls è¡¨æ ¼å·²å­˜åœ¨")
                        continue
                    except:
                        logger.info("âŒ discovered_urls è¡¨æ ¼ä¸å­˜åœ¨ï¼Œéœ€è¦å‰µå»º")
                
                elif "sitemap_hierarchy" in sql and "CREATE TABLE" in sql:
                    try:
                        supabase.table("sitemap_hierarchy").select("*").limit(1).execute()
                        logger.info("âœ… sitemap_hierarchy è¡¨æ ¼å·²å­˜åœ¨")
                        continue
                    except:
                        logger.info("âŒ sitemap_hierarchy è¡¨æ ¼ä¸å­˜åœ¨ï¼Œéœ€è¦å‰µå»º")
                
                # å˜—è©¦ä½¿ç”¨ RPC åŸ·è¡Œ SQL
                if execute_sql_via_function(supabase, sql.strip()):
                    logger.info(f"âœ… èªå¥ {i} åŸ·è¡ŒæˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ èªå¥ {i} åŸ·è¡Œå¤±æ•—æˆ–è¡¨æ ¼å·²å­˜åœ¨")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ èªå¥ {i} åŸ·è¡Œç•°å¸¸: {e}")
                continue
        
        # åŸ·è¡Œç´¢å¼•å‰µå»ºèªå¥
        logger.info("ğŸ“Š å‰µå»ºç´¢å¼•...")
        for i, sql in enumerate(INDEX_STATEMENTS, 1):
            try:
                if execute_sql_via_function(supabase, sql.strip()):
                    logger.info(f"âœ… ç´¢å¼• {i} å‰µå»ºæˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ ç´¢å¼• {i} å‰µå»ºå¤±æ•—æˆ–å·²å­˜åœ¨")
            except Exception as e:
                logger.warning(f"âš ï¸ ç´¢å¼• {i} å‰µå»ºç•°å¸¸: {e}")
                continue
        
        logger.info("ğŸ” é©—è­‰è¡¨æ ¼å‰µå»ºçµæœ...")
        
        # é©—è­‰è¡¨æ ¼æ˜¯å¦å‰µå»ºæˆåŠŸ
        tables_to_check = ["sitemaps", "discovered_urls", "robots_txt", "sitemap_hierarchy"]
        success_count = 0
        
        for table in tables_to_check:
            try:
                result = supabase.table(table).select("*").limit(1).execute()
                logger.info(f"âœ… {table} è¡¨æ ¼é©—è­‰æˆåŠŸ")
                success_count += 1
            except Exception as e:
                logger.error(f"âŒ {table} è¡¨æ ¼é©—è­‰å¤±æ•—: {e}")
        
        if success_count == len(tables_to_check):
            logger.info("ğŸ‰ æ‰€æœ‰è¡¨æ ¼å‰µå»ºæˆåŠŸ!")
            return True
        else:
            logger.warning(f"âš ï¸ éƒ¨åˆ†è¡¨æ ¼å‰µå»ºæˆåŠŸ ({success_count}/{len(tables_to_check)})")
            return False
        
    except Exception as e:
        logger.error(f"Schema æ›´æ–°å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ”§ æ›´æ–°è³‡æ–™åº« Schema - æ·»åŠ  Sitemap è¡¨æ ¼")
    print("=" * 50)
    
    if update_schema_step_by_step():
        print("âœ… Schema æ›´æ–°æˆåŠŸ!")
    else:
        print("âš ï¸ Schema æ›´æ–°éƒ¨åˆ†å®Œæˆï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
