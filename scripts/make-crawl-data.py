#!/usr/bin/env python3
"""
make-crawl-data.py
çˆ¬èŸ²ç¬¬äºŒéšæ®µï¼šæ·±åº¦çˆ¬å–è³‡æ–™å…§å®¹
æ”¯æ´å¤šç¨®çˆ¬å–ç­–ç•¥ï¼šDFSã€BFSã€å¾æª”æ¡ˆè®€å–
"""

import sys
import asyncio
from pathlib import Path
from typing import List, Dict
from urllib.parse import urlparse
from datetime import datetime

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from database.client import SupabaseClient
from database.operations import DatabaseOperations
from database.models import ArticleModel, CrawlStatus
from config import Config
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

class CrawlStrategy:
    """çˆ¬å–ç­–ç•¥åŸºé¡"""
    
    def __init__(self, max_urls: int = 10):
        self.max_urls = max_urls
        self.crawled_count = 0
    
    async def crawl_with_crawl4ai(self, url: str) -> dict:
        """ä½¿ç”¨ crawl4ai çˆ¬å–å–®å€‹ç¶²å€"""
        try:
            from crawl4ai import AsyncWebCrawler
            
            print(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–: {url}")
            
            async with AsyncWebCrawler(headless=True, verbose=False) as crawler:
                result = await crawler.arun(
                    url=url,
                    word_count_threshold=10,
                    bypass_cache=True
                )
                
                if result.success:
                    content = result.markdown or result.cleaned_html or "ç„¡å…§å®¹"
                    metadata = result.metadata or {}
                    title = metadata.get('title', f"ä¾†è‡ª {urlparse(url).netloc} çš„æ–‡ç« ")
                    
                    return {
                        'success': True,
                        'url': url,
                        'title': title,
                        'content': content,
                        'word_count': len(content.split()),
                        'metadata': metadata
                    }
                else:
                    return {
                        'success': False,
                        'url': url,
                        'error': "çˆ¬å–å¤±æ•—"
                    }
                    
        except Exception as e:
            print(f"âŒ çˆ¬å–éŒ¯èª¤ {url}: {e}")
            return {
                'success': False,
                'url': url,
                'error': str(e)
            }

class DFSCrawlStrategy(CrawlStrategy):
    """æ·±åº¦å„ªå…ˆæœå°‹çˆ¬å–ç­–ç•¥"""
    
    async def crawl(self, start_urls: List[str], db_ops: DatabaseOperations) -> int:
        """ä½¿ç”¨ crawl4ai çš„ DFS ç­–ç•¥é€²è¡Œæ·±åº¦çˆ¬å–"""
        try:
            from crawl4ai import AsyncWebCrawler
            from crawl4ai.async_crawler_strategy import AsyncCrawlerStrategy
            
            print("ğŸŒŠ ä½¿ç”¨ DFS æ·±åº¦å„ªå…ˆç­–ç•¥çˆ¬å–")
            
            success_count = 0
            
            async with AsyncWebCrawler(headless=True, verbose=False) as crawler:
                for start_url in start_urls[:3]:  # é™åˆ¶èµ·å§‹ URL æ•¸é‡
                    if self.crawled_count >= self.max_urls:
                        break
                    
                    print(f"ğŸ¯ DFS èµ·å§‹é»: {start_url}")
                    
                    # ä½¿ç”¨ crawl4ai çš„å…§å»º DFS ç­–ç•¥
                    try:
                        from crawl4ai.crawling_strategy import BFSCrawlerStrategy
                        
                        # å‰µå»ºçˆ¬å–ç­–ç•¥
                        strategy = BFSCrawlerStrategy(
                            max_depth=2,
                            max_pages=min(10, self.max_urls - self.crawled_count)
                        )
                        
                        # åŸ·è¡Œæ·±åº¦çˆ¬å–
                        results = await crawler.arun_many(
                            urls=[start_url],
                            strategy=strategy,
                            word_count_threshold=10,
                            bypass_cache=True
                        )
                        
                        for result in results:
                            if self.crawled_count >= self.max_urls:
                                break
                                
                            if result.success:
                                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                                if db_ops.article_exists(result.url):
                                    print(f"â­ï¸ è·³é (å·²å­˜åœ¨): {result.url}")
                                    continue
                                
                                # è™•ç†å…§å®¹
                                content = result.markdown or result.cleaned_html or "ç„¡å…§å®¹"
                                metadata = result.metadata or {}
                                
                                if len(content.split()) < 10:
                                    print(f"âš ï¸ å…§å®¹å¤ªçŸ­ï¼Œè·³é: {result.url}")
                                    continue
                                
                                # å‰µå»ºæ–‡ç« è¨˜éŒ„
                                article = ArticleModel(
                                    url=result.url,
                                    title=metadata.get('title', f"DFS æ–‡ç«  {self.crawled_count + 1}"),
                                    content=content,
                                    metadata={
                                        'word_count': len(content.split()),
                                        'content_type': 'markdown',
                                        'crawl_strategy': 'DFS',
                                        'crawl_timestamp': datetime.now().isoformat(),
                                        'source': 'crawl4ai_dfs',
                                        **metadata
                                    }
                                )
                                
                                # ä¿å­˜åˆ°è³‡æ–™åº«
                                if db_ops.create_article(article):
                                    success_count += 1
                                    self.crawled_count += 1
                                    print(f"âœ… DFS æˆåŠŸ: {article.title[:50]}...")
                                
                    except ImportError:
                        # å¦‚æœæ²’æœ‰ç­–ç•¥æ¨¡çµ„ï¼Œä½¿ç”¨åŸºæœ¬æ–¹æ³•
                        print("âš ï¸ ä½¿ç”¨åŸºæœ¬ DFS æ–¹æ³•")
                        result = await self.crawl_with_crawl4ai(start_url)
                        if result['success'] and not db_ops.article_exists(result['url']):
                            if result['word_count'] >= 10:
                                article = ArticleModel(
                                    url=result['url'],
                                    title=result['title'],
                                    content=result['content'],
                                    metadata={**result['metadata'], 'crawl_strategy': 'DFS_basic'}
                                )
                                if db_ops.create_article(article):
                                    success_count += 1
                                    self.crawled_count += 1
                    
                    except Exception as e:
                        logger.error(f"DFS çˆ¬å–å¤±æ•— {start_url}: {e}")
                        continue
            
            return success_count
            
        except Exception as e:
            logger.error(f"DFS ç­–ç•¥å¤±æ•—: {e}")
            return 0

class BFSCrawlStrategy(CrawlStrategy):
    """å»£åº¦å„ªå…ˆæœå°‹çˆ¬å–ç­–ç•¥"""
    
    async def crawl(self, start_urls: List[str], db_ops: DatabaseOperations) -> int:
        """ä½¿ç”¨ BFS ç­–ç•¥é€²è¡Œå»£åº¦çˆ¬å–"""
        print("ğŸŒ ä½¿ç”¨ BFS å»£åº¦å„ªå…ˆç­–ç•¥çˆ¬å–")
        
        success_count = 0
        queue = start_urls[:self.max_urls]
        
        while queue and self.crawled_count < self.max_urls:
            url = queue.pop(0)
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if db_ops.article_exists(url):
                print(f"â­ï¸ è·³é (å·²å­˜åœ¨): {url}")
                continue
            
            # çˆ¬å–å…§å®¹
            result = await self.crawl_with_crawl4ai(url)
            
            if result['success'] and result['word_count'] >= 10:
                # å‰µå»ºæ–‡ç« è¨˜éŒ„
                article = ArticleModel(
                    url=result['url'],
                    title=result['title'],
                    content=result['content'],
                    metadata={
                        **result['metadata'],
                        'crawl_strategy': 'BFS',
                        'crawl_timestamp': datetime.now().isoformat()
                    }
                )
                
                # ä¿å­˜åˆ°è³‡æ–™åº«
                if db_ops.create_article(article):
                    success_count += 1
                    self.crawled_count += 1
                    print(f"âœ… BFS æˆåŠŸ [{self.crawled_count}/{self.max_urls}]: {article.title[:50]}...")
        
        return success_count

class DatabaseCrawlStrategy(CrawlStrategy):
    """åŸºæ–¼è³‡æ–™åº«çš„çˆ¬å–ç­–ç•¥ - å¾ discovered_urls è¡¨æ ¼è®€å–å¾…çˆ¬å–çš„ URL"""
    
    def __init__(self, max_urls: int = 10, url_type: str = "content"):
        super().__init__(max_urls)
        self.url_type = url_type
    
    async def crawl(self, start_urls: List[str], db_ops: DatabaseOperations) -> int:
        """å¾è³‡æ–™åº«è®€å–å¾…çˆ¬å–çš„ URL"""
        print(f"ğŸ—„ï¸ å¾è³‡æ–™åº«è®€å–å¾…çˆ¬å–çš„ URL (é¡å‹: {self.url_type})")
        
        # å¾è³‡æ–™åº«ç²å–å¾…çˆ¬å–çš„ URL
        try:
            pending_urls = db_ops.get_pending_urls(limit=self.max_urls, url_type=self.url_type)
            print(f"ğŸ“‹ å¾è³‡æ–™åº«è¼‰å…¥ {len(pending_urls)} å€‹å¾…çˆ¬å– URL")
            
            if not pending_urls:
                print("âš ï¸ æ²’æœ‰æ‰¾åˆ°å¾…çˆ¬å–çš„ URLï¼Œå˜—è©¦è®€å–æª”æ¡ˆ...")
                return await self._fallback_to_file(start_urls, db_ops)
            
        except Exception as e:
            logger.error(f"å¾è³‡æ–™åº«è®€å– URL å¤±æ•—: {e}")
            print("âš ï¸ è³‡æ–™åº«è®€å–å¤±æ•—ï¼Œå˜—è©¦è®€å–æª”æ¡ˆ...")
            return await self._fallback_to_file(start_urls, db_ops)
        
        success_count = 0
        
        for i, discovered_url in enumerate(pending_urls, 1):
            url = discovered_url.url
            print(f"ğŸ“„ [{i}/{len(pending_urls)}] {url}")
            
            # æ›´æ–°ç‹€æ…‹ç‚ºçˆ¬å–ä¸­
            db_ops.update_discovered_url_status(
                discovered_url.id, 
                CrawlStatus.CRAWLING.value
            )
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–‡ç« 
            if db_ops.article_exists(url):
                print(f"â­ï¸ è·³é (æ–‡ç« å·²å­˜åœ¨): {url}")
                # æ›´æ–°ç‹€æ…‹ç‚ºå·²å®Œæˆä¸¦é—œè¯åˆ°æ–‡ç« 
                existing_article = db_ops.get_article_by_url(url)
                if existing_article:
                    db_ops.update_discovered_url_status(
                        discovered_url.id,
                        CrawlStatus.COMPLETED.value,
                        existing_article.id
                    )
                continue
            
            # çˆ¬å–å…§å®¹
            result = await self.crawl_with_crawl4ai(url)
            
            if result['success']:
                # æª¢æŸ¥å…§å®¹å“è³ª
                word_count = result['word_count']
                if word_count < 10:
                    print(f"âš ï¸ å…§å®¹å¤ªçŸ­ ({word_count} å­—)ï¼Œè·³é")
                    db_ops.update_discovered_url_status(
                        discovered_url.id,
                        CrawlStatus.SKIPPED.value,
                        error_message="å…§å®¹å¤ªçŸ­"
                    )
                    continue
                
                # å‰µå»ºæ–‡ç« è¨˜éŒ„
                article = ArticleModel(
                    url=url,
                    title=result['title'],
                    content=result['content'],
                    metadata={
                        **result['metadata'],
                        'crawl_strategy': 'database',
                        'source_sitemap_id': discovered_url.source_sitemap_id,
                        'original_priority': discovered_url.priority,
                        'crawl_timestamp': datetime.now().isoformat()
                    }
                )
                
                # ä¿å­˜åˆ°è³‡æ–™åº«
                if db_ops.create_article(article):
                    success_count += 1
                    print(f"âœ… æˆåŠŸä¿å­˜: {article.title[:50]}... ({word_count} å­—)")
                    
                    # æ›´æ–° discovered_url ç‹€æ…‹ç‚ºå·²å®Œæˆ
                    db_ops.update_discovered_url_status(
                        discovered_url.id,
                        CrawlStatus.COMPLETED.value,
                        article.id
                    )
                else:
                    print(f"âŒ ä¿å­˜å¤±æ•—: {url}")
                    db_ops.update_discovered_url_status(
                        discovered_url.id,
                        CrawlStatus.ERROR.value,
                        error_message="æ–‡ç« ä¿å­˜å¤±æ•—"
                    )
            else:
                print(f"âŒ çˆ¬å–å¤±æ•—: {result.get('error', 'Unknown error')}")
                db_ops.update_discovered_url_status(
                    discovered_url.id,
                    CrawlStatus.ERROR.value,
                    error_message=result.get('error', 'Unknown error')
                )
        
        return success_count
    
    async def _fallback_to_file(self, start_urls: List[str], db_ops: DatabaseOperations) -> int:
        """ç•¶è³‡æ–™åº«æ²’æœ‰å¾…çˆ¬å– URL æ™‚ï¼Œå›é€€åˆ°æª”æ¡ˆè®€å–"""
        file_strategy = FileBasedCrawlStrategy(max_urls=self.max_urls)
        return await file_strategy.crawl(start_urls, db_ops)

class FileBasedCrawlStrategy(CrawlStrategy):
    """åŸºæ–¼æª”æ¡ˆçš„çˆ¬å–ç­–ç•¥"""
    
    def __init__(self, max_urls: int = 10, urls_file: str = "discovered_urls.txt"):
        super().__init__(max_urls)
        self.urls_file = urls_file
    
    async def crawl(self, start_urls: List[str], db_ops: DatabaseOperations) -> int:
        """å¾æª”æ¡ˆè®€å– URL é€²è¡Œçˆ¬å–"""
        print(f"ğŸ“ å¾æª”æ¡ˆè®€å– URL: {self.urls_file}")
        
        # è®€å–æª”æ¡ˆä¸­çš„ URL
        urls_to_crawl = []
        try:
            if Path(self.urls_file).exists():
                with open(self.urls_file, 'r', encoding='utf-8') as f:
                    urls_to_crawl = [line.strip() for line in f if line.strip()]
                print(f"ğŸ“‹ å¾æª”æ¡ˆè¼‰å…¥ {len(urls_to_crawl)} å€‹ URL")
            else:
                print(f"âš ï¸ æª”æ¡ˆä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­ URL")
                urls_to_crawl = start_urls
        except Exception as e:
            logger.error(f"è®€å–æª”æ¡ˆå¤±æ•—: {e}")
            urls_to_crawl = start_urls
        
        # é™åˆ¶æ•¸é‡
        urls_to_crawl = urls_to_crawl[:self.max_urls]
        success_count = 0
        
        for i, url in enumerate(urls_to_crawl, 1):
            print(f"ğŸ“„ [{i}/{len(urls_to_crawl)}] {url}")
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if db_ops.article_exists(url):
                print(f"â­ï¸ è·³é (å·²å­˜åœ¨): {url}")
                continue
            
            # çˆ¬å–å…§å®¹
            result = await self.crawl_with_crawl4ai(url)
            
            if result['success']:
                # æª¢æŸ¥å…§å®¹å“è³ª
                word_count = result['word_count']
                if word_count < 10:
                    print(f"âš ï¸ å…§å®¹å¤ªçŸ­ ({word_count} å­—)ï¼Œè·³é")
                    continue
                
                # å‰µå»ºæ–‡ç« è¨˜éŒ„
                article = ArticleModel(
                    url=url,
                    title=result['title'],
                    content=result['content'],
                    metadata={
                        **result['metadata'],
                        'crawl_strategy': 'file_based',
                        'source_file': self.urls_file,
                        'crawl_timestamp': datetime.now().isoformat()
                    }
                )
                
                # ä¿å­˜åˆ°è³‡æ–™åº«
                try:
                    if db_ops.create_article(article):
                        success_count += 1
                        print(f"âœ… æˆåŠŸå„²å­˜: {article.title[:50]}...")
                        print(f"   ğŸ“ å…§å®¹: {word_count} å­—")
                    else:
                        print(f"âŒ å„²å­˜å¤±æ•—: å¯èƒ½æ˜¯è³‡æ–™åº«éŒ¯èª¤")
                except Exception as e:
                    if "å·²å­˜åœ¨" in str(e):
                        print(f"â­ï¸ æ–‡ç« å·²å­˜åœ¨ï¼Œè·³é")
                    else:
                        print(f"âŒ å„²å­˜éŒ¯èª¤: {e}")
            else:
                print(f"âŒ çˆ¬å–å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
        
        return success_count

def get_default_urls() -> List[str]:
    """ç²å–é è¨­ URL"""
    urls = []
    
    # å¾é…ç½®ç²å–ç¶²å€
    if hasattr(Config, 'TARGET_URLS') and Config.TARGET_URLS:
        urls.extend([url.strip() for url in Config.TARGET_URLS if url.strip()])
    
    # å¦‚æœæ²’æœ‰é…ç½®ï¼Œä½¿ç”¨é è¨­ç¶²å€
    if not urls:
        urls = [
            "https://httpbin.org/html",
            "https://httpbin.org/json"
        ]
    
    return urls

async def crawl_data_phase(strategy: str = "database", max_urls: int = 10, urls: List[str] = None):
    """
    çˆ¬èŸ²ç¬¬äºŒéšæ®µï¼šè³‡æ–™çˆ¬å–
    æ”¯æ´å¤šç¨®ç­–ç•¥ï¼šdatabase, file, dfs, bfs
    """
    print("ğŸ“Š RAG çˆ¬èŸ²éšæ®µ 2: è³‡æ–™å…§å®¹çˆ¬å–")
    print("=" * 60)
    
    if urls is None:
        urls = get_default_urls()
    
    try:
        # åˆå§‹åŒ–è³‡æ–™åº«
        print("ğŸ”— é€£æ¥è³‡æ–™åº«...")
        supabase_client = SupabaseClient()
        db_ops = DatabaseOperations(supabase_client.get_client())
        
        # é¸æ“‡çˆ¬å–ç­–ç•¥
        if strategy.lower() == "database" or strategy.lower() == "db":
            crawler = DatabaseCrawlStrategy(max_urls)
            print("ğŸ—„ï¸ ä½¿ç”¨è³‡æ–™åº«é©…å‹•ç­–ç•¥")
        elif strategy.lower() == "dfs":
            crawler = DFSCrawlStrategy(max_urls)
            print("ğŸŒŠ ä½¿ç”¨æ·±åº¦å„ªå…ˆ (DFS) ç­–ç•¥")
        elif strategy.lower() == "bfs":
            crawler = BFSCrawlStrategy(max_urls)
            print("ğŸŒ ä½¿ç”¨å»£åº¦å„ªå…ˆ (BFS) ç­–ç•¥")
        else:  # file
            crawler = FileBasedCrawlStrategy(max_urls)
            print("ğŸ“ ä½¿ç”¨æª”æ¡ˆé©…å‹•ç­–ç•¥")
        
        # åŸ·è¡Œçˆ¬å–
        success_count = await crawler.crawl(urls, db_ops)
        
        # ç¸½çµ
        print(f"ğŸ“Š è³‡æ–™çˆ¬å–å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {success_count}/{max_urls}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {(success_count/max_urls*100):.1f}%")
        
        # é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆ
        try:
            stats = db_ops.get_database_statistics()
            if stats:
                print(f"\nğŸ“ˆ è³‡æ–™åº«çµ±è¨ˆ:")
                for table_name, stat in stats.items():
                    if isinstance(stat, dict) and 'count' in stat:
                        print(f"  {table_name}: {stat['count']} æ¢è¨˜éŒ„")
        except Exception as e:
            logger.warning(f"ç„¡æ³•ç²å–çµ±è¨ˆä¿¡æ¯: {e}")
        
        if success_count > 0:
            print("ğŸ¯ ä¸‹ä¸€æ­¥: åŸ·è¡Œ 'make chunk' é€²è¡Œè³‡æ–™åˆ†å¡Š")
            return True
        else:
            print("âŒ æ²’æœ‰æˆåŠŸçˆ¬å–ä»»ä½•è³‡æ–™")
            return False
            
    except Exception as e:
        logger.error(f"è³‡æ–™çˆ¬å–éšæ®µå¤±æ•—: {e}")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="RAG è³‡æ–™çˆ¬å–å·¥å…· - ç¬¬äºŒéšæ®µ")
    parser.add_argument("--strategy", choices=["database", "file", "dfs", "bfs"], default="database",
                       help="çˆ¬å–ç­–ç•¥: database(è³‡æ–™åº«ä½‡åˆ—), file(æª”æ¡ˆ), dfs(æ·±åº¦å„ªå…ˆ), bfs(å»£åº¦å„ªå…ˆ)")
    parser.add_argument("--urls", nargs="+", help="æŒ‡å®šè¦çˆ¬å–çš„ç¶²å€")
    parser.add_argument("--max-urls", type=int, default=10, help="æœ€å¤§çˆ¬å–æ•¸é‡")
    
    args = parser.parse_args()
    
    # åŸ·è¡Œè³‡æ–™çˆ¬å–
    try:
        result = asyncio.run(crawl_data_phase(args.strategy, args.max_urls, args.urls))
        
        if result:
            print(f"\nğŸ‰ è³‡æ–™çˆ¬å–å®Œæˆ!")
            sys.exit(0)
        else:
            print(f"\nâŒ è³‡æ–™çˆ¬å–å¤±æ•—!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹å¼ç•°å¸¸: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

def get_target_urls() -> List[str]:
    """å¾ç’°å¢ƒè®Šæ•¸ç²å–ç›®æ¨™ç¶²å€"""
    urls = []
    
    # å¾é…ç½®ç²å–ç¶²å€
    if hasattr(Config, 'TARGET_URLS') and Config.TARGET_URLS:
        urls.extend([url.strip() for url in Config.TARGET_URLS if url.strip()])
        print(f"ğŸ“‹ å¾é…ç½®è¼‰å…¥ {len(urls)} å€‹ç¶²å€")
    
    # å¦‚æœæ²’æœ‰é…ç½®ï¼Œä½¿ç”¨é è¨­ç¶²å€
    if not urls:
        print("âš ï¸ æœªæ‰¾åˆ°é…ç½®çš„ç¶²å€ï¼Œä½¿ç”¨é è¨­æ¸¬è©¦ç¶²å€")
        urls = [
            "https://httpbin.org/html",
            "https://httpbin.org/json"
        ]
    
    return urls

async def crawl_with_crawl4ai(url: str) -> dict:
    """ä½¿ç”¨ crawl4ai çˆ¬å–å–®å€‹ç¶²å€"""
    try:
        from crawl4ai import AsyncWebCrawler
        
        print(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–: {url}")
        
        async with AsyncWebCrawler(headless=True, verbose=False) as crawler:
            result = await crawler.arun(
                url=url,
                word_count_threshold=10,
                bypass_cache=True
            )
            
            if result.success:
                content = result.markdown or result.cleaned_html or "ç„¡å…§å®¹"
                metadata = result.metadata or {}
                title = metadata.get('title', f"ä¾†è‡ª {urlparse(url).netloc} çš„æ–‡ç« ")
                
                return {
                    'success': True,
                    'url': url,
                    'title': title,
                    'content': content,
                    'word_count': len(content.split()),
                    'metadata': metadata
                }
            else:
                return {
                    'success': False,
                    'url': url,
                    'error': "çˆ¬å–å¤±æ•—"
                }
                
    except Exception as e:
        print(f"âŒ çˆ¬å–éŒ¯èª¤ {url}: {e}")
        return {
            'success': False,
            'url': url,
            'error': str(e)
        }

async def crawl_urls(urls: List[str] = None, max_urls: int = 10):
    """çˆ¬å–ç¶²å€åˆ—è¡¨"""
    print("ğŸ•·ï¸ RAG æ­¥é©Ÿ 2: è³‡æ–™çˆ¬å–")
    print("=" * 50)
    
    if urls is None:
        urls = get_target_urls()
    
    if not urls:
        print("âŒ æ²’æœ‰è¦çˆ¬å–çš„ç¶²å€")
        return False
    
    # é™åˆ¶æ•¸é‡
    urls = urls[:max_urls]
    print(f"ğŸ“Š æº–å‚™çˆ¬å– {len(urls)} å€‹ç¶²å€")
    
    try:
        # åˆå§‹åŒ–è³‡æ–™åº«
        print("ğŸ”— é€£æ¥è³‡æ–™åº«...")
        supabase_client = SupabaseClient()
        db_ops = DatabaseOperations(supabase_client)
        
        success_count = 0
        
        for i, url in enumerate(urls, 1):
            print(f"\nğŸ“„ [{i}/{len(urls)}] {url}")
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if db_ops.article_exists(url):
                print(f"â­ï¸ è·³é (å·²å­˜åœ¨): {url}")
                continue
            
            # çˆ¬å–å…§å®¹
            result = await crawl_with_crawl4ai(url)
            
            if result['success']:
                # æª¢æŸ¥å…§å®¹å“è³ª
                word_count = result['word_count']
                if word_count < 10:
                    print(f"âš ï¸ å…§å®¹å¤ªçŸ­ ({word_count} å­—)ï¼Œè·³é")
                    continue
                
                # å‰µå»ºæ–‡ç« è¨˜éŒ„
                article = ArticleModel(
                    url=url,
                    title=result['title'],
                    content=result['content'],
                    metadata={
                        'word_count': word_count,
                        'content_type': 'markdown',
                        'crawl_timestamp': datetime.now().isoformat(),
                        'source': 'crawl4ai',
                        **result.get('metadata', {})
                    }
                )
                
                # å„²å­˜åˆ°è³‡æ–™åº«
                try:
                    saved = db_ops.create_article(article)
                    if saved:
                        success_count += 1
                        print(f"âœ… æˆåŠŸå„²å­˜: {article.title[:50]}...")
                        print(f"   ğŸ“ å…§å®¹: {word_count} å­—")
                    else:
                        print(f"âŒ å„²å­˜å¤±æ•—: å¯èƒ½æ˜¯è³‡æ–™åº«éŒ¯èª¤")
                except Exception as e:
                    if "å·²å­˜åœ¨" in str(e):
                        print(f"â­ï¸ æ–‡ç« å·²å­˜åœ¨ï¼Œè·³é")
                    else:
                        print(f"âŒ å„²å­˜éŒ¯èª¤: {e}")
            else:
                print(f"âŒ çˆ¬å–å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
        
        # ç¸½çµ
        print(f"\nğŸ“Š çˆ¬å–å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {success_count}/{len(urls)}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {(success_count/len(urls)*100):.1f}%")
        
        if success_count > 0:
            print("ğŸ¯ ä¸‹ä¸€æ­¥: åŸ·è¡Œ 'make chunk' é€²è¡Œè³‡æ–™åˆ†å¡Š")
            return True
        else:
            print("âŒ æ²’æœ‰æˆåŠŸçˆ¬å–ä»»ä½•è³‡æ–™")
            return False
            
    except Exception as e:
        print(f"âŒ çˆ¬å–éç¨‹å‡ºéŒ¯: {e}")
        logger.exception("è©³ç´°éŒ¯èª¤ä¿¡æ¯")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="RAG è³‡æ–™çˆ¬å–å·¥å…·")
    parser.add_argument("--urls", nargs="+", help="æŒ‡å®šè¦çˆ¬å–çš„ç¶²å€")
    parser.add_argument("--max-urls", type=int, default=5, help="æœ€å¤§çˆ¬å–æ•¸é‡")
    
    args = parser.parse_args()
    
    # åŸ·è¡Œçˆ¬å–
    try:
        result = asyncio.run(crawl_urls(args.urls, args.max_urls))
        
        if result:
            print("\nğŸ‰ çˆ¬å–ä»»å‹™å®Œæˆ!")
            sys.exit(0)
        else:
            print("\nâŒ çˆ¬å–ä»»å‹™å¤±æ•—!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹å¼ç•°å¸¸: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
