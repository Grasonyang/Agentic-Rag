#!/usr/bin/env python3
"""
getUrls.py - URL æå–å’Œæ’åºè…³æœ¬

åŠŸèƒ½ï¼š
1. è®€å– Sitemap æ¸…å–®æ–‡ä»¶
2. ä¾åºè§£ææ¯å€‹ Sitemap
3. æå–æ‰€æœ‰ URL ä¸¦ä¿æŒåŸå§‹é †åº
4. éæ¿¾é‡è¤‡å’Œç„¡æ•ˆ URL
5. è¼¸å‡ºæœ‰åºçš„ URL æ¸…å–®

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/getUrls.py --sitemap-list sitemaps.txt
    python scripts/getUrls.py --sitemap-list sitemaps.txt --output urls.txt --max-urls 1000
    make get-urls SITEMAP_LIST=sitemaps.txt
"""

import argparse
import asyncio
import sys
import os
from pathlib import Path
from typing import List, Set, Dict, Optional
from urllib.parse import urlparse, urljoin
from datetime import datetime

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))

from spider.crawlers.sitemap_parser import SitemapParser
from database.client import SupabaseClient
from database.models import DiscoveredURLModel, URLType, CrawlStatus


class URLExtractor:
    """URL æå–å’Œç®¡ç†é¡"""
    
    def __init__(self, output_file: str = "urls.txt", max_urls: int = None):
        self.parser = SitemapParser()
        self.db_client = SupabaseClient()
        self.output_file = output_file
        self.max_urls = max_urls
        self.discovered_urls: List[Dict] = []
        self.url_set: Set[str] = set()  # ç”¨æ–¼å»é‡
        self.stats = {
            'total_sitemaps': 0,
            'processed_sitemaps': 0,
            'total_urls': 0,
            'unique_urls': 0,
            'skipped_urls': 0,
            'errors': 0
        }
    
    def load_sitemap_list(self, sitemap_file: str) -> List[str]:
        """å¾æ–‡ä»¶è¼‰å…¥ Sitemap æ¸…å–®"""
        print(f"ğŸ“– æ­£åœ¨è®€å– Sitemap æ¸…å–®: {sitemap_file}")
        
        if not os.path.exists(sitemap_file):
            raise FileNotFoundError(f"Sitemap æ¸…å–®æ–‡ä»¶ä¸å­˜åœ¨: {sitemap_file}")
        
        sitemaps = []
        try:
            with open(sitemap_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    
                    # è·³éè¨»é‡‹å’Œç©ºè¡Œ
                    if line.startswith('#') or not line:
                        continue
                    
                    # è™•ç†ç¸®æ’çš„å­ sitemapï¼ˆä¿æŒå±¤æ¬¡çµæ§‹ï¼‰
                    if line.startswith('  '):
                        # å­ sitemapï¼Œç§»é™¤ç¸®æ’
                        clean_url = line.strip()
                        if self._is_valid_url(clean_url):
                            sitemaps.append(clean_url)
                    else:
                        # ä¸» sitemap
                        if self._is_valid_url(line):
                            sitemaps.append(line)
                        else:
                            print(f"âš ï¸ ç¬¬ {line_num} è¡Œ URL æ ¼å¼ä¸æ­£ç¢º: {line}")
            
            self.stats['total_sitemaps'] = len(sitemaps)
            print(f"âœ… è¼‰å…¥ {len(sitemaps)} å€‹ Sitemap")
            return sitemaps
            
        except Exception as e:
            raise Exception(f"è®€å– Sitemap æ¸…å–®å¤±æ•—: {e}")
    
    def _is_valid_url(self, url: str) -> bool:
        """é©—è­‰ URL æ ¼å¼"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False
    
    async def extract_urls_from_sitemap(self, sitemap_url: str) -> List[Dict]:
        """å¾å–®å€‹ Sitemap æå– URL"""
        print(f"ğŸ” æ­£åœ¨è§£æ Sitemap: {sitemap_url}")
        
        try:
            # ä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•ä¾†è§£æ sitemap
            urls_data = await self.parser.parse_sitemaps([sitemap_url])
            
            extracted_urls = []
            for entry in urls_data:
                url = entry.url if hasattr(entry, 'url') else str(entry)
                
                # åŸºæœ¬é©—è­‰
                if not self._is_valid_url(url):
                    self.stats['skipped_urls'] += 1
                    continue
                
                # å»é‡æª¢æŸ¥
                if url in self.url_set:
                    self.stats['skipped_urls'] += 1
                    continue
                
                self.url_set.add(url)
                
                # æ§‹å»º URL è³‡è¨Š
                url_data = {
                    'url': url,
                    'source_sitemap': sitemap_url,
                    'priority': getattr(entry, 'priority', None),
                    'changefreq': getattr(entry, 'changefreq', None),
                    'lastmod': getattr(entry, 'lastmod', None),
                    'discovered_at': datetime.now().isoformat(),
                    'url_type': self._classify_url_type(url)
                }
                
                extracted_urls.append(url_data)
                self.stats['total_urls'] += 1
                
                # æª¢æŸ¥æ˜¯å¦é”åˆ°æœ€å¤§ URL é™åˆ¶
                if self.max_urls and self.stats['total_urls'] >= self.max_urls:
                    print(f"âš ï¸ å·²é”åˆ°æœ€å¤§ URL é™åˆ¶: {self.max_urls}")
                    break
            
            print(f"âœ… å¾ {sitemap_url} æå– {len(extracted_urls)} å€‹ URL")
            return extracted_urls
            
        except Exception as e:
            print(f"âŒ è§£æ {sitemap_url} å¤±æ•—: {e}")
            self.stats['errors'] += 1
            return []
    
    def _classify_url_type(self, url: str) -> str:
        """æ ¹æ“š URL åˆ†é¡å…§å®¹é¡å‹"""
        # åŸºæœ¬çš„ URL åˆ†é¡é‚è¼¯
        url_lower = url.lower()
        
        # å¸¸è¦‹çš„éå…§å®¹é é¢
        if any(pattern in url_lower for pattern in [
            '/category/', '/tag/', '/archive/', '/page/',
            '/search/', '/feed/', '/rss/', '/sitemap'
        ]):
            return 'other'
        
        # åª’é«”æ–‡ä»¶
        if any(url_lower.endswith(ext) for ext in [
            '.jpg', '.jpeg', '.png', '.gif', '.pdf', '.doc', '.zip'
        ]):
            return 'media'
        
        # é»˜èªç‚ºå…§å®¹é é¢
        return 'content'
    
    async def process_all_sitemaps(self, sitemap_urls: List[str]) -> None:
        """è™•ç†æ‰€æœ‰ Sitemap"""
        print(f"\nğŸš€ é–‹å§‹è™•ç† {len(sitemap_urls)} å€‹ Sitemap")
        
        for i, sitemap_url in enumerate(sitemap_urls, 1):
            print(f"\n[{i}/{len(sitemap_urls)}] è™•ç†ä¸­...")
            
            urls = await self.extract_urls_from_sitemap(sitemap_url)
            self.discovered_urls.extend(urls)
            self.stats['processed_sitemaps'] += 1
            
            # å¦‚æœé”åˆ°æœ€å¤§ URL é™åˆ¶ï¼Œåœæ­¢è™•ç†
            if self.max_urls and self.stats['total_urls'] >= self.max_urls:
                break
            
            # æ·»åŠ å°å»¶é²é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
            await asyncio.sleep(0.5)
        
        self.stats['unique_urls'] = len(self.discovered_urls)
        print(f"\nâœ… è™•ç†å®Œæˆï¼ç¸½è¨ˆæå– {self.stats['unique_urls']} å€‹å”¯ä¸€ URL")
    
    async def save_to_database(self) -> None:
        """ä¿å­˜ URL åˆ°è³‡æ–™åº«"""
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ {len(self.discovered_urls)} å€‹ URL åˆ°è³‡æ–™åº«...")
        
        try:
            supabase = self.db_client.get_client()
            batch_size = 100
            saved_count = 0
            
            for i in range(0, len(self.discovered_urls), batch_size):
                batch = self.discovered_urls[i:i + batch_size]
                
                # æº–å‚™æ‰¹é‡æ’å…¥æ•¸æ“š
                batch_data = []
                for url_data in batch:
                    # ç²å–å°æ‡‰çš„ sitemap_idï¼ˆéœ€è¦æŸ¥è©¢è³‡æ–™åº«ï¼‰
                    sitemap_result = supabase.from_('sitemaps')\
                        .select('id')\
                        .eq('url', url_data['source_sitemap'])\
                        .limit(1)\
                        .execute()
                    
                    sitemap_id = None
                    if sitemap_result.data:
                        sitemap_id = sitemap_result.data[0]['id']
                    
                    if sitemap_id:
                        url_model = DiscoveredURLModel(
                            url=url_data['url'],
                            source_sitemap_id=sitemap_id,
                            url_type=URLType.CONTENT if url_data['url_type'] == 'content' else URLType.OTHER,
                            priority=url_data['priority'],
                            changefreq=url_data['changefreq'],
                            lastmod=datetime.fromisoformat(url_data['lastmod']) if url_data['lastmod'] else None,
                            crawl_status=CrawlStatus.PENDING,
                            metadata={
                                'discovered_from': 'sitemap',
                                'discovered_at': url_data['discovered_at'],
                                'url_type_detail': url_data['url_type']
                            }
                        )
                        
                        batch_data.append(url_model.to_dict())
                
                # æ‰¹é‡æ’å…¥
                if batch_data:
                    result = supabase.from_('discovered_urls')\
                        .upsert(batch_data, on_conflict='url')\
                        .execute()
                    
                    if result.data:
                        saved_count += len(result.data)
                        print(f"âœ… å·²ä¿å­˜æ‰¹æ¬¡ {i//batch_size + 1}: {len(result.data)} å€‹ URL")
                
            print(f"âœ… è³‡æ–™åº«ä¿å­˜å®Œæˆï¼ç¸½è¨ˆ: {saved_count} å€‹ URL")
            
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«ä¿å­˜å¤±æ•—: {e}")
    
    def save_to_file(self) -> None:
        """ä¿å­˜ URL æ¸…å–®åˆ°æ–‡ä»¶"""
        print(f"\nğŸ“ æ­£åœ¨ä¿å­˜ URL æ¸…å–®åˆ° {self.output_file}...")
        
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                # å¯«å…¥æ¨™é¡Œå’Œçµ±è¨ˆ
                f.write("# Discovered URLs\n")
                f.write(f"# Generated by getUrls.py\n")
                f.write(f"# Total unique URLs: {len(self.discovered_urls)}\n")
                f.write(f"# Processed sitemaps: {self.stats['processed_sitemaps']}\n")
                f.write(f"# Generated at: {datetime.now().isoformat()}\n\n")
                
                # æŒ‰ä¾†æº sitemap åˆ†çµ„å¯«å…¥
                current_sitemap = None
                for url_data in self.discovered_urls:
                    if url_data['source_sitemap'] != current_sitemap:
                        current_sitemap = url_data['source_sitemap']
                        f.write(f"\n# From: {current_sitemap}\n")
                    
                    # å¯«å…¥ URL å’Œé¡å¤–è³‡è¨Šï¼ˆè¨»é‡‹å½¢å¼ï¼‰
                    f.write(f"{url_data['url']}")
                    
                    # æ·»åŠ å…ƒæ•¸æ“šä½œç‚ºè¨»é‡‹
                    metadata = []
                    if url_data.get('priority'):
                        metadata.append(f"priority={url_data['priority']}")
                    if url_data.get('changefreq'):
                        metadata.append(f"freq={url_data['changefreq']}")
                    if url_data.get('lastmod'):
                        lastmod_str = url_data['lastmod']
                        if hasattr(lastmod_str, 'isoformat'):  # datetime ç‰©ä»¶
                            lastmod_str = lastmod_str.isoformat()
                        metadata.append(f"modified={lastmod_str[:10]}")  # åªé¡¯ç¤ºæ—¥æœŸ
                    
                    if metadata:
                        f.write(f"  # {', '.join(metadata)}")
                    
                    f.write("\n")
            
            print(f"âœ… URL æ¸…å–®å·²ä¿å­˜åˆ° {self.output_file}")
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±æ•—: {e}")
    
    def print_summary(self) -> None:
        """æ‰“å°æå–æ‘˜è¦"""
        print(f"\nğŸ“‹ URL æå–æ‘˜è¦:")
        print(f"=" * 50)
        print(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
        print(f"   â€¢ ç¸½ Sitemap æ•¸: {self.stats['total_sitemaps']}")
        print(f"   â€¢ å·²è™•ç† Sitemap: {self.stats['processed_sitemaps']}")
        print(f"   â€¢ ç™¼ç¾ URL ç¸½æ•¸: {self.stats['total_urls']}")
        print(f"   â€¢ å”¯ä¸€ URL æ•¸: {self.stats['unique_urls']}")
        print(f"   â€¢ è·³éé‡è¤‡/ç„¡æ•ˆ: {self.stats['skipped_urls']}")
        print(f"   â€¢ éŒ¯èª¤æ•¸: {self.stats['errors']}")
        
        # URL é¡å‹çµ±è¨ˆ
        type_counts = {}
        for url_data in self.discovered_urls:
            url_type = url_data['url_type']
            type_counts[url_type] = type_counts.get(url_type, 0) + 1
        
        if type_counts:
            print(f"\nğŸ·ï¸ URL é¡å‹åˆ†å¸ƒ:")
            for url_type, count in sorted(type_counts.items()):
                percentage = (count / len(self.discovered_urls)) * 100
                print(f"   â€¢ {url_type}: {count} ({percentage:.1f}%)")
        
        print(f"=" * 50)
        print(f"ğŸ¯ æº–å‚™çˆ¬å– {self.stats['unique_urls']} å€‹ URL")


async def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='URL æå–å’Œæ’åºå·¥å…·')
    parser.add_argument('--sitemap-list', required=True, help='Sitemap æ¸…å–®æ–‡ä»¶')
    parser.add_argument('--output', default='urls.txt', help='è¼¸å‡ºæ–‡ä»¶åç¨±')
    parser.add_argument('--max-urls', type=int, help='æœ€å¤§ URL æ•¸é‡é™åˆ¶')
    parser.add_argument('--no-db', action='store_true', help='ä¸ä¿å­˜åˆ°è³‡æ–™åº«')
    parser.add_argument('--content-only', action='store_true', help='åªæå–å…§å®¹é é¢ URL')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ é–‹å§‹ URL æå–æµç¨‹")
    print(f"ğŸ“– Sitemap æ¸…å–®: {args.sitemap_list}")
    print(f"ğŸ“ è¼¸å‡ºæ–‡ä»¶: {args.output}")
    if args.max_urls:
        print(f"ğŸ”¢ URL é™åˆ¶: {args.max_urls}")
    
    extractor = URLExtractor(args.output, args.max_urls)
    
    try:
        # 1. è¼‰å…¥ Sitemap æ¸…å–®
        sitemap_urls = extractor.load_sitemap_list(args.sitemap_list)
        
        # 2. è™•ç†æ‰€æœ‰ Sitemap
        await extractor.process_all_sitemaps(sitemap_urls)
        
        # 3. éæ¿¾å…§å®¹é¡å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.content_only:
            original_count = len(extractor.discovered_urls)
            extractor.discovered_urls = [
                url_data for url_data in extractor.discovered_urls
                if url_data['url_type'] == 'content'
            ]
            filtered_count = len(extractor.discovered_urls)
            print(f"ğŸ” å…§å®¹éæ¿¾: {original_count} -> {filtered_count} URLs")
        
        # 4. ä¿å­˜åˆ°è³‡æ–™åº«ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if not args.no_db and extractor.discovered_urls:
            await extractor.save_to_database()
        
        # 5. ä¿å­˜åˆ°æ–‡ä»¶
        if extractor.discovered_urls:
            extractor.save_to_file()
        
        # 6. æ‰“å°æ‘˜è¦
        extractor.print_summary()
        
        if extractor.discovered_urls:
            print(f"\nğŸ‰ URL æå–å®Œæˆï¼")
            print(f"ğŸ“„ ä¸‹ä¸€æ­¥: python scripts/getChunking.py --url-list {args.output}")
        else:
            print(f"\nâš ï¸ æœªæå–åˆ°ä»»ä½• URL")
        
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
