#!/usr/bin/env python3
"""
getEmbedding.py - å‘é‡åµŒå…¥å’Œè³‡æ–™åº«å­˜å„²è…³æœ¬

åŠŸèƒ½ï¼š
1. è®€å–åˆ†å¡Šæ¸…å–®æ–‡ä»¶
2. æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡åµŒå…¥
3. æ›´æ–°è³‡æ–™åº«ä¸­çš„å‘é‡æ•¸æ“š
4. è¨˜éŒ„åµŒå…¥è™•ç†æ—¥èªŒ
5. é©—è­‰å‘é‡å­˜å„²å®Œæ•´æ€§

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/getEmbedding.py --chunk-list chunks.txt
    python scripts/getEmbedding.py --chunk-list chunks.txt --batch-size 32 --device cuda
    make get-embedding CHUNK_LIST=chunks.txt
"""

import argparse
import asyncio
import sys
import os
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import re
import numpy as np

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))

from embedding.embedding import EmbeddingManager
from database.client import SupabaseClient
from database.models import SearchLogModel


class EmbeddingProcessor:
    """å‘é‡åµŒå…¥è™•ç†é¡"""
    
    def __init__(self, batch_size: int = 16, device: str = 'auto'):
        self.embedding_manager = EmbeddingManager(device=device)
        self.db_client = SupabaseClient()
        self.batch_size = batch_size
        
        # è™•ç†çµ±è¨ˆ
        self.stats = {
            'total_chunks': 0,
            'processed_chunks': 0,
            'successful_embeddings': 0,
            'failed_embeddings': 0,
            'updated_database': 0,
            'processing_time': 0,
            'errors': []
        }
        
        # å­˜å„²è™•ç†æ•¸æ“š
        self.chunks_data: List[Dict] = []
        self.embedding_results: List[Dict] = []
    
    def parse_chunks_file(self, chunks_file: str) -> List[Dict]:
        """è§£æåˆ†å¡Šæ–‡ä»¶ï¼Œæå–åˆ†å¡Šè³‡è¨Š"""
        print(f"ğŸ“– æ­£åœ¨è§£æåˆ†å¡Šæ–‡ä»¶: {chunks_file}")
        
        if not os.path.exists(chunks_file):
            raise FileNotFoundError(f"åˆ†å¡Šæ–‡ä»¶ä¸å­˜åœ¨: {chunks_file}")
        
        chunks = []
        current_chunk = None
        content_lines = []
        
        try:
            with open(chunks_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.rstrip()
                    
                    # è§£ææ–‡ç« è³‡è¨Š
                    if line.startswith('# Article:'):
                        current_article_title = line.replace('# Article:', '').strip()
                    elif line.startswith('# URL:'):
                        current_article_url = line.replace('# URL:', '').strip()
                    elif line.startswith('# Article ID:'):
                        current_article_id = line.replace('# Article ID:', '').strip()
                    
                    # è§£æåˆ†å¡Šè³‡è¨Š
                    elif line.startswith('## Chunk'):
                        # ä¿å­˜å‰ä¸€å€‹åˆ†å¡Š
                        if current_chunk and content_lines:
                            current_chunk['content'] = '\n'.join(content_lines).strip()
                            if current_chunk['content']:
                                chunks.append(current_chunk)
                        
                        # é–‹å§‹æ–°åˆ†å¡Š
                        chunk_match = re.match(r'## Chunk (\d+)', line)
                        if chunk_match:
                            current_chunk = {
                                'chunk_index': int(chunk_match.group(1)),
                                'article_title': current_article_title,
                                'article_url': current_article_url,
                                'article_id': current_article_id,
                                'line_number': line_num
                            }
                            content_lines = []
                    
                    elif line.startswith('# Chunk ID:'):
                        if current_chunk:
                            current_chunk['chunk_id'] = line.replace('# Chunk ID:', '').strip()
                    
                    elif line.startswith('# Length:'):
                        if current_chunk:
                            length_match = re.search(r'(\d+)', line)
                            if length_match:
                                current_chunk['content_length'] = int(length_match.group(1))
                    
                    elif line == '---':
                        # åˆ†å¡ŠçµæŸæ¨™è¨˜ï¼Œä¿å­˜ç•¶å‰åˆ†å¡Š
                        if current_chunk and content_lines:
                            current_chunk['content'] = '\n'.join(content_lines).strip()
                            if current_chunk['content']:
                                chunks.append(current_chunk)
                            current_chunk = None
                            content_lines = []
                    
                    elif line and not line.startswith('#'):
                        # åˆ†å¡Šå…§å®¹
                        if current_chunk is not None:
                            content_lines.append(line)
                
                # è™•ç†æœ€å¾Œä¸€å€‹åˆ†å¡Š
                if current_chunk and content_lines:
                    current_chunk['content'] = '\n'.join(content_lines).strip()
                    if current_chunk['content']:
                        chunks.append(current_chunk)
            
            # é©—è­‰å’Œæ¸…ç†æ•¸æ“š
            valid_chunks = []
            for chunk in chunks:
                if self._validate_chunk_data(chunk):
                    valid_chunks.append(chunk)
                else:
                    print(f"âš ï¸ è·³éç„¡æ•ˆåˆ†å¡Š: è¡Œ {chunk.get('line_number', '?')}")
            
            self.stats['total_chunks'] = len(valid_chunks)
            print(f"âœ… è§£æå®Œæˆï¼Œæ‰¾åˆ° {len(valid_chunks)} å€‹æœ‰æ•ˆåˆ†å¡Š")
            return valid_chunks
            
        except Exception as e:
            raise Exception(f"è§£æåˆ†å¡Šæ–‡ä»¶å¤±æ•—: {e}")
    
    def _validate_chunk_data(self, chunk: Dict) -> bool:
        """é©—è­‰åˆ†å¡Šæ•¸æ“šå®Œæ•´æ€§"""
        required_fields = ['chunk_id', 'article_id', 'content']
        
        for field in required_fields:
            if not chunk.get(field):
                return False
        
        # å…§å®¹é•·åº¦æª¢æŸ¥
        if len(chunk['content'].strip()) < 10:
            return False
        
        return True
    
    async def process_embeddings_batch(self, chunks: List[Dict]) -> None:
        """æ‰¹é‡è™•ç†å‘é‡åµŒå…¥"""
        print(f"\nğŸš€ é–‹å§‹æ‰¹é‡ç”Ÿæˆå‘é‡åµŒå…¥ (æ‰¹æ¬¡å¤§å°: {self.batch_size})")
        
        start_time = datetime.now()
        
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(chunks) + self.batch_size - 1) // self.batch_size
            
            print(f"\nğŸ§  è™•ç†åµŒå…¥æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} å€‹åˆ†å¡Š)")
            
            try:
                # æå–æ‰¹æ¬¡æ–‡æœ¬
                batch_texts = [chunk['content'] for chunk in batch]
                
                # ç”ŸæˆåµŒå…¥å‘é‡
                print("   âš¡ æ­£åœ¨ç”Ÿæˆå‘é‡åµŒå…¥...")
                embeddings = await asyncio.to_thread(
                    self.embedding_manager.get_embeddings, batch_texts
                )
                
                if len(embeddings) != len(batch):
                    raise Exception(f"åµŒå…¥æ•¸é‡ä¸åŒ¹é…: æœŸæœ› {len(batch)}, å¾—åˆ° {len(embeddings)}")
                
                # æº–å‚™çµæœæ•¸æ“š
                for j, (chunk, embedding) in enumerate(zip(batch, embeddings)):
                    if embedding is not None and len(embedding) > 0:
                        result = {
                            'chunk_id': chunk['chunk_id'],
                            'article_id': chunk['article_id'],
                            'content': chunk['content'],
                            'embedding': embedding.tolist() if isinstance(embedding, np.ndarray) else embedding,
                            'embedding_dim': len(embedding),
                            'content_length': len(chunk['content']),
                            'processed_at': datetime.now().isoformat()
                        }
                        self.embedding_results.append(result)
                        self.stats['successful_embeddings'] += 1
                    else:
                        error_msg = f"åµŒå…¥ç”Ÿæˆå¤±æ•—: chunk_id={chunk['chunk_id']}"
                        print(f"   âŒ {error_msg}")
                        self.stats['failed_embeddings'] += 1
                        self.stats['errors'].append({
                            'chunk_id': chunk['chunk_id'],
                            'error': error_msg
                        })
                
                self.stats['processed_chunks'] += len(batch)
                
                # é¡¯ç¤ºé€²åº¦
                progress = (self.stats['processed_chunks'] / self.stats['total_chunks']) * 100
                print(f"   âœ… æ‰¹æ¬¡å®Œæˆï¼Œé€²åº¦: {progress:.1f}%")
                
            except Exception as e:
                error_msg = f"æ‰¹æ¬¡ {batch_num} è™•ç†å¤±æ•—: {str(e)}"
                print(f"   âŒ {error_msg}")
                
                # è¨˜éŒ„æ‰¹æ¬¡ä¸­æ¯å€‹åˆ†å¡Šçš„éŒ¯èª¤
                for chunk in batch:
                    self.stats['failed_embeddings'] += 1
                    self.stats['errors'].append({
                        'chunk_id': chunk['chunk_id'],
                        'error': error_msg
                    })
                
                self.stats['processed_chunks'] += len(batch)
        
        end_time = datetime.now()
        self.stats['processing_time'] = (end_time - start_time).total_seconds()
        
        print(f"\nâœ… åµŒå…¥ç”Ÿæˆå®Œæˆï¼")
        print(f"   â±ï¸ è€—æ™‚: {self.stats['processing_time']:.2f} ç§’")
        print(f"   ğŸ¯ æˆåŠŸ: {self.stats['successful_embeddings']} å€‹")
        print(f"   âŒ å¤±æ•—: {self.stats['failed_embeddings']} å€‹")
    
    async def update_database(self) -> None:
        """æ›´æ–°è³‡æ–™åº«ä¸­çš„å‘é‡æ•¸æ“š"""
        if not self.embedding_results:
            print("âš ï¸ æ²’æœ‰åµŒå…¥çµæœéœ€è¦æ›´æ–°åˆ°è³‡æ–™åº«")
            return
        
        print(f"\nğŸ’¾ æ­£åœ¨æ›´æ–°è³‡æ–™åº«å‘é‡æ•¸æ“š...")
        print(f"ğŸ“Š æº–å‚™æ›´æ–° {len(self.embedding_results)} å€‹åˆ†å¡Šçš„å‘é‡")
        
        try:
            supabase = self.db_client.get_client()
            batch_size = 50  # è³‡æ–™åº«æ›´æ–°æ‰¹æ¬¡å¤§å°
            updated_count = 0
            
            for i in range(0, len(self.embedding_results), batch_size):
                batch = self.embedding_results[i:i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(self.embedding_results) + batch_size - 1) // batch_size
                
                print(f"   ğŸ“¦ æ›´æ–°æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} å€‹åˆ†å¡Š)")
                
                # æº–å‚™æ›´æ–°æ•¸æ“š
                updates = []
                for result in batch:
                    update_data = {
                        'id': result['chunk_id'],
                        'embedding': result['embedding'],
                        'metadata': {
                            'embedding_model': self.embedding_manager.model_name,
                            'embedding_dim': result['embedding_dim'],
                            'embedded_at': result['processed_at'],
                            'content_length': result['content_length']
                        }
                    }
                    updates.append(update_data)
                
                # æ‰¹é‡æ›´æ–°
                try:
                    result = supabase.from_('article_chunks')\
                        .upsert(updates, on_conflict='id')\
                        .execute()
                    
                    if result.data:
                        batch_updated = len(result.data)
                        updated_count += batch_updated
                        print(f"   âœ… æ›´æ–°æˆåŠŸ: {batch_updated} å€‹åˆ†å¡Š")
                    else:
                        print(f"   âš ï¸ æ‰¹æ¬¡æ›´æ–°ç„¡æ•¸æ“šè¿”å›")
                
                except Exception as e:
                    print(f"   âŒ æ‰¹æ¬¡ {batch_num} æ›´æ–°å¤±æ•—: {e}")
                    continue
            
            self.stats['updated_database'] = updated_count
            print(f"\nâœ… è³‡æ–™åº«æ›´æ–°å®Œæˆï¼")
            print(f"   ğŸ“Š æˆåŠŸæ›´æ–°: {updated_count} å€‹åˆ†å¡Šå‘é‡")
            
            # è¨˜éŒ„è™•ç†æ—¥èªŒ
            await self._log_processing_summary()
            
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«æ›´æ–°å¤±æ•—: {e}")
    
    async def _log_processing_summary(self) -> None:
        """è¨˜éŒ„è™•ç†æ‘˜è¦åˆ°æœç´¢æ—¥èªŒ"""
        try:
            supabase = self.db_client.get_client()
            
            log_entry = SearchLogModel(
                query=f"embedding_processing_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                results_count=self.stats['successful_embeddings'],
                response_time_ms=int(self.stats['processing_time'] * 1000),
                search_type='embedding_processing',
                metadata={
                    'total_chunks': self.stats['total_chunks'],
                    'successful_embeddings': self.stats['successful_embeddings'],
                    'failed_embeddings': self.stats['failed_embeddings'],
                    'updated_database': self.stats['updated_database'],
                    'processing_time_seconds': self.stats['processing_time'],
                    'batch_size': self.batch_size,
                    'embedding_model': self.embedding_manager.model_name,
                    'error_count': len(self.stats['errors'])
                }
            )
            
            result = supabase.from_('search_logs').insert(log_entry.to_dict()).execute()
            
            if result.data:
                print(f"ğŸ“ è™•ç†æ—¥èªŒå·²è¨˜éŒ„")
        
        except Exception as e:
            print(f"âš ï¸ è¨˜éŒ„è™•ç†æ—¥èªŒå¤±æ•—: {e}")
    
    async def validate_embeddings(self) -> None:
        """é©—è­‰å‘é‡åµŒå…¥å®Œæ•´æ€§"""
        if not self.embedding_results:
            return
        
        print(f"\nğŸ” æ­£åœ¨é©—è­‰å‘é‡åµŒå…¥å®Œæ•´æ€§...")
        
        try:
            supabase = self.db_client.get_client()
            
            # éš¨æ©Ÿé¸æ“‡å¹¾å€‹åˆ†å¡Šé€²è¡Œé©—è­‰
            import random
            sample_size = min(5, len(self.embedding_results))
            samples = random.sample(self.embedding_results, sample_size)
            
            print(f"   ğŸ¯ éš¨æ©Ÿæª¢æŸ¥ {sample_size} å€‹åˆ†å¡Š")
            
            valid_count = 0
            for sample in samples:
                chunk_id = sample['chunk_id']
                
                # å¾è³‡æ–™åº«æŸ¥è©¢å‘é‡
                result = supabase.from_('article_chunks')\
                    .select('id, embedding')\
                    .eq('id', chunk_id)\
                    .limit(1)\
                    .execute()
                
                if result.data and result.data[0].get('embedding'):
                    db_embedding = result.data[0]['embedding']
                    
                    # é©—è­‰ç¶­åº¦
                    if len(db_embedding) == len(sample['embedding']):
                        valid_count += 1
                        print(f"   âœ… {chunk_id}: å‘é‡å®Œæ•´ ({len(db_embedding)} ç¶­)")
                    else:
                        print(f"   âŒ {chunk_id}: å‘é‡ç¶­åº¦ä¸åŒ¹é…")
                else:
                    print(f"   âŒ {chunk_id}: æœªæ‰¾åˆ°å‘é‡æ•¸æ“š")
            
            if valid_count == sample_size:
                print(f"   ğŸ‰ é©—è­‰é€šéï¼æ‰€æœ‰æ¨£æœ¬å‘é‡æ­£å¸¸")
            else:
                print(f"   âš ï¸ é©—è­‰çµæœ: {valid_count}/{sample_size} å€‹æ¨£æœ¬æ­£å¸¸")
        
        except Exception as e:
            print(f"   âŒ é©—è­‰éç¨‹å‡ºéŒ¯: {e}")
    
    def print_summary(self) -> None:
        """æ‰“å°è™•ç†æ‘˜è¦"""
        print(f"\nğŸ“‹ å‘é‡åµŒå…¥è™•ç†æ‘˜è¦:")
        print(f"=" * 60)
        print(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
        print(f"   â€¢ ç¸½åˆ†å¡Šæ•¸: {self.stats['total_chunks']}")
        print(f"   â€¢ å·²è™•ç†åˆ†å¡Š: {self.stats['processed_chunks']}")
        print(f"   â€¢ æˆåŠŸåµŒå…¥: {self.stats['successful_embeddings']}")
        print(f"   â€¢ å¤±æ•—åµŒå…¥: {self.stats['failed_embeddings']}")
        print(f"   â€¢ è³‡æ–™åº«æ›´æ–°: {self.stats['updated_database']}")
        
        if self.stats['processed_chunks'] > 0:
            success_rate = (self.stats['successful_embeddings'] / self.stats['processed_chunks']) * 100
            print(f"   â€¢ æˆåŠŸç‡: {success_rate:.1f}%")
        
        print(f"\nâ±ï¸ æ€§èƒ½çµ±è¨ˆ:")
        print(f"   â€¢ ç¸½è™•ç†æ™‚é–“: {self.stats['processing_time']:.2f} ç§’")
        
        if self.stats['successful_embeddings'] > 0:
            avg_time = self.stats['processing_time'] / self.stats['successful_embeddings']
            print(f"   â€¢ å¹³å‡æ¯å€‹åµŒå…¥: {avg_time:.3f} ç§’")
            
            throughput = self.stats['successful_embeddings'] / self.stats['processing_time']
            print(f"   â€¢ è™•ç†é€Ÿåº¦: {throughput:.1f} å€‹/ç§’")
        
        print(f"\nğŸ§  æ¨¡å‹è³‡è¨Š:")
        print(f"   â€¢ åµŒå…¥æ¨¡å‹: {self.embedding_manager.model_name}")
        print(f"   â€¢ å‘é‡ç¶­åº¦: {self.embedding_manager.embedding_dim}")
        print(f"   â€¢ è¨ˆç®—è¨­å‚™: {self.embedding_manager.device}")
        print(f"   â€¢ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        
        # é¡¯ç¤ºéŒ¯èª¤æ‘˜è¦
        if self.stats['errors']:
            print(f"\nâŒ éŒ¯èª¤æ‘˜è¦ (å‰ 3 å€‹):")
            for i, error in enumerate(self.stats['errors'][:3], 1):
                print(f"   {i}. {error['chunk_id']}: {error['error']}")
            
            if len(self.stats['errors']) > 3:
                print(f"   ... é‚„æœ‰ {len(self.stats['errors']) - 3} å€‹éŒ¯èª¤")
        
        print(f"=" * 60)
        
        if self.stats['updated_database'] > 0:
            print(f"ğŸ¯ å‘é‡åµŒå…¥æµç¨‹å®Œæˆï¼è³‡æ–™åº«å·²æº–å‚™å¥½é€²è¡Œèªç¾©æœç´¢")
            print(f"ğŸ” æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼æ¸¬è©¦æœç´¢åŠŸèƒ½:")
            print(f"   python -c \"from database.operations import DatabaseOperations; db = DatabaseOperations(); print(db.semantic_search('æ¸¬è©¦æŸ¥è©¢', limit=3))\"")


async def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='å‘é‡åµŒå…¥å’Œè³‡æ–™åº«å­˜å„²å·¥å…·')
    parser.add_argument('--chunk-list', required=True, help='åˆ†å¡Šæ¸…å–®æ–‡ä»¶')
    parser.add_argument('--batch-size', type=int, default=16, help='åµŒå…¥æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--device', default='auto', choices=['auto', 'cuda', 'cpu'], help='è¨ˆç®—è¨­å‚™')
    parser.add_argument('--no-db-update', action='store_true', help='ä¸æ›´æ–°è³‡æ–™åº«')
    parser.add_argument('--validate', action='store_true', help='é©—è­‰åµŒå…¥å®Œæ•´æ€§')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ é–‹å§‹å‘é‡åµŒå…¥è™•ç†æµç¨‹")
    print(f"ğŸ“– åˆ†å¡Šæ¸…å–®: {args.chunk_list}")
    print(f"ğŸ§  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"ğŸ’» è¨ˆç®—è¨­å‚™: {args.device}")
    
    processor = EmbeddingProcessor(
        batch_size=args.batch_size,
        device=args.device
    )
    
    try:
        # 1. è§£æåˆ†å¡Šæ–‡ä»¶
        chunks = processor.parse_chunks_file(args.chunk_list)
        
        if not chunks:
            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆåˆ†å¡Šï¼Œé€€å‡ºè™•ç†")
            return
        
        # 2. æ‰¹é‡ç”ŸæˆåµŒå…¥
        await processor.process_embeddings_batch(chunks)
        
        # 3. æ›´æ–°è³‡æ–™åº«ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if not args.no_db_update and processor.embedding_results:
            await processor.update_database()
        
        # 4. é©—è­‰åµŒå…¥ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if args.validate and processor.embedding_results:
            await processor.validate_embeddings()
        
        # 5. æ‰“å°æ‘˜è¦
        processor.print_summary()
        
        if processor.stats['updated_database'] > 0:
            print(f"\nğŸ‰ å‘é‡åµŒå…¥æµç¨‹å®Œæˆï¼")
            print(f"ğŸ” RAG ç³»çµ±å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥é€²è¡Œèªç¾©æœç´¢")
        else:
            print(f"\nâš ï¸ å‘é‡åµŒå…¥å®Œæˆï¼Œä½†æœªæ›´æ–°åˆ°è³‡æ–™åº«")
        
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
