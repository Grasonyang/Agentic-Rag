#!/usr/bin/env python3
"""
getChunking.py - ç¶²é å…§å®¹çˆ¬å–å’Œåˆ†å¡Šè…³æœ¬

åŠŸèƒ½ï¼š
1. è®€å– URL æ¸…å–®æ–‡ä»¶
2. ä¾åºçˆ¬å–æ¯å€‹ç¶²é å…§å®¹
3. å°å…§å®¹é€²è¡Œæ™ºèƒ½åˆ†å¡Šè™•ç†
4. ä¿å­˜æ–‡ç« å’Œåˆ†å¡Šè³‡è¨Š
5. ç”Ÿæˆå¾…åµŒå…¥çš„åˆ†å¡Šæ¸…å–®

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/getChunking.py --url-list urls.txt
    python scripts/getChunking.py --url-list urls.txt --chunker sliding_window --chunk-size 200
    make get-chunking URL_LIST=urls.txt
"""

import argparse
import asyncio
import sys
import os
import logging
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import hashlib
import re

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))

# è¨­ç½®æ—¥å¿—
def setup_logging():
    """è¨­ç½®æ—¥å¿—é…ç½®"""
    log_dir = os.path.join(os.path.dirname(__file__), 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(log_dir, f'chunking_{timestamp}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)

from spider.crawlers.simple_crawler import SimpleWebCrawler
from spider.chunking.chunker_factory import ChunkerFactory
from database.client import SupabaseClient
from database.models_simplified import ArticleModel, ChunkModel


class ContentProcessor:
    """å…§å®¹çˆ¬å–å’Œåˆ†å¡Šè™•ç†é¡"""
    
    def __init__(self, output_file: str = "chunks.txt", chunker_type: str = "sliding_window", 
                 chunker_params: Dict = None, max_workers: int = 3):
        self.crawler = SimpleWebCrawler()
        self.db_client = SupabaseClient()
        self.output_file = output_file
        self.chunker_type = chunker_type
        self.chunker_params = chunker_params or {}
        self.max_workers = max_workers
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–åˆ†å¡Šå™¨
        try:
            self.chunker = ChunkerFactory.create_chunker(chunker_type, self.chunker_params)
            self.logger.info(f"åˆ†å¡Šå™¨ {chunker_type} åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"åˆ†å¡Šå™¨åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨é è¨­ sliding_window: {e}")
            print(f"âš ï¸ åˆ†å¡Šå™¨åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨é è¨­ sliding_window: {e}")
            self.chunker = ChunkerFactory.create_chunker("sliding_window", {})
        
        # è™•ç†çµ±è¨ˆ
        self.stats = {
            'total_urls': 0,
            'processed_urls': 0,
            'successful_crawls': 0,
            'failed_crawls': 0,
            'total_articles': 0,
            'total_chunks': 0,
            'processing_time': 0,
            'errors': []
        }
        
        # å­˜å„²è™•ç†æ•¸æ“š
        self.articles_data = []
        self.chunks_data = []
    
    def parse_urls_file(self, urls_file: str) -> List[Dict]:
        """è§£æ URL æ¸…å–®æ–‡ä»¶"""
        print(f"ğŸ“– æ­£åœ¨è§£æ URL æ¸…å–®: {urls_file}")
        
        if not os.path.exists(urls_file):
            raise FileNotFoundError(f"URL æ¸…å–®æ–‡ä»¶ä¸å­˜åœ¨: {urls_file}")
        
        urls = []
        current_sitemap = None
        
        try:
            with open(urls_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    
                    if not line or line.startswith('#'):
                        continue
                    
                    # è§£æ sitemap åˆ†çµ„
                    if line.startswith('## Sitemap:'):
                        current_sitemap = line.replace('## Sitemap:', '').strip()
                        continue
                    
                    # è§£æ URL è¡Œ - æ”¯æŒç°¡åŒ–æ ¼å¼
                    url_info = {}
                    
                    if line.startswith('- '):
                        # å®Œæ•´æ ¼å¼ï¼š- url metadata
                        url_match = re.search(r'- (https?://[^\s]+)', line)
                        if url_match:
                            url_info['url'] = url_match.group(1)
                        else:
                            continue
                    elif line.startswith('https://') or line.startswith('http://'):
                        # ç°¡åŒ–æ ¼å¼ï¼šç›´æ¥æ˜¯ URL
                        url_parts = line.split()
                        if url_parts:
                            url_info['url'] = url_parts[0]
                            # è§£æè¨»é‡‹ä¸­çš„ metadata
                            if '#' in line:
                                comment_part = line.split('#', 1)[1].strip()
                                # è§£æ priority, freq, modified
                                priority_match = re.search(r'priority=([\d.]+)', comment_part)
                                if priority_match:
                                    url_info['priority'] = float(priority_match.group(1))
                                
                                freq_match = re.search(r'freq=(\w+)', comment_part)
                                if freq_match:
                                    url_info['changefreq'] = freq_match.group(1)
                                
                                modified_match = re.search(r'modified=([\d-]+)', comment_part)
                                if modified_match:
                                    url_info['lastmod'] = modified_match.group(1)
                        else:
                            continue
                    else:
                        continue
                    
                    # è¨­ç½® sitemap ä¾†æº
                    url_info['sitemap_source'] = current_sitemap
                    url_info['line_number'] = line_num
                    
                    urls.append(url_info)
            
            self.stats['total_urls'] = len(urls)
            print(f"âœ… è§£æå®Œæˆï¼Œæ‰¾åˆ° {len(urls)} å€‹ URL")
            return urls
            
        except Exception as e:
            raise Exception(f"è§£æ URL æ¸…å–®å¤±æ•—: {e}")
    
    async def process_urls_batch(self, urls: List[Dict], batch_size: int = 5) -> None:
        """æ‰¹é‡è™•ç† URL çˆ¬å–å’Œåˆ†å¡Š"""
        print(f"\nğŸš€ é–‹å§‹æ‰¹é‡è™•ç† URL (æ‰¹æ¬¡å¤§å°: {batch_size})")
        
        start_time = datetime.now()
        
        for i in range(0, len(urls), batch_size):
            batch = urls[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(urls) + batch_size - 1) // batch_size
            
            print(f"\nğŸ“¦ è™•ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} å€‹ URL)")
            
            # ä¸¦ç™¼çˆ¬å–æ‰¹æ¬¡ URL
            crawl_tasks = []
            for url_info in batch:
                task = self._crawl_and_process_single_url(url_info)
                crawl_tasks.append(task)
            
            # åŸ·è¡Œæ‰¹æ¬¡çˆ¬å–
            batch_results = await asyncio.gather(*crawl_tasks, return_exceptions=True)
            
            # è™•ç†æ‰¹æ¬¡çµæœ
            for url_info, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    error_msg = f"æ‰¹æ¬¡è™•ç†å¤±æ•—: {str(result)}"
                    print(f"   âŒ {url_info['url'][:50]}... - {error_msg}")
                    self.stats['failed_crawls'] += 1
                    self.stats['errors'].append({
                        'url': url_info['url'],
                        'error': error_msg
                    })
                else:
                    self.stats['successful_crawls'] += 1
            
            self.stats['processed_urls'] += len(batch)
            
            # é¡¯ç¤ºé€²åº¦
            progress = (self.stats['processed_urls'] / self.stats['total_urls']) * 100
            print(f"   âœ… æ‰¹æ¬¡å®Œæˆï¼Œç¸½é€²åº¦: {progress:.1f}%")
        
        end_time = datetime.now()
        self.stats['processing_time'] = (end_time - start_time).total_seconds()
        
        print(f"\nâœ… URL è™•ç†å®Œæˆï¼")
        print(f"   â±ï¸ è€—æ™‚: {self.stats['processing_time']:.2f} ç§’")
        print(f"   ğŸ¯ æˆåŠŸ: {self.stats['successful_crawls']} å€‹")
        print(f"   âŒ å¤±æ•—: {self.stats['failed_crawls']} å€‹")
    
    async def _crawl_and_process_single_url(self, url_info: Dict) -> Dict:
        """çˆ¬å–å’Œè™•ç†å–®å€‹ URL"""
        url = url_info['url']
        
        try:
            # çˆ¬å–ç¶²é å…§å®¹
            print(f"   ğŸ” çˆ¬å–: {url[:60]}...")
            crawl_result = await self.crawler.crawl_url(url)
            
            if not crawl_result or not crawl_result.success or not crawl_result.content:
                raise Exception("çˆ¬å–çµæœç‚ºç©ºæˆ–ç„¡å…§å®¹")
            
            # æº–å‚™æ–‡ç« æ•¸æ“š
            article_data = {
                'url': url,
                'title': crawl_result.title or '',
                'content': crawl_result.content,
                'metadata': {
                    'sitemap_source': url_info.get('sitemap_source'),
                    'priority': url_info.get('priority'),
                    'lastmod': url_info.get('lastmod'),
                    'changefreq': url_info.get('changefreq'),
                    'crawled_at': datetime.now().isoformat(),
                    'content_length': len(crawl_result.content),
                    'chunker_type': self.chunker_type,
                    'chunker_params': self.chunker_params
                }
            }
            
            # ç”Ÿæˆæ–‡ç«  ID
            article_id = self._generate_article_id(url)
            article_data['id'] = article_id
            
            # é€²è¡Œå…§å®¹åˆ†å¡Š
            print(f"   âœ‚ï¸ åˆ†å¡Š: {article_data['title'][:40]}...")
            chunks = await self._chunk_content(article_data)
            
            # ä¿å­˜åˆ°è³‡æ–™åº«
            await self._save_article_and_chunks(article_data, chunks)
            
            # è¨˜éŒ„è™•ç†çµæœ
            self.articles_data.append(article_data)
            self.chunks_data.extend(chunks)
            
            self.stats['total_articles'] += 1
            self.stats['total_chunks'] += len(chunks)
            
            print(f"   âœ… å®Œæˆ: {len(chunks)} å€‹åˆ†å¡Š")
            
            return {
                'url': url,
                'article_id': article_id,
                'chunks_count': len(chunks),
                'success': True
            }
            
        except Exception as e:
            error_msg = f"è™•ç† {url} å¤±æ•—: {str(e)}"
            print(f"   âŒ {error_msg}")
            raise Exception(error_msg)
    
    def _generate_article_id(self, url: str) -> str:
        """ç”Ÿæˆæ–‡ç« å”¯ä¸€ ID"""
        # ä½¿ç”¨ URL çš„ SHA-256 å“ˆå¸Œç”Ÿæˆ UUID æ ¼å¼çš„ ID
        import uuid
        hash_value = hashlib.sha256(url.encode('utf-8')).hexdigest()
        # å°†å“ˆå¸Œå€¼è½¬æ¢ä¸º UUID æ ¼å¼
        uuid_str = f"{hash_value[:8]}-{hash_value[8:12]}-{hash_value[12:16]}-{hash_value[16:20]}-{hash_value[20:32]}"
        return uuid_str
    
    async def _chunk_content(self, article_data: Dict) -> List[Dict]:
        """å°æ–‡ç« å…§å®¹é€²è¡Œåˆ†å¡Š"""
        content = article_data['content']
        
        try:
            # ä½¿ç”¨åˆ†å¡Šå™¨è™•ç†å…§å®¹
            chunks = await asyncio.to_thread(self.chunker.chunk, content)
            
            # æº–å‚™åˆ†å¡Šæ•¸æ“š
            chunk_data_list = []
            for i, chunk_obj in enumerate(chunks):
                # è™•ç† Chunk å°è±¡ï¼Œç²å–å…¶ content å±¬æ€§
                if hasattr(chunk_obj, 'content'):
                    chunk_text = chunk_obj.content
                else:
                    chunk_text = str(chunk_obj)
                
                if not chunk_text.strip():
                    continue
                
                chunk_id = f"{article_data['id']}_{i:04d}"
                
                # å°†åˆ†å— ID è½¬æ¢ä¸º UUID æ ¼å¼
                chunk_hash = hashlib.sha256(f"{article_data['url']}_{i}".encode('utf-8')).hexdigest()
                chunk_uuid = f"{chunk_hash[:8]}-{chunk_hash[8:12]}-{chunk_hash[12:16]}-{chunk_hash[16:20]}-{chunk_hash[20:32]}"
                
                chunk_data = {
                    'id': chunk_uuid,
                    'article_id': article_data['id'],
                    'chunk_index': i,
                    'content': chunk_text.strip(),
                    'metadata': {
                        'chunk_length': len(chunk_text.strip()),
                        'chunker_type': self.chunker_type,
                        'chunker_params': self.chunker_params,
                        'created_at': datetime.now().isoformat()
                    }
                }
                
                chunk_data_list.append(chunk_data)
            
            return chunk_data_list
            
        except Exception as e:
            raise Exception(f"å…§å®¹åˆ†å¡Šå¤±æ•—: {e}")
    
    async def _save_article_and_chunks(self, article_data: Dict, chunks: List[Dict]) -> None:
        """ä¿å­˜æ–‡ç« å’Œåˆ†å¡Šåˆ°è³‡æ–™åº«"""
        try:
            supabase = self.db_client.get_client()
            
            # æº–å‚™æ–‡ç« æ¨¡å‹
            article_model = ArticleModel(
                url=article_data['url'],
                title=article_data['title'],
                content=article_data['content'],
                metadata=article_data['metadata']
            )
            
            # è¨­ç½®è‡ªå®šç¾© ID
            article_model.id = article_data['id']
            
            # ä¿å­˜æ–‡ç« 
            result = supabase.from_('articles').upsert(
                article_model.to_dict(), on_conflict='id'
            ).execute()
            
            if not result.data:
                raise Exception("æ–‡ç« ä¿å­˜å¤±æ•—")
            
            # æº–å‚™åˆ†å¡Šæ¨¡å‹
            chunk_models = []
            for chunk_data in chunks:
                chunk_model = ChunkModel(
                    article_id=chunk_data['article_id'],
                    chunk_index=chunk_data['chunk_index'],
                    content=chunk_data['content'],
                    metadata=chunk_data['metadata']
                )
                # è¨­ç½®è‡ªå®šç¾© ID
                chunk_model.id = chunk_data['id']
                chunk_models.append(chunk_model.to_dict())
            
            # æ‰¹é‡ä¿å­˜åˆ†å¡Š
            if chunk_models:
                result = supabase.from_('article_chunks').upsert(
                    chunk_models, on_conflict='id'
                ).execute()
                
                if not result.data:
                    raise Exception("åˆ†å¡Šä¿å­˜å¤±æ•—")
            
        except Exception as e:
            raise Exception(f"è³‡æ–™åº«ä¿å­˜å¤±æ•—: {e}")
    
    async def generate_chunks_list(self) -> None:
        """ç”Ÿæˆåˆ†å¡Šæ¸…å–®æ–‡ä»¶"""
        if not self.chunks_data:
            print("âš ï¸ æ²’æœ‰åˆ†å¡Šæ•¸æ“šéœ€è¦è¼¸å‡º")
            return
        
        print(f"\nğŸ“ æ­£åœ¨ç”Ÿæˆåˆ†å¡Šæ¸…å–®æ–‡ä»¶: {self.output_file}")
        
        try:
            # æŒ‰æ–‡ç« åˆ†çµ„åˆ†å¡Š
            articles_chunks = {}
            for chunk in self.chunks_data:
                article_id = chunk['article_id']
                if article_id not in articles_chunks:
                    articles_chunks[article_id] = []
                articles_chunks[article_id].append(chunk)
            
            # å¯«å…¥åˆ†å¡Šæ¸…å–®æ–‡ä»¶
            with open(self.output_file, 'w', encoding='utf-8') as f:
                f.write(f"# åˆ†å¡Šæ¸…å–®æ–‡ä»¶\n")
                f.write(f"# ç”Ÿæˆæ™‚é–“: {datetime.now().isoformat()}\n")
                f.write(f"# åˆ†å¡Šå™¨é¡å‹: {self.chunker_type}\n")
                f.write(f"# ç¸½æ–‡ç« æ•¸: {len(articles_chunks)}\n")
                f.write(f"# ç¸½åˆ†å¡Šæ•¸: {len(self.chunks_data)}\n")
                f.write(f"\n")
                
                # æ‰¾åˆ°å°æ‡‰çš„æ–‡ç« æ•¸æ“š
                articles_lookup = {article['id']: article for article in self.articles_data}
                
                for article_id, chunks in articles_chunks.items():
                    article = articles_lookup.get(article_id, {})
                    
                    f.write(f"# Article: {article.get('title', 'Unknown Title')}\n")
                    f.write(f"# URL: {article.get('url', 'Unknown URL')}\n")
                    f.write(f"# Article ID: {article_id}\n")
                    f.write(f"\n")
                    
                    # æŒ‰åˆ†å¡Šç´¢å¼•æ’åº
                    chunks.sort(key=lambda x: x['chunk_index'])
                    
                    for chunk in chunks:
                        f.write(f"## Chunk {chunk['chunk_index'] + 1}\n")
                        f.write(f"# Chunk ID: {chunk['id']}\n")
                        f.write(f"# Length: {len(chunk['content'])} å­—ç¬¦\n")
                        f.write(f"\n")
                        f.write(f"{chunk['content']}\n")
                        f.write(f"\n---\n\n")
            
            print(f"âœ… åˆ†å¡Šæ¸…å–®å·²ä¿å­˜: {self.output_file}")
            print(f"   ğŸ“Š åŒ…å« {len(articles_chunks)} ç¯‡æ–‡ç« ï¼Œ{len(self.chunks_data)} å€‹åˆ†å¡Š")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆåˆ†å¡Šæ¸…å–®å¤±æ•—: {e}")
    
    def print_summary(self) -> None:
        """æ‰“å°è™•ç†æ‘˜è¦"""
        print(f"\nğŸ“‹ å…§å®¹è™•ç†æ‘˜è¦:")
        print(f"=" * 60)
        print(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
        print(f"   â€¢ ç¸½ URL æ•¸: {self.stats['total_urls']}")
        print(f"   â€¢ å·²è™•ç† URL: {self.stats['processed_urls']}")
        print(f"   â€¢ æˆåŠŸçˆ¬å–: {self.stats['successful_crawls']}")
        print(f"   â€¢ å¤±æ•—çˆ¬å–: {self.stats['failed_crawls']}")
        print(f"   â€¢ ç¸½æ–‡ç« æ•¸: {self.stats['total_articles']}")
        print(f"   â€¢ ç¸½åˆ†å¡Šæ•¸: {self.stats['total_chunks']}")
        
        if self.stats['processed_urls'] > 0:
            success_rate = (self.stats['successful_crawls'] / self.stats['processed_urls']) * 100
            print(f"   â€¢ æˆåŠŸç‡: {success_rate:.1f}%")
        
        if self.stats['total_articles'] > 0:
            avg_chunks = self.stats['total_chunks'] / self.stats['total_articles']
            print(f"   â€¢ å¹³å‡åˆ†å¡Šæ•¸: {avg_chunks:.1f} å€‹/ç¯‡")
        
        print(f"\nâ±ï¸ æ€§èƒ½çµ±è¨ˆ:")
        print(f"   â€¢ ç¸½è™•ç†æ™‚é–“: {self.stats['processing_time']:.2f} ç§’")
        
        if self.stats['successful_crawls'] > 0:
            avg_time_per_url = self.stats['processing_time'] / self.stats['successful_crawls']
            print(f"   â€¢ å¹³å‡æ¯å€‹ URL: {avg_time_per_url:.2f} ç§’")
            
            throughput = self.stats['successful_crawls'] / self.stats['processing_time']
            print(f"   â€¢ è™•ç†é€Ÿåº¦: {throughput:.2f} URL/ç§’")
        
        print(f"ğŸ”§ é…ç½®è³‡è¨Š:")
        print(f"   â€¢ åˆ†å¡Šå™¨é¡å‹: {self.chunker_type}")
        print(f"   â€¢ åˆ†å¡Šå™¨åƒæ•¸: {self.chunker_params}")
        print(f"   â€¢ è¼¸å‡ºæ–‡ä»¶: {self.output_file}")
        print(f"   â€¢ æœ€å¤§å·¥ä½œåŸ·è¡Œç·’: {self.max_workers}")
        
        # é¡¯ç¤ºéŒ¯èª¤æ‘˜è¦
        if self.stats['errors']:
            print(f"\nâŒ éŒ¯èª¤æ‘˜è¦ (å‰ 3 å€‹):")
            for i, error in enumerate(self.stats['errors'][:3], 1):
                print(f"   {i}. {error['url'][:50]}... - {error['error']}")
            
            if len(self.stats['errors']) > 3:
                print(f"   ... é‚„æœ‰ {len(self.stats['errors']) - 3} å€‹éŒ¯èª¤")
        
        print(f"=" * 60)
        
        if self.stats['total_chunks'] > 0:
            print(f"ğŸ¯ å…§å®¹çˆ¬å–å’Œåˆ†å¡Šå®Œæˆï¼")
            print(f"ğŸ“ åˆ†å¡Šæ¸…å–®æ–‡ä»¶: {self.output_file}")
            print(f"ğŸ”„ ä¸‹ä¸€æ­¥å¯åŸ·è¡Œ: python scripts/getEmbedding.py --chunk-list {self.output_file}")


async def main():
    """ä¸»å‡½æ•¸"""
    # åˆå§‹åŒ–æ—¥å¿—
    logger = setup_logging()
    logger.info("é–‹å§‹å…§å®¹çˆ¬å–å’Œåˆ†å¡Šæµç¨‹")
    
    parser = argparse.ArgumentParser(description='ç¶²é å…§å®¹çˆ¬å–å’Œåˆ†å¡Šå·¥å…·')
    parser.add_argument('--url-list', required=True, help='URL æ¸…å–®æ–‡ä»¶')
    parser.add_argument('--output', default='chunks.txt', help='åˆ†å¡Šæ¸…å–®è¼¸å‡ºæ–‡ä»¶')
    parser.add_argument('--chunker', default='sliding_window', 
                       choices=['sliding_window', 'sentence', 'semantic'],
                       help='åˆ†å¡Šå™¨é¡å‹')
    parser.add_argument('--chunk-size', type=int, default=300, help='åˆ†å¡Šå¤§å°')
    parser.add_argument('--overlap', type=int, default=50, help='é‡ç–Šå¤§å°')
    parser.add_argument('--batch-size', type=int, default=5, help='æ‰¹æ¬¡è™•ç†å¤§å°')
    parser.add_argument('--max-workers', type=int, default=3, help='æœ€å¤§ä¸¦ç™¼æ•¸')
    
    args = parser.parse_args()
    
    # è¨˜éŒ„é‹è¡Œåƒæ•¸
    logger.info(f"é‹è¡Œåƒæ•¸: {vars(args)}")
    
    print(f"ğŸš€ é–‹å§‹å…§å®¹çˆ¬å–å’Œåˆ†å¡Šæµç¨‹")
    print(f"ğŸ“– URL æ¸…å–®: {args.url_list}")
    print(f"ğŸ“ è¼¸å‡ºæ–‡ä»¶: {args.output}")
    print(f"âœ‚ï¸ åˆ†å¡Šå™¨: {args.chunker}")
    print(f"ğŸ“ åˆ†å¡Šå¤§å°: {args.chunk_size}")
    print(f"ğŸ”„ é‡ç–Šå¤§å°: {args.overlap}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"âš¡ ä¸¦ç™¼æ•¸: {args.max_workers}")
    
    # æº–å‚™åˆ†å¡Šå™¨åƒæ•¸
    chunker_params = {
        'window_size': args.chunk_size,
        'step_size': args.overlap
    }
    
    processor = ContentProcessor(
        output_file=args.output,
        chunker_type=args.chunker,
        chunker_params=chunker_params,
        max_workers=args.max_workers
    )
    
    try:
        # 1. è§£æ URL æ¸…å–®  
        logger.info("é–‹å§‹è§£æ URL æ¸…å–®")
        urls = processor.parse_urls_file(args.url_list)
        logger.info(f"è§£æåˆ° {len(urls)} å€‹ URL")
        
        if not urls:
            logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆ URL")
            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆ URLï¼Œé€€å‡ºè™•ç†")
            return
        
        # 2. æ‰¹é‡è™•ç† URL
        logger.info("é–‹å§‹æ‰¹é‡è™•ç† URL")
        await processor.process_urls_batch(urls, args.batch_size)
        
        # 3. ç”Ÿæˆåˆ†å¡Šæ¸…å–®
        logger.info("ç”Ÿæˆåˆ†å¡Šæ¸…å–®")
        await processor.generate_chunks_list()
        
        # 4. æ‰“å°æ‘˜è¦
        processor.print_summary()
        
        logger.info(f"è™•ç†å®Œæˆ: ç¸½æ–‡ç« æ•¸={processor.stats['total_articles']}, ç¸½åˆ†å¡Šæ•¸={processor.stats['total_chunks']}")
        
        if processor.stats['total_chunks'] > 0:
            print(f"\nğŸ‰ å…§å®¹çˆ¬å–å’Œåˆ†å¡Šå®Œæˆï¼")
            print(f"ğŸ“ åˆ†å¡Šæ¸…å–®å·²ç”Ÿæˆ: {args.output}")
            print(f"ğŸ”„ ä¸‹ä¸€æ­¥è«‹åŸ·è¡Œå‘é‡åµŒå…¥:")
            print(f"   python scripts/getEmbedding.py --chunk-list {args.output}")
        else:
            print(f"\nâš ï¸ æœªç”Ÿæˆä»»ä½•åˆ†å¡Šï¼Œè«‹æª¢æŸ¥ URL æ¸…å–®æˆ–ç¶²çµ¡é€£æ¥")
        
    except Exception as e:
        logger.error(f"åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
