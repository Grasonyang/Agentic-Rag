#!/usr/bin/env python3
"""
getSiteMap.py - Sitemap ç™¼ç¾å’Œè§£æè…³æœ¬

åŠŸèƒ½ï¼š
1. å¾æŒ‡å®š URL ç²å– robots.txt
2. è§£ææ‰€æœ‰ Sitemap é€£çµ
3. éæ­¸è§£æ Sitemap Index
4. è¼¸å‡ºæ‰€æœ‰ç™¼ç¾çš„ Sitemap åˆ°æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/getSiteMap.py --url https://example.com
    python scripts/getSiteMap.py --url         # 1. å¾ robots.txt ç™¼ç¾åˆå§‹ sitemap
        initial_sitemaps = await discovery.discover_sitemaps(args.url)tps://example.com --output sitemaps.txt
    make get-sitemap URL=https://example.com
"""

import argparse
import asyncio
import sys
import os
from pathlib import Path
from typing import List, Set
from urllib.parse import urljoin, urlparse

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))

from spider.crawlers.sitemap_parser import SitemapParser
from database.client import SupabaseClient
from database.models import SitemapModel, SitemapType, CrawlStatus


class SitemapDiscovery:
    """Sitemap ç™¼ç¾å’Œç®¡ç†é¡"""
    
    def __init__(self, output_file: str = "sitemaps.txt"):
        self.parser = SitemapParser()
        self.db_client = SupabaseClient()
        self.output_file = output_file
        self.discovered_sitemaps: Set[str] = set()
        self.sitemap_hierarchy: List[dict] = []
    
    async def discover_sitemaps(self, robots_url: str) -> List[str]:
        """å¾ robots.txt ç™¼ç¾ sitemap"""
        print(f"ğŸ¤– æ­£åœ¨è§£æ robots.txt: {robots_url}")
        
        try:
            sitemaps = await self._parse_robots_txt(robots_url)
            
            if not sitemaps:
                print("âš ï¸ æœªåœ¨ robots.txt ä¸­æ‰¾åˆ° Sitemap æ¢ç›®ï¼Œå˜—è©¦å¸¸è¦‹è·¯å¾‘")
                # å˜—è©¦å¸¸è¦‹çš„ sitemap è·¯å¾‘
                base_url = robots_url.replace('/robots.txt', '')
                common_paths = ['/sitemap.xml', '/sitemap_index.xml', '/sitemapindex.xml']
                
                for path in common_paths:
                    potential_url = base_url + path
                    print(f"   ï¿½ æª¢æŸ¥: {potential_url}")
                    # é€™è£¡å¯ä»¥æ·»åŠ æª¢æŸ¥é‚è¼¯
                
            return sitemaps
            
        except Exception as e:
            raise Exception(f"è§£æ robots.txt å¤±æ•—: {e}")
    
    async def _parse_robots_txt(self, robots_url: str) -> List[str]:
        """è§£æ robots.txt æ–‡ä»¶æå– sitemap URL"""
        import aiohttp
        
        sitemaps = []
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(robots_url) as response:
                    if response.status == 200:
                        text = await response.text()
                        
                        # è§£ææ¯ä¸€è¡ŒæŸ¥æ‰¾ Sitemap æ¢ç›®
                        for line in text.split('\n'):
                            line = line.strip()
                            if line.lower().startswith('sitemap:'):
                                sitemap_url = line[8:].strip()  # ç§»é™¤ 'Sitemap:' å‰ç¶´
                                if sitemap_url:
                                    sitemaps.append(sitemap_url)
                                    print(f"   âœ… ç™¼ç¾ Sitemap: {sitemap_url}")
                    else:
                        raise Exception(f"HTTP {response.status}: ç„¡æ³•è¨ªå• robots.txt")
                        
        except Exception as e:
            raise Exception(f"è®€å– robots.txt å¤±æ•—: {e}")
        
        return sitemaps
    
    async def analyze_sitemap_hierarchy(self, sitemap_urls: List[str]) -> None:
        """åˆ†æ Sitemap å±¤æ¬¡çµæ§‹"""
        print(f"\nğŸ” æ­£åœ¨åˆ†æ Sitemap å±¤æ¬¡çµæ§‹...")
        
        for sitemap_url in sitemap_urls:
            try:
                # æª¢æŸ¥æ˜¯å¦ç‚º Sitemap Index
                sitemap_info = await self.parser.get_sitemap_info(sitemap_url)
                
                sitemap_data = {
                    'url': sitemap_url,
                    'type': sitemap_info.get('type', 'unknown'),
                    'url_count': sitemap_info.get('url_count', 0),
                    'last_modified': sitemap_info.get('lastmod'),
                    'sub_sitemaps': []
                }
                
                # å¦‚æœæ˜¯ Sitemap Indexï¼Œç²å–å­ Sitemap
                if sitemap_info.get('type') == 'sitemapindex':
                    print(f"ğŸ“š {sitemap_url} æ˜¯ Sitemap Index")
                    sub_sitemaps = await self.parser.get_sub_sitemaps(sitemap_url)
                    sitemap_data['sub_sitemaps'] = sub_sitemaps
                    
                    for sub_sitemap in sub_sitemaps:
                        self.discovered_sitemaps.add(sub_sitemap)
                        print(f"   â””â”€â”€ ğŸ“„ {sub_sitemap}")
                else:
                    print(f"ğŸ“„ {sitemap_url} åŒ…å« {sitemap_data['url_count']} å€‹ URL")
                
                self.sitemap_hierarchy.append(sitemap_data)
                
            except Exception as e:
                print(f"âš ï¸ åˆ†æ {sitemap_url} æ™‚å‡ºéŒ¯: {e}")
                # ä»ç„¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œä½†æ¨™è¨˜ç‚ºéŒ¯èª¤
                self.sitemap_hierarchy.append({
                    'url': sitemap_url,
                    'type': 'error',
                    'error': str(e),
                    'url_count': 0,
                    'sub_sitemaps': []
                })
    
    async def save_to_database(self) -> None:
        """ä¿å­˜ Sitemap è³‡è¨Šåˆ°è³‡æ–™åº«"""
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ {len(self.sitemap_hierarchy)} å€‹ Sitemap åˆ°è³‡æ–™åº«...")
        
        try:
            supabase = self.db_client.get_client()
            
            for sitemap_data in self.sitemap_hierarchy:
                # å‰µå»º Sitemap æ¨¡å‹
                sitemap_model = SitemapModel(
                    url=sitemap_data['url'],
                    sitemap_type=SitemapType.SITEMAPINDEX if sitemap_data['type'] == 'sitemapindex' else SitemapType.SITEMAP,
                    status=CrawlStatus.COMPLETED if sitemap_data['type'] != 'error' else CrawlStatus.ERROR,
                    urls_count=sitemap_data['url_count'],
                    metadata={
                        'discovered_from': 'robots.txt',
                        'hierarchy_level': 0,
                        'sub_sitemaps_count': len(sitemap_data.get('sub_sitemaps', [])),
                        'analysis_timestamp': asyncio.get_event_loop().time()
                    }
                )
                
                if sitemap_data['type'] == 'error':
                    sitemap_model.error_message = sitemap_data.get('error', 'Unknown error')
                
                # ä¿å­˜åˆ°è³‡æ–™åº«
                result = supabase.from_('sitemaps').upsert(
                    sitemap_model.to_dict(),
                    on_conflict='url'
                ).execute()
                
                if result.data:
                    print(f"âœ… å·²ä¿å­˜: {sitemap_data['url']}")
                else:
                    print(f"âš ï¸ ä¿å­˜å¤±æ•—: {sitemap_data['url']}")
        
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«ä¿å­˜å¤±æ•—: {e}")
    
    def save_to_file(self) -> None:
        """ä¿å­˜ Sitemap æ¸…å–®åˆ°æ–‡ä»¶"""
        print(f"\nğŸ“ æ­£åœ¨ä¿å­˜ Sitemap æ¸…å–®åˆ° {self.output_file}...")
        
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                # å¯«å…¥æ¨™é¡Œ
                f.write("# Discovered Sitemaps\n")
                f.write(f"# Generated by getSiteMap.py\n")
                f.write(f"# Total: {len(self.discovered_sitemaps)} sitemaps\n\n")
                
                # æŒ‰å±¤æ¬¡çµæ§‹å¯«å…¥
                for sitemap_data in self.sitemap_hierarchy:
                    f.write(f"{sitemap_data['url']}\n")
                    
                    # å¦‚æœæœ‰å­ Sitemapï¼Œç¸®æ’å¯«å…¥
                    for sub_sitemap in sitemap_data.get('sub_sitemaps', []):
                        f.write(f"  {sub_sitemap}\n")
                
                # å¦‚æœæœ‰å…¶ä»–ç™¼ç¾çš„ sitemapï¼ˆå®¹éŒ¯è™•ç†ï¼‰
                written_urls = set()
                for sitemap_data in self.sitemap_hierarchy:
                    written_urls.add(sitemap_data['url'])
                    written_urls.update(sitemap_data.get('sub_sitemaps', []))
                
                remaining_sitemaps = self.discovered_sitemaps - written_urls
                if remaining_sitemaps:
                    f.write("\n# Additional discovered sitemaps\n")
                    for sitemap in sorted(remaining_sitemaps):
                        f.write(f"{sitemap}\n")
            
            print(f"âœ… Sitemap æ¸…å–®å·²ä¿å­˜åˆ° {self.output_file}")
            print(f"ğŸ“Š ç¸½è¨ˆ: {len(self.discovered_sitemaps)} å€‹ Sitemap")
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±æ•—: {e}")
    
    def print_summary(self) -> None:
        """æ‰“å°ç™¼ç¾æ‘˜è¦"""
        print(f"\nğŸ“‹ Sitemap ç™¼ç¾æ‘˜è¦:")
        print(f"=" * 50)
        
        total_urls = 0
        for sitemap_data in self.sitemap_hierarchy:
            sitemap_type = sitemap_data['type']
            url_count = sitemap_data['url_count']
            total_urls += url_count
            
            status_icon = "âœ…" if sitemap_type != 'error' else "âŒ"
            type_label = {
                'sitemapindex': 'Sitemap Index',
                'sitemap': 'Sitemap',
                'urlset': 'URL Set',
                'error': 'Error'
            }.get(sitemap_type, 'Unknown')
            
            print(f"{status_icon} {type_label}: {url_count} URLs")
            print(f"   ğŸ“ {sitemap_data['url']}")
            
            if sitemap_data.get('sub_sitemaps'):
                print(f"   ğŸ“š åŒ…å« {len(sitemap_data['sub_sitemaps'])} å€‹å­ Sitemap")
        
        print(f"=" * 50)
        print(f"ğŸ¯ ç¸½è¨ˆ: {len(self.discovered_sitemaps)} å€‹ Sitemap")
        print(f"ğŸ”¢ é ä¼° URL ç¸½æ•¸: {total_urls}")


async def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='Sitemap ç™¼ç¾å’Œè§£æå·¥å…·')
    parser.add_argument('--url', required=True, help='è¦è§£æçš„ç¶²ç«™ URL')
    parser.add_argument('--output', default='sitemaps.txt', help='è¼¸å‡ºæ–‡ä»¶åç¨±')
    parser.add_argument('--no-db', action='store_true', help='ä¸ä¿å­˜åˆ°è³‡æ–™åº«')
    parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°è¼¸å‡º')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ é–‹å§‹ Sitemap ç™¼ç¾æµç¨‹")
    print(f"ğŸ¯ ç›®æ¨™ç¶²ç«™: {args.url}")
    print(f"ğŸ“ è¼¸å‡ºæ–‡ä»¶: {args.output}")
    
    discovery = SitemapDiscovery(args.output)
    
    try:
        # 1. å¾ robots.txt ç™¼ç¾ Sitemap
        initial_sitemaps = await discovery.discover_sitemaps(args.url)
        
        # 2. åˆ†æ Sitemap å±¤æ¬¡çµæ§‹
        await discovery.analyze_sitemap_hierarchy(initial_sitemaps)
        
        # 3. ä¿å­˜åˆ°è³‡æ–™åº«ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if not args.no_db:
            await discovery.save_to_database()
        
        # 4. ä¿å­˜åˆ°æ–‡ä»¶
        discovery.save_to_file()
        
        # 5. æ‰“å°æ‘˜è¦
        discovery.print_summary()
        
        print(f"\nğŸ‰ Sitemap ç™¼ç¾å®Œæˆï¼")
        print(f"ğŸ“„ ä¸‹ä¸€æ­¥: python scripts/getUrls.py --sitemap-list {args.output}")
        
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
