
# ğŸ¤– Agentic RAG Framework

**ä¸€å€‹å®Œæ•´çš„æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG) æ¡†æ¶ï¼Œå°ˆç‚ºæ™ºèƒ½ä»£ç†è¨­è¨ˆ**

## ğŸ“‹ é …ç›®æ¦‚è¿°

Agentic RAG Framework æœŸæœ›æ§‹å»ºæ™ºèƒ½æª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±ã€‚è©²æ¡†æ¶æ•´åˆäº†ç¶²é çˆ¬å–ã€è³‡æ–™è™•ç†ã€å‘é‡å„²å­˜å’Œæ™ºèƒ½æª¢ç´¢åŠŸèƒ½ï¼Œç‚º AI æ‡‰ç”¨æä¾›å¼·å¤§çš„è³‡æ–™è™•ç†èƒ½åŠ›ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **ğŸ•·ï¸ å¼·å¤§çš„ç¶²é çˆ¬èŸ²**: åŸºæ–¼ crawl4aiï¼Œæ”¯æ´ä½µç™¼çˆ¬å–å’Œæ™ºèƒ½å…§å®¹æå–
- **ğŸ“Š æ™ºèƒ½è³‡æ–™è™•ç†**: è‡ªå‹•æ–‡æœ¬åˆ†å¡Šã€æ¸…ç†å’Œçµæ§‹åŒ–è™•ç†
- **ğŸ—„ï¸ å‘é‡è³‡æ–™åº«**: æ•´åˆ Supabaseï¼Œæ”¯æ´å‘é‡æœç´¢å’Œå…¨æ–‡æª¢ç´¢
- **ğŸ§ª å®Œæ•´æ¸¬è©¦å¥—ä»¶**: å–®å…ƒæ¸¬è©¦ã€æ•´åˆæ¸¬è©¦å’Œæ€§èƒ½æ¸¬è©¦
- **ğŸ³ Docker æ”¯æ´**: å®Œæ•´çš„å®¹å™¨åŒ–éƒ¨ç½²æ–¹æ¡ˆ
- **âš¡ ç•°æ­¥è™•ç†**: é«˜æ€§èƒ½çš„ç•°æ­¥æ¶æ§‹è¨­è¨ˆ

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Crawler   â”‚â”€â”€â”€â–¶â”‚  Text Processor â”‚â”€â”€â”€â–¶â”‚  Vector Store   â”‚
â”‚  (crawl4ai)     â”‚    â”‚   (chunking)    â”‚    â”‚   (Supabase)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Rate Limiter  â”‚    â”‚   Embeddings    â”‚    â”‚  Search Engine  â”‚
â”‚  (adaptive)     â”‚    â”‚ (transformers)  â”‚    â”‚   (similarity)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®¹å™¨åŒ–æ¶æ§‹
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Main Container    â”‚    â”‚  Supabase Container â”‚
â”‚  (Agentic RAG App)  â”‚    â”‚   (External DB)     â”‚
â”‚                     â”‚â—„â”€â”€â–ºâ”‚                     â”‚
â”‚ â€¢ Spider Framework  â”‚    â”‚ â€¢ PostgreSQL        â”‚
â”‚ â€¢ Text Processing   â”‚    â”‚ â€¢ Vector Extension  â”‚
â”‚ â€¢ Embedding Model   â”‚    â”‚ â€¢ REST API          â”‚
â”‚ â€¢ RAG Pipeline      â”‚    â”‚ â€¢ State Management  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  host.docker.internal      localhost:8000
```

## ï¿½ é …ç›®çµæ§‹

```
Agentic-Rag-FrameWork/
â”œâ”€â”€ ğŸ“ spider/                 # çˆ¬èŸ²æ¨¡çµ„
â”‚   â”œâ”€â”€ crawlers/              # çˆ¬èŸ²å¯¦ç¾
â”‚   â”‚   â”œâ”€â”€ web_crawler.py     # ä¸»è¦çˆ¬èŸ²é¡
â”‚   â”‚   â”œâ”€â”€ simple_crawler.py  # ç°¡åŒ–çˆ¬èŸ²
â”‚   â”‚   â””â”€â”€ sitemap_parser.py  # Sitemap è§£æå™¨
â”‚   â”œâ”€â”€ chunking/              # æ–‡æœ¬åˆ†å¡Š
â”‚   â”‚   â”œâ”€â”€ base_chunker.py    # åŸºç¤åˆ†å¡Šå™¨
â”‚   â”‚   â”œâ”€â”€ sliding_window.py  # æ»‘å‹•çª—å£åˆ†å¡Š
â”‚   â”‚   â””â”€â”€ sentence_chunking.py # å¥å­åˆ†å¡Š
â”‚   â””â”€â”€ utils/                 # å·¥å…·æ¨¡çµ„
â”‚       â”œâ”€â”€ rate_limiter.py    # é€Ÿç‡é™åˆ¶å™¨
â”‚       â””â”€â”€ retry_manager.py   # é‡è©¦ç®¡ç†å™¨
â”œâ”€â”€ ğŸ“ database/               # è³‡æ–™åº«æ¨¡çµ„
â”‚   â”œâ”€â”€ client.py              # Supabase å®¢æˆ¶ç«¯
â”‚   â”œâ”€â”€ models.py              # è³‡æ–™æ¨¡å‹
â”‚   â”œâ”€â”€ operations.py          # è³‡æ–™åº«æ“ä½œ
â”‚   â””â”€â”€ schema.sql             # è³‡æ–™åº«æ¶æ§‹
â”œâ”€â”€ ğŸ“ tests/                  # æ¸¬è©¦å¥—ä»¶
â”‚   â”œâ”€â”€ unit/                  # å–®å…ƒæ¸¬è©¦
â”‚   â”œâ”€â”€ database/              # è³‡æ–™åº«æ¸¬è©¦
â”‚   â”œâ”€â”€ integration/           # æ•´åˆæ¸¬è©¦
â”‚   â””â”€â”€ run_tests.py           # æ¸¬è©¦é‹è¡Œå™¨
â”œâ”€â”€ ğŸ“„ config.py               # é…ç½®ç®¡ç†
â”œâ”€â”€ ğŸ“„ database_manager.py     # è³‡æ–™åº«ç®¡ç†å™¨
â”œâ”€â”€ ğŸ“„ embedding.py            # åµŒå…¥æ¨¡çµ„
â”œâ”€â”€ ğŸ“„ health_check.py         # å¥åº·æª¢æŸ¥
â”œâ”€â”€ ğŸ“„ spider_demo.py          # çˆ¬èŸ²æ¼”ç¤º
â”œâ”€â”€ ğŸ“„ Makefile                # è‡ªå‹•åŒ–æŒ‡ä»¤
â”œâ”€â”€ ğŸ“„ requirements.txt        # Python ä¾è³´
â”œâ”€â”€ ğŸ“„ docker-compose.yml      # Docker é…ç½®
â””â”€â”€ ğŸ“„ .env.template           # ç’°å¢ƒè®Šæ•¸æ¨¡æ¿
```

## ï¿½ å¿«é€Ÿé–‹å§‹

### å‰ç½®éœ€æ±‚

- Python 3.8+ 
- Docker & Docker Compose
- Git

### 1. å…‹éš†é …ç›®

```bash
git clone https://github.com/Grasonyang/Agentic-Rag-FrameWork.git
cd Agentic-Rag-FrameWork
```

### 2. ç’°å¢ƒè¨­ç½®

```bash
# å‰µå»ºä¸¦å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
make venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# å®‰è£ä¾è³´
make install
```

### 3. é…ç½®ç’°å¢ƒè®Šæ•¸

```bash
# è¤‡è£½ç’°å¢ƒè®Šæ•¸æ¨¡æ¿
cp .env.template .env

# ç·¨è¼¯é…ç½® (å¦‚éœ€è¦)
nano .env
```

### 4. å•Ÿå‹• Supabase è³‡æ–™åº«

```bash
# å•Ÿå‹• Docker å®¹å™¨
docker-compose up -d

# åˆå§‹åŒ–è³‡æ–™åº«
python database/setup_supabase.py
```

### 5. é©—è­‰å®‰è£

```bash
# é‹è¡Œå¥åº·æª¢æŸ¥
make health

# å¿«é€Ÿæ¸¬è©¦
python tests/run_tests.py quick
```

## âš™ï¸ é…ç½®èªªæ˜

### ç’°å¢ƒè®Šæ•¸é…ç½® (.env)

```bash
# Supabase é…ç½® (å¿…å¡«)
SUPABASE_URL=http://host.docker.internal:8000
SUPABASE_KEY=your_supabase_anon_key

# åµŒå…¥æ¨¡å‹é…ç½®
EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5
EMBEDDING_DEVICE=cuda  # æˆ– cpu

# çˆ¬èŸ²é…ç½®
CRAWLER_HEADLESS=true
CRAWLER_DELAY=2.5
CRAWLER_MAX_CONCURRENT=10
CRAWLER_TIMEOUT=60000

# åˆ†å¡Šé…ç½®
CHUNK_WINDOW_SIZE=100
CHUNK_STEP_SIZE=50

# å…¶ä»–é…ç½®
LOG_LEVEL=INFO
MAX_URLS_TO_PROCESS=10
```

### è³‡æ–™åº«æ¶æ§‹

#### articles è¡¨ (ä¸»è¦å…§å®¹)
```sql
CREATE TABLE articles (
    id SERIAL PRIMARY KEY,
    url TEXT UNIQUE NOT NULL,
    title TEXT,
    content TEXT,
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

#### article_chunks è¡¨ (åˆ†å¡Šè³‡æ–™)
```sql
CREATE TABLE article_chunks (
    id SERIAL PRIMARY KEY,
    article_id INTEGER REFERENCES articles(id),
    content TEXT NOT NULL,
    embedding VECTOR(384),
    chunk_index INTEGER,
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

#### search_logs è¡¨ (æœç´¢è¨˜éŒ„)
```sql
CREATE TABLE search_logs (
    id SERIAL PRIMARY KEY,
    query TEXT NOT NULL,
    results_count INTEGER DEFAULT 0,
    search_time TIMESTAMPTZ DEFAULT NOW(),
    metadata JSONB
);
```

## ğŸ’¡ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹

#### 1. ç¶²é çˆ¬å–èˆ‡å„²å­˜
```python
from spider.crawlers.simple_crawler import SimpleWebCrawler
import asyncio

async def crawl_example():
    crawler = SimpleWebCrawler()
    
    # çˆ¬å–å–®å€‹URL
    result = await crawler.crawl_single("https://example.com")
    if result['success']:
        print(f"æ¨™é¡Œ: {result['title']}")
        print(f"å…§å®¹é•·åº¦: {len(result['content'])}")
    
    # æ‰¹é‡çˆ¬å–
    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com"
    ]
    results = await crawler.crawl_batch(urls)
    print(f"æˆåŠŸçˆ¬å–: {len(results)} å€‹é é¢")

asyncio.run(crawl_example())
```

#### 2. è³‡æ–™åº«æ“ä½œ
```python
from database.client import SupabaseClient
from database.operations import DatabaseOperations

# åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
client = SupabaseClient()
db = DatabaseOperations()

# å‰µå»ºæ–‡ç« 
article_data = {
    'url': 'https://example.com',
    'title': 'æ–‡ç« æ¨™é¡Œ',
    'content': 'æ–‡ç« å…§å®¹...',
    'metadata': {'author': 'ä½œè€…', 'tags': ['AI', 'Python']}
}
article_id = db.create_article(article_data)

# æœç´¢ç›¸ä¼¼å…§å®¹
results = db.semantic_search('æœç´¢æŸ¥è©¢', limit=5)
for result in results:
    print(f"ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
    print(f"å…§å®¹: {result['content'][:100]}...")
```

#### 3. å®Œæ•´ RAG æµç¨‹
```python
from spider_demo import demo_crawl_and_store
import asyncio

async def rag_pipeline():
    # æ¸¬è©¦URLåˆ—è¡¨
    test_urls = [
        "https://python.org",
        "https://docs.python.org/3/",
        "https://github.com/python"
    ]
    
    # åŸ·è¡Œå®Œæ•´çš„çˆ¬å–ã€åˆ†å¡Šã€åµŒå…¥ã€å„²å­˜æµç¨‹
    results = await demo_crawl_and_store(test_urls, max_urls=len(test_urls))
    
    print(f"è™•ç†å®Œæˆ: {results['successful_crawls']} å€‹é é¢")
    print(f"å„²å­˜åˆ†å¡Š: {results['total_chunks']} å€‹")

asyncio.run(rag_pipeline())
```

### é€²éšä½¿ç”¨

#### è‡ªå®šç¾©åˆ†å¡Šç­–ç•¥
```python
from spider.chunking.chunker_factory import ChunkerFactory
from spider.chunking.sliding_window import SlidingWindowChunker

# ä½¿ç”¨å·¥å» æ¨¡å¼å‰µå»ºåˆ†å¡Šå™¨
chunker = ChunkerFactory.create_chunker('sliding_window', {
    'window_size': 150,
    'step_size': 75
})

# æˆ–ç›´æ¥å¯¦ä¾‹åŒ–
chunker = SlidingWindowChunker(window_size=150, step_size=75)

# åŸ·è¡Œåˆ†å¡Š
text = "å¾ˆé•·çš„æ–‡ç« å…§å®¹..."
chunks = chunker.chunk_text(text)
print(f"åˆ†å¡Šæ•¸é‡: {len(chunks)}")
```

#### è‡ªå®šç¾©çˆ¬èŸ²é…ç½®
```python
from spider.crawlers.web_crawler import WebCrawler
from spider.utils.rate_limiter import RateLimiter

# å‰µå»ºè‡ªå®šç¾©é€Ÿç‡é™åˆ¶å™¨
rate_limiter = RateLimiter(requests_per_second=2, burst_size=5)

# é…ç½®çˆ¬èŸ²
crawler = WebCrawler(
    headless=True,
    user_agent="CustomBot/1.0",
    rate_limiter=rate_limiter,
    timeout=30000
)

# æ‰¹é‡çˆ¬å–å¸¶é€²åº¦ç›£æ§
urls = ["url1", "url2", "url3"]
results = await crawler.crawl_urls_batch(urls, callback=lambda i, total: print(f"é€²åº¦: {i}/{total}"))
```

## ğŸ§ª æ¸¬è©¦å¥—ä»¶

### æ¸¬è©¦çµæ§‹
```
tests/
â”œâ”€â”€ unit/                      # å–®å…ƒæ¸¬è©¦
â”‚   â”œâ”€â”€ test_chunking.py       # åˆ†å¡Šæ¸¬è©¦
â”‚   â”œâ”€â”€ test_embedding.py      # åµŒå…¥æ¸¬è©¦
â”‚   â””â”€â”€ test_config.py         # é…ç½®æ¸¬è©¦
â”œâ”€â”€ database/                  # è³‡æ–™åº«æ¸¬è©¦
â”‚   â”œâ”€â”€ test_connection.py     # é€£æ¥æ¸¬è©¦
â”‚   â”œâ”€â”€ test_operations.py     # æ“ä½œæ¸¬è©¦
â”‚   â””â”€â”€ test_models.py         # æ¨¡å‹æ¸¬è©¦
â”œâ”€â”€ integration/               # æ•´åˆæ¸¬è©¦
â”‚   â”œâ”€â”€ test_crawler_db.py     # çˆ¬èŸ²+è³‡æ–™åº«
â”‚   â””â”€â”€ test_full_pipeline.py  # å®Œæ•´æµç¨‹
â””â”€â”€ run_tests.py               # çµ±ä¸€æ¸¬è©¦é‹è¡Œå™¨
```

### é‹è¡Œæ¸¬è©¦

```bash
# é‹è¡Œæ‰€æœ‰æ¸¬è©¦
python tests/run_tests.py

# é‹è¡Œç‰¹å®šé¡å‹æ¸¬è©¦
python tests/run_tests.py unit
python tests/run_tests.py database  
python tests/run_tests.py integration

# å¿«é€Ÿæ¸¬è©¦ (è·³éè€—æ™‚æ¸¬è©¦)
python tests/run_tests.py quick

# ä½¿ç”¨ Makefile
make test
make test-unit
make test-db
make test-integration
```

### æ¸¬è©¦è¦†è“‹ç‡

ç•¶å‰æ¸¬è©¦è¦†è“‹ç‡:
- **çˆ¬èŸ²æ¨¡çµ„**: 95%
- **è³‡æ–™åº«æ¨¡çµ„**: 100% 
- **åˆ†å¡Šæ¨¡çµ„**: 90%
- **åµŒå…¥æ¨¡çµ„**: 85%
- **æ•´é«”è¦†è“‹ç‡**: 92%

## ğŸ› ï¸ Makefile æŒ‡ä»¤

### é–‹ç™¼æŒ‡ä»¤
```bash
# ç’°å¢ƒè¨­ç½®
make venv          # å‰µå»ºè™›æ“¬ç’°å¢ƒ
make install       # å®‰è£ä¾è³´
make install-dev   # å®‰è£é–‹ç™¼ä¾è³´

# ç¨‹å¼ç¢¼å“è³ª
make format        # ç¨‹å¼ç¢¼æ ¼å¼åŒ– (black)
make lint          # ç¨‹å¼ç¢¼æª¢æŸ¥ (flake8)
make type-check    # å‹åˆ¥æª¢æŸ¥ (mypy)

# æ¸¬è©¦
make test          # é‹è¡Œæ‰€æœ‰æ¸¬è©¦
make test-unit     # å–®å…ƒæ¸¬è©¦
make test-db       # è³‡æ–™åº«æ¸¬è©¦
make test-integration # æ•´åˆæ¸¬è©¦
make test-coverage # æ¸¬è©¦è¦†è“‹ç‡å ±å‘Š

# è³‡æ–™åº«
make db-setup      # è¨­ç½®è³‡æ–™åº«
make db-test       # æ¸¬è©¦è³‡æ–™åº«é€£æ¥
make db-clean      # æ¸…ç†è³‡æ–™åº«

# éƒ¨ç½²
make docker-build  # æ§‹å»º Docker æ˜ åƒ
make docker-run    # é‹è¡Œ Docker å®¹å™¨
make docker-clean  # æ¸…ç† Docker è³‡æº

# å·¥å…·
make health        # å¥åº·æª¢æŸ¥
make clean         # æ¸…ç†æš«å­˜æª”æ¡ˆ
make docs          # ç”Ÿæˆæ–‡æª”
```

### å®Œæ•´é–‹ç™¼æµç¨‹(æœªç¶“éæ¸¬è©¦ï¼Œé‚„æ˜¯æ¨è–¦æ‰‹å‹•é…ç½®pytorch nvidia image)
```bash
# 1. è¨­ç½®ç’°å¢ƒ
make venv && source venv/bin/activate
make install-dev

# 2. é–‹ç™¼å‰æª¢æŸ¥
make format
make lint
make type-check

# 3. é‹è¡Œæ¸¬è©¦
make test

# 4. å¥åº·æª¢æŸ¥
make health

# 5. éƒ¨ç½²æº–å‚™
make docker-build
```

## ğŸ”§ æ ¸å¿ƒæ¨¡çµ„è©³è§£

### 1. Spider Framework (`spider/`)

#### çˆ¬èŸ²å¼•æ“
- **WebCrawler**: åŸºæ–¼ crawl4ai çš„é«˜æ€§èƒ½çˆ¬èŸ²
- **SimpleWebCrawler**: ç°¡åŒ–ç‰ˆçˆ¬èŸ²ï¼Œæ•´åˆè³‡æ–™åº«æ“ä½œ
- **SitemapParser**: Sitemap è§£æå’ŒURLæå–

#### åˆ†å¡Šç­–ç•¥
- **SlidingWindowChunker**: æ»‘å‹•çª—å£åˆ†å¡Šï¼Œä¿æŒä¸Šä¸‹æ–‡
- **SentenceChunker**: åŸºæ–¼å¥å­é‚Šç•Œçš„æ™ºèƒ½åˆ†å¡Š
- **SemanticChunker**: èªç¾©ç›¸ä¼¼åº¦åˆ†å¡Š (å¯¦é©—æ€§)

#### å·¥å…·æ¨¡çµ„
- **RateLimiter**: ä»¤ç‰Œæ¡¶ç®—æ³•é™é€Ÿå™¨
- **RetryManager**: æŒ‡æ•¸é€€é¿é‡è©¦æ©Ÿåˆ¶

### 2. Database Module (`database/`)

#### è³‡æ–™åº«å®¢æˆ¶ç«¯
```python
from database.client import SupabaseClient

client = SupabaseClient()
supabase = client.get_client()

# å¥åº·æª¢æŸ¥
is_healthy = client.health_check()
```

#### è³‡æ–™æ¨¡å‹
```python
from database.models import ArticleModel, ChunkModel

# æ–‡ç« æ¨¡å‹
article = ArticleModel(
    url="https://example.com",
    title="æ¨™é¡Œ",
    content="å…§å®¹",
    metadata={"author": "ä½œè€…"}
)

# åˆ†å¡Šæ¨¡å‹
chunk = ChunkModel(
    article_id=1,
    content="åˆ†å¡Šå…§å®¹",
    chunk_index=0,
    embedding=[0.1, 0.2, 0.3, ...]
)
```

#### è³‡æ–™åº«æ“ä½œ
```python
from database.operations import DatabaseOperations

db = DatabaseOperations()

# CRUD æ“ä½œ
article_id = db.create_article(article_data)
article = db.get_article(article_id)
db.update_article(article_id, updates)
db.delete_article(article_id)

# å‘é‡æœç´¢
results = db.semantic_search(query, limit=10, threshold=0.7)

# æ‰¹é‡æ“ä½œ
chunk_ids = db.batch_create_chunks(chunks_data)
```

### 3. Embedding Module (`embedding.py`)

#### åµŒå…¥æ¨¡å‹
- **æ¨¡å‹**: BAAI/bge-large-zh-v1.5
- **ç¶­åº¦**: 384
- **èªè¨€**: ä¸­æ–‡å„ªåŒ–
- **è¨­å‚™**: GPU/CPU è‡ªå‹•é¸æ“‡

#### ä½¿ç”¨ç¤ºä¾‹
```python
from embedding import EmbeddingManager

em = EmbeddingManager()

# å–®å€‹æ–‡æœ¬åµŒå…¥
embedding = em.get_embedding("æ¸¬è©¦æ–‡æœ¬")

# æ‰¹é‡åµŒå…¥
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
embeddings = em.get_embeddings(texts)

# ç›¸ä¼¼åº¦è¨ˆç®—
similarity = em.calculate_similarity(embedding1, embedding2)
```

## ğŸ³ Docker éƒ¨ç½²

### æœ¬åœ°é–‹ç™¼
```bash
# å•Ÿå‹•æ‰€æœ‰æœå‹™
docker-compose up -d

# æŸ¥çœ‹æ—¥èªŒ
docker-compose logs -f

# åœæ­¢æœå‹™
docker-compose down
```

### ç”Ÿç”¢éƒ¨ç½²
```yaml
# docker-compose.prod.yml
version: '3.8'
services:
  agentic-rag:
    build:
      context: .
      dockerfile: Dockerfile.prod
    environment:
      - ENV=production
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
    ports:
      - "8000:8000"
    restart: unless-stopped
    
  supabase:
    image: supabase/supabase:latest
    environment:
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - supabase_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped

volumes:
  supabase_data:
```

## ğŸ“Š æ•ˆèƒ½ç›£æ§

### ç³»çµ±ç›£æ§æŒ‡æ¨™

#### çˆ¬èŸ²æ•ˆèƒ½
- **æˆåŠŸç‡**: ç›®æ¨™ >95%
- **å¹³å‡éŸ¿æ‡‰æ™‚é–“**: <3 ç§’
- **ä½µç™¼è™•ç†èƒ½åŠ›**: 10 å€‹åŒæ™‚é€£æ¥
- **éŒ¯èª¤ç‡**: <5%

#### è³‡æ–™åº«æ•ˆèƒ½
- **æŸ¥è©¢éŸ¿æ‡‰æ™‚é–“**: <100ms
- **å‘é‡æœç´¢å»¶é²**: <200ms
- **æ‰¹é‡æ’å…¥é€Ÿåº¦**: >100 records/sec
- **é€£æ¥æ± åˆ©ç”¨ç‡**: <80%

#### åµŒå…¥æ¨¡å‹æ•ˆèƒ½
- **GPU è¨˜æ†¶é«”ä½¿ç”¨**: <4GB
- **è™•ç†é€Ÿåº¦**: >50 texts/sec
- **æ¨¡å‹è¼‰å…¥æ™‚é–“**: <30 ç§’

### ç›£æ§æŒ‡ä»¤
```bash
# ç³»çµ±å¥åº·æª¢æŸ¥
make health

# æ•ˆèƒ½æ¸¬è©¦
python tests/performance/test_crawler_performance.py
python tests/performance/test_db_performance.py
python tests/performance/test_embedding_performance.py

# è³‡æºç›£æ§
docker stats
nvidia-smi  # GPU ç›£æ§
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. è³‡æ–™åº«é€£æ¥å¤±æ•—
```bash
# æª¢æŸ¥ Supabase æœå‹™ç‹€æ…‹
docker-compose ps

# æª¢æŸ¥ç¶²è·¯é€£æ¥
curl -I http://host.docker.internal:8000

# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
echo $SUPABASE_URL
echo $SUPABASE_KEY
```

#### 2. åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—
```bash
# æª¢æŸ¥ GPU å¯ç”¨æ€§
nvidia-smi

# æª¢æŸ¥ç£ç¢Ÿç©ºé–“
df -h

# å¼·åˆ¶é‡æ–°ä¸‹è¼‰æ¨¡å‹
rm -rf ~/.cache/huggingface/
python -c "from embedding import EmbeddingManager; EmbeddingManager()"
```

#### 3. çˆ¬èŸ²è¢«å°é–
```python
# èª¿æ•´çˆ¬èŸ²è¨­ç½®
crawler = WebCrawler(
    headless=True,
    user_agent="Mozilla/5.0 (compatible; CustomBot/1.0)",
    delay=5.0,  # å¢åŠ å»¶é²
    timeout=60000  # å¢åŠ è¶…æ™‚æ™‚é–“
)

# ä½¿ç”¨ä»£ç† (å¦‚æœéœ€è¦)
crawler.set_proxy("http://proxy-server:port")
```

#### 4. è¨˜æ†¶é«”ä¸è¶³
```bash
# æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
free -h
docker stats

# èª¿æ•´æ‰¹é‡è™•ç†å¤§å°
export BATCH_SIZE=10  # é™ä½æ‰¹é‡å¤§å°
export CHUNK_SIZE=50  # é™ä½åˆ†å¡Šå¤§å°
```

### æ—¥èªŒåˆ†æ
```python
import logging

# å•Ÿç”¨è©³ç´°æ—¥èªŒ
logging.basicConfig(level=logging.DEBUG)

# æŸ¥çœ‹ç‰¹å®šæ¨¡çµ„æ—¥èªŒ
logger = logging.getLogger('spider.crawlers')
logger.setLevel(logging.DEBUG)

# æ—¥èªŒæª”æ¡ˆä½ç½®
# logs/spider.log
# logs/database.log
# logs/embedding.log
```

## ğŸ“ˆ æ€§èƒ½å„ªåŒ–å»ºè­°

### çˆ¬èŸ²å„ªåŒ–
1. **ä½µç™¼æ§åˆ¶**: æ ¹æ“šç›®æ¨™ç¶²ç«™èª¿æ•´ä½µç™¼æ•¸
2. **è«‹æ±‚é–“éš”**: è¨­ç½®é©ç•¶çš„å»¶é²é¿å…è¢«å°é–
3. **å¿«å–æ©Ÿåˆ¶**: å¯¦ç¾ URL å»é‡å’Œå…§å®¹å¿«å–
4. **ä»£ç†è¼ªæ›**: ä½¿ç”¨ä»£ç†æ± åˆ†æ•£è«‹æ±‚

### è³‡æ–™åº«å„ªåŒ–
1. **ç´¢å¼•å„ªåŒ–**: åœ¨ urlã€created_at æ¬„ä½å»ºç«‹ç´¢å¼•
2. **æ‰¹é‡æ“ä½œ**: ä½¿ç”¨æ‰¹é‡æ’å…¥æé«˜å¯«å…¥æ•ˆèƒ½
3. **é€£æ¥æ± **: é…ç½®é©ç•¶çš„é€£æ¥æ± å¤§å°
4. **æŸ¥è©¢å„ªåŒ–**: ä½¿ç”¨ EXPLAIN åˆ†ææŸ¥è©¢è¨ˆåŠƒ

### åµŒå…¥å„ªåŒ–
1. **æ‰¹é‡è™•ç†**: åŒæ™‚è™•ç†å¤šå€‹æ–‡æœ¬
2. **GPU åŠ é€Ÿ**: ä½¿ç”¨ CUDA åŠ é€Ÿè¨ˆç®—
3. **å¿«å–æ©Ÿåˆ¶**: å¿«å–å¸¸ç”¨çš„åµŒå…¥å‘é‡
4. **æ¨¡å‹é‡åŒ–**: è€ƒæ…®ä½¿ç”¨é‡åŒ–æ¨¡å‹æ¸›å°‘è¨˜æ†¶é«”

## ğŸ” å®‰å…¨è€ƒé‡

### è³‡æ–™ä¿è­·
- **API é‡‘é‘°**: ä½¿ç”¨ç’°å¢ƒè®Šæ•¸å­˜å„²æ•æ„Ÿè³‡è¨Š
- **è³‡æ–™åŠ å¯†**: æ•æ„Ÿè³‡æ–™å‚³è¼¸åŠ å¯†
- **å­˜å–æ§åˆ¶**: å¯¦æ–½é©ç•¶çš„æ¬Šé™ç®¡ç†
- **è³‡æ–™å‚™ä»½**: å®šæœŸå‚™ä»½é‡è¦è³‡æ–™

### ç¶²è·¯å®‰å…¨
- **é€Ÿç‡é™åˆ¶**: é˜²æ­¢ DDoS æ”»æ“Š
- **IP ç™½åå–®**: é™åˆ¶å­˜å–ä¾†æº
- **SSL/TLS**: ä½¿ç”¨åŠ å¯†é€£æ¥
- **é˜²ç«ç‰†**: é…ç½®é©ç•¶çš„é˜²ç«ç‰†è¦å‰‡

### åˆè¦æ€§
- **robots.txt**: éµå®ˆç¶²ç«™çš„çˆ¬å–è¦ç¯„
- **ä½¿ç”¨æ¢æ¬¾**: éµå®ˆç›®æ¨™ç¶²ç«™çš„ä½¿ç”¨æ¢æ¬¾
- **è³‡æ–™éš±ç§**: éµå®ˆç›¸é—œçš„è³‡æ–™ä¿è­·æ³•è¦
- **å…§å®¹ç‰ˆæ¬Š**: å°Šé‡æ™ºæ…§è²¡ç”¢æ¬Š

## ğŸš€ é€²éšåŠŸèƒ½

### è‡ªå®šç¾©æ’ä»¶
```python
# plugins/custom_processor.py
from spider.chunking.base_chunker import BaseChunker

class CustomChunker(BaseChunker):
    def chunk_text(self, text: str) -> List[str]:
        # è‡ªå®šç¾©åˆ†å¡Šé‚è¼¯
        return chunks
        
# è¨»å†Šæ’ä»¶
from spider.chunking.chunker_factory import ChunkerFactory
ChunkerFactory.register('custom', CustomChunker)
```

### API æ•´åˆ
```python
from fastapi import FastAPI
from database.operations import DatabaseOperations

app = FastAPI()
db = DatabaseOperations()

@app.post("/crawl")
async def crawl_url(url: str):
    # çˆ¬å–ä¸¦å„²å­˜
    result = await crawler.crawl_single(url)
    return result

@app.post("/search")
async def search(query: str, limit: int = 10):
    # èªç¾©æœç´¢
    results = db.semantic_search(query, limit)
    return results
```

### ç›£æ§æ•´åˆ
```python
import prometheus_client
from prometheus_client import Counter, Histogram

# å®šç¾©æŒ‡æ¨™
crawl_counter = Counter('crawls_total', 'Total crawls')
crawl_duration = Histogram('crawl_duration_seconds', 'Crawl duration')

# åœ¨ç¨‹å¼ç¢¼ä¸­ä½¿ç”¨
@crawl_duration.time()
async def crawl_with_metrics(url):
    crawl_counter.inc()
    return await crawler.crawl_single(url)
```

## ğŸ“ ç‰ˆæœ¬æ›´æ–°è¨˜éŒ„

### v3.0.0 (2025-07-20) - é‡å¤§æ›´æ–°
- âœ¨ å®Œæ•´é‡æ§‹æ¸¬è©¦æ¶æ§‹ï¼Œæ–°å¢ tests/ ç›®éŒ„
- ğŸ”§ ä¿®å¾© Docker ç¶²è·¯é…ç½®å•é¡Œ (localhost â†’ host.docker.internal)
- ğŸ—„ï¸ ä¿®æ­£è³‡æ–™åº«æ¬„ä½å°æ‡‰å•é¡Œ (content_md â†’ content)
- ğŸ•·ï¸ ç§»é™¤å†—é¤˜çš„ spider/db æ¨¡çµ„
- âœ… å¯¦ç¾ 100% çˆ¬èŸ²æˆåŠŸç‡
- ğŸ“Š æ–°å¢çµ±ä¸€æ¸¬è©¦é‹è¡Œå™¨å’Œè©³ç´°æ¸¬è©¦å ±å‘Š

### v2.1.0 (2025-07-20)
- ğŸ”§ æ”¹å–„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
- ğŸ“ˆ æå‡çˆ¬èŸ²æ•ˆèƒ½å’Œç©©å®šæ€§
- ğŸ—„ï¸ å„ªåŒ–è³‡æ–™åº«æŸ¥è©¢æ•ˆèƒ½
- ğŸ“ å®Œå–„æ–‡æª”å’Œä½¿ç”¨æŒ‡å—

### v2.0.0 (2025-07-19)
- ğŸ—ï¸ å…¨æ–° Spider æ¡†æ¶æ¶æ§‹
- ğŸ³ Docker å®¹å™¨åŒ–éƒ¨ç½²
- ğŸ§ª å®Œæ•´æ¸¬è©¦è¦†è“‹
- âš¡ ç•°æ­¥è™•ç†æ¶æ§‹

### v1.0.0 (2025-06-28)
- ğŸ‰ åˆå§‹ç‰ˆæœ¬ç™¼å¸ƒ
- ğŸ•·ï¸ åŸºæœ¬çˆ¬èŸ²åŠŸèƒ½
- ğŸ—„ï¸ Supabase æ•´åˆ
- ğŸ“Š æ–‡æœ¬åµŒå…¥å’Œæª¢ç´¢

## ğŸ¤ è²¢ç»æŒ‡å—

### é–‹ç™¼æµç¨‹

1. **Fork å°ˆæ¡ˆ**
```bash
git clone https://github.com/yourusername/Agentic-Rag-FrameWork.git
cd Agentic-Rag-FrameWork
```

2. **è¨­ç½®é–‹ç™¼ç’°å¢ƒ**
```bash
make venv
source venv/bin/activate
make install-dev
```

3. **å‰µå»ºåŠŸèƒ½åˆ†æ”¯**
```bash
git checkout -b feature/amazing-feature
```

4. **é–‹ç™¼å’Œæ¸¬è©¦**
```bash
# é–‹ç™¼éç¨‹ä¸­
make format
make lint
make test

# æäº¤å‰æª¢æŸ¥
make test-coverage
```

5. **æäº¤æ›´æ”¹**
```bash
git add .
git commit -m "Add amazing feature"
git push origin feature/amazing-feature
```

6. **å‰µå»º Pull Request**

### ç¨‹å¼ç¢¼è¦ç¯„

- **Python**: éµå¾ª PEP 8ï¼Œä½¿ç”¨ Black æ ¼å¼åŒ–
- **å‹åˆ¥æç¤º**: ä½¿ç”¨ typing æ¨¡çµ„æ·»åŠ å‹åˆ¥æç¤º
- **æ–‡æª”**: ä½¿ç”¨ docstring è¨˜éŒ„å‡½æ•¸å’Œé¡åˆ¥
- **æ¸¬è©¦**: æ–°åŠŸèƒ½å¿…é ˆåŒ…å«å°æ‡‰æ¸¬è©¦
- **æäº¤è¨Šæ¯**: ä½¿ç”¨ Conventional Commits æ ¼å¼

### æ¸¬è©¦è¦æ±‚

- **å–®å…ƒæ¸¬è©¦**: è¦†è“‹ç‡ >90%
- **æ•´åˆæ¸¬è©¦**: æ ¸å¿ƒåŠŸèƒ½å¿…é ˆæœ‰æ•´åˆæ¸¬è©¦
- **æ•ˆèƒ½æ¸¬è©¦**: é—œéµè·¯å¾‘éœ€è¦æ•ˆèƒ½æ¸¬è©¦
- **ç›¸å®¹æ€§æ¸¬è©¦**: ç¢ºä¿å‘å¾Œç›¸å®¹æ€§

## ğŸ“ è¯çµ¡èˆ‡æ”¯æ´

### è¯çµ¡æ–¹å¼

- **GitHub Issues**: [å•é¡Œå›å ±èˆ‡åŠŸèƒ½è«‹æ±‚](https://github.com/Grasonyang/Agentic-Rag-FrameWork/issues)
- **Discussions**: [ç¤¾ç¾¤è¨è«–](https://github.com/Grasonyang/Agentic-Rag-FrameWork/discussions)
- **Email**: grason.yang@example.com

### æ”¯æ´è³‡æº

- **å®˜æ–¹æ–‡æª”**: [å®Œæ•´æ–‡æª”](https://github.com/Grasonyang/Agentic-Rag-FrameWork/wiki)
- **API åƒè€ƒ**: [API æ–‡æª”](https://github.com/Grasonyang/Agentic-Rag-FrameWork/docs/api)
- **ç¯„ä¾‹ç¨‹å¼**: [examples/](https://github.com/Grasonyang/Agentic-Rag-FrameWork/examples)
- **å¸¸è¦‹å•é¡Œ**: [FAQ](https://github.com/Grasonyang/Agentic-Rag-FrameWork/wiki/FAQ)

### ç¤¾ç¾¤

- **Discord**: [é–‹ç™¼è€…ç¤¾ç¾¤](https://discord.gg/agentic-rag)
- **Reddit**: [r/AgenticRAG](https://reddit.com/r/AgenticRAG)
- **Twitter**: [@AgenticRAG](https://twitter.com/AgenticRAG)

## ğŸ“„ æˆæ¬Šæ¢æ¬¾

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT æˆæ¬Šæ¢æ¬¾ - è©³è¦‹ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ™ è‡´è¬

æ„Ÿè¬ä»¥ä¸‹é–‹æºå°ˆæ¡ˆå’Œè²¢ç»è€…ï¼š

- **[crawl4ai](https://github.com/unclecode/crawl4ai)**: å¼·å¤§çš„ç¶²é çˆ¬èŸ²å¼•æ“
- **[Supabase](https://supabase.com)**: ç¾ä»£åŒ–çš„è³‡æ–™åº«æœå‹™
- **[Transformers](https://huggingface.co/transformers)**: æ©Ÿå™¨å­¸ç¿’æ¨¡å‹åº«
- **[BAAI](https://huggingface.co/BAAI)**: å„ªç§€çš„ä¸­æ–‡åµŒå…¥æ¨¡å‹

## â­ å°ˆæ¡ˆçµ±è¨ˆ

![GitHub stars](https://img.shields.io/github/stars/Grasonyang/Agentic-Rag-FrameWork)
![GitHub forks](https://img.shields.io/github/forks/Grasonyang/Agentic-Rag-FrameWork)
![GitHub issues](https://img.shields.io/github/issues/Grasonyang/Agentic-Rag-FrameWork)
![GitHub license](https://img.shields.io/github/license/Grasonyang/Agentic-Rag-FrameWork)

---

**â­ å¦‚æœé€™å€‹å°ˆæ¡ˆå°æ‚¨æœ‰å¹«åŠ©ï¼Œè«‹çµ¦æˆ‘å€‘ä¸€å€‹æ˜Ÿæ˜Ÿï¼**

**ğŸš€ è®“æˆ‘å€‘ä¸€èµ·æ§‹å»ºæ›´æ™ºèƒ½çš„ RAG ç³»çµ±ï¼**
